{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importing the library\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "from IPython.display import display # Manage multiple output per cell\n",
    "import datetime\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "odd_H = 'INFO_BbAvH'\n",
    "odd_A = 'INFO_BbAvA'\n",
    "odd_D = 'INFO_BbAvD'\n",
    "target = 'INFO_FTR'\n",
    "start_date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "season_list = [2014, 2015, 2016]\n",
    "league = 'D1'\n",
    "classes = ['A', 'D', 'H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'C': 8.291,\n",
    "    'penalty': 'l2',\n",
    "    'class_weight': None,\n",
    "    'solver': 'sag',\n",
    "    'max_iter': 270,\n",
    "    'multi_class': 'multinomial'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_features_MLP = ['A_MEANS_FIVE_AC', 'A_MEANS_FIVE_AS', 'A_MEANS_FIVE_AST','A_MEANS_FIVE_FTAG', 'A_MEANS_FIVE_FTHG', 'A_MEANS_FIVE_FTR_H','A_MEANS_FIVE_HC', 'A_MEANS_FIVE_HS', 'A_MEANS_FIVE_HST','A_MEANS_FIVE_HTR_A', 'H_MEANS_FIVE_AC', 'H_MEANS_FIVE_AS','H_MEANS_FIVE_AST', 'H_MEANS_FIVE_AY', 'H_MEANS_FIVE_FTAG','H_MEANS_FIVE_FTHG', 'H_MEANS_FIVE_FTR_A', 'H_MEANS_FIVE_FTR_H','H_MEANS_FIVE_HC', 'H_MEANS_FIVE_HS', 'H_MEANS_FIVE_HST','H_MEANS_FIVE_HTR_H', 'A_MEANS_THREE_AC', 'A_MEANS_THREE_AS','A_MEANS_THREE_FTHG', 'A_MEANS_THREE_HS', 'H_MEANS_THREE_AS','A_STD_FIVE_HF', 'H_STD_FIVE_HC', 'H_STD_FIVE_HST']\n",
    "all_features = [\"A_MEANS_FIVE_AC\",\"A_MEANS_FIVE_AF\",\"A_MEANS_FIVE_AR\",\"A_MEANS_FIVE_AS\",\"A_MEANS_FIVE_AST\",\"A_MEANS_FIVE_AY\",\"A_MEANS_FIVE_FTAG\",\"A_MEANS_FIVE_FTHG\",\"A_MEANS_FIVE_FTR_A\",\"A_MEANS_FIVE_FTR_D\",\"A_MEANS_FIVE_FTR_H\",\"A_MEANS_FIVE_HC\",\"A_MEANS_FIVE_HF\",\"A_MEANS_FIVE_HR\",\"A_MEANS_FIVE_HS\",\"A_MEANS_FIVE_HST\",\"A_MEANS_FIVE_HTAG\",\"A_MEANS_FIVE_HTHG\",\"A_MEANS_FIVE_HTR_A\",\"A_MEANS_FIVE_HTR_D\",\"A_MEANS_FIVE_HTR_H\",\"A_MEANS_FIVE_HY\",\"H_MEANS_FIVE_AC\",\"H_MEANS_FIVE_AF\",\"H_MEANS_FIVE_AR\",\"H_MEANS_FIVE_AS\",\"H_MEANS_FIVE_AST\",\"H_MEANS_FIVE_AY\",\"H_MEANS_FIVE_FTAG\",\"H_MEANS_FIVE_FTHG\",\"H_MEANS_FIVE_FTR_A\",\"H_MEANS_FIVE_FTR_D\",\"H_MEANS_FIVE_FTR_H\",\"H_MEANS_FIVE_HC\",\"H_MEANS_FIVE_HF\",\"H_MEANS_FIVE_HR\",\"H_MEANS_FIVE_HS\",\"H_MEANS_FIVE_HST\",\"H_MEANS_FIVE_HTAG\",\"H_MEANS_FIVE_HTHG\",\"H_MEANS_FIVE_HTR_A\",\"H_MEANS_FIVE_HTR_D\",\"H_MEANS_FIVE_HTR_H\",\"H_MEANS_FIVE_HY\",\"A_MEANS_THREE_AC\",\"A_MEANS_THREE_AF\",\"A_MEANS_THREE_AR\",\"A_MEANS_THREE_AS\",\"A_MEANS_THREE_AST\",\"A_MEANS_THREE_AY\",\"A_MEANS_THREE_FTAG\",\"A_MEANS_THREE_FTHG\",\"A_MEANS_THREE_FTR_A\",\"A_MEANS_THREE_FTR_D\",\"A_MEANS_THREE_FTR_H\",\"A_MEANS_THREE_HC\",\"A_MEANS_THREE_HF\",\"A_MEANS_THREE_HR\",\"A_MEANS_THREE_HS\",\"A_MEANS_THREE_HST\",\"A_MEANS_THREE_HTAG\",\"A_MEANS_THREE_HTHG\",\"A_MEANS_THREE_HTR_A\",\"A_MEANS_THREE_HTR_D\",\"A_MEANS_THREE_HTR_H\",\"A_MEANS_THREE_HY\",\"H_MEANS_THREE_AC\",\"H_MEANS_THREE_AF\",\"H_MEANS_THREE_AR\",\"H_MEANS_THREE_AS\",\"H_MEANS_THREE_AST\",\"H_MEANS_THREE_AY\",\"H_MEANS_THREE_FTAG\",\"H_MEANS_THREE_FTHG\",\"H_MEANS_THREE_FTR_A\",\"H_MEANS_THREE_FTR_D\",\"H_MEANS_THREE_FTR_H\",\"H_MEANS_THREE_HC\",\"H_MEANS_THREE_HF\",\"H_MEANS_THREE_HR\",\"H_MEANS_THREE_HS\",\"H_MEANS_THREE_HST\",\"H_MEANS_THREE_HTAG\",\"H_MEANS_THREE_HTHG\",\"H_MEANS_THREE_HTR_A\",\"H_MEANS_THREE_HTR_D\",\"H_MEANS_THREE_HTR_H\",\"H_MEANS_THREE_HY\",\"A_STD_FIVE_AC\",\"A_STD_FIVE_AF\",\"A_STD_FIVE_AR\",\"A_STD_FIVE_AS\",\"A_STD_FIVE_AST\",\"A_STD_FIVE_AY\",\"A_STD_FIVE_FTAG\",\"A_STD_FIVE_FTHG\",\"A_STD_FIVE_FTR_A\",\"A_STD_FIVE_FTR_D\",\"A_STD_FIVE_FTR_H\",\"A_STD_FIVE_HC\",\"A_STD_FIVE_HF\",\"A_STD_FIVE_HR\",\"A_STD_FIVE_HS\",\"A_STD_FIVE_HST\",\"A_STD_FIVE_HTAG\",\"A_STD_FIVE_HTHG\",\"A_STD_FIVE_HTR_A\",\"A_STD_FIVE_HTR_D\",\"A_STD_FIVE_HTR_H\",\"A_STD_FIVE_HY\",\"H_STD_FIVE_AC\",\"H_STD_FIVE_AF\",\"H_STD_FIVE_AR\",\"H_STD_FIVE_AS\",\"H_STD_FIVE_AST\",\"H_STD_FIVE_AY\",\"H_STD_FIVE_FTAG\",\"H_STD_FIVE_FTHG\",\"H_STD_FIVE_FTR_A\",\"H_STD_FIVE_FTR_D\",\"H_STD_FIVE_FTR_H\",\"H_STD_FIVE_HC\",\"H_STD_FIVE_HF\",\"H_STD_FIVE_HR\",\"H_STD_FIVE_HS\",\"H_STD_FIVE_HST\",\"H_STD_FIVE_HTAG\",\"H_STD_FIVE_HTHG\",\"H_STD_FIVE_HTR_A\",\"H_STD_FIVE_HTR_D\",\"H_STD_FIVE_HTR_H\",\"H_STD_FIVE_HY\",\"A_STD_THREE_AC\",\"A_STD_THREE_AF\",\"A_STD_THREE_AR\",\"A_STD_THREE_AS\",\"A_STD_THREE_AST\",\"A_STD_THREE_AY\",\"A_STD_THREE_FTAG\",\"A_STD_THREE_FTHG\",\"A_STD_THREE_FTR_A\",\"A_STD_THREE_FTR_D\",\"A_STD_THREE_FTR_H\",\"A_STD_THREE_HC\",\"A_STD_THREE_HF\",\"A_STD_THREE_HR\",\"A_STD_THREE_HS\",\"A_STD_THREE_HST\",\"A_STD_THREE_HTAG\",\"A_STD_THREE_HTHG\",\"A_STD_THREE_HTR_A\",\"A_STD_THREE_HTR_D\",\"A_STD_THREE_HTR_H\",\"A_STD_THREE_HY\",\"H_STD_THREE_AC\",\"H_STD_THREE_AF\",\"H_STD_THREE_AR\",\"H_STD_THREE_AS\",\"H_STD_THREE_AST\",\"H_STD_THREE_AY\",\"H_STD_THREE_FTAG\",\"H_STD_THREE_FTHG\",\"H_STD_THREE_FTR_A\",\"H_STD_THREE_FTR_D\",\"H_STD_THREE_FTR_H\",\"H_STD_THREE_HC\",\"H_STD_THREE_HF\",\"H_STD_THREE_HR\",\"H_STD_THREE_HS\",\"H_STD_THREE_HST\",\"H_STD_THREE_HTAG\",\"H_STD_THREE_HTHG\",\"H_STD_THREE_HTR_A\",\"H_STD_THREE_HTR_D\",\"H_STD_THREE_HTR_H\",\"H_STD_THREE_HY\"]\n",
    "best_features_NB = ['A_MEANS_FIVE_AC', 'A_MEANS_FIVE_AS', 'A_MEANS_FIVE_AST','A_MEANS_FIVE_FTAG', 'A_MEANS_FIVE_FTHG', 'A_MEANS_FIVE_FTR_H','A_MEANS_FIVE_HC', 'A_MEANS_FIVE_HS', 'A_MEANS_FIVE_HST','A_MEANS_FIVE_HTR_A', 'H_MEANS_FIVE_AC', 'H_MEANS_FIVE_AS','H_MEANS_FIVE_AST', 'H_MEANS_FIVE_AY', 'H_MEANS_FIVE_FTAG','H_MEANS_FIVE_FTHG', 'H_MEANS_FIVE_FTR_A', 'H_MEANS_FIVE_FTR_H','H_MEANS_FIVE_HC', 'H_MEANS_FIVE_HS', 'H_MEANS_FIVE_HST','H_MEANS_FIVE_HTR_H', 'A_MEANS_THREE_AC', 'A_MEANS_THREE_AS','A_MEANS_THREE_FTHG', 'A_MEANS_THREE_HS', 'H_MEANS_THREE_AS','A_STD_FIVE_HF', 'H_STD_FIVE_HC', 'H_STD_FIVE_HST']\n",
    "features_list = [\n",
    "    ['best_features_MLP', best_features_MLP],\n",
    "    ['all_features', all_features],\n",
    "    ['best_features_NB', best_features_NB]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct base layer\n",
    "base_layer = [\n",
    "    ['XGBoost', False, 'no', 9, XGBClassifier(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=200,\n",
    "        max_depth=4,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.41,\n",
    "        subsample=0.76,\n",
    "        colsample_bytree=0.4,\n",
    "        objective='multi:softprob',\n",
    "        reg_alpha=0.46,\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=15), \n",
    "     ['all_features', all_features]\n",
    "    ],\n",
    "    ['NB', True, 'no', 9, GaussianNB(), ['best_features_NB', best_features_NB]],\n",
    "    ['MLP', True, 'no', 9, MLPClassifier(\n",
    "        random_state=0,\n",
    "        activation='logistic', \n",
    "        alpha=1.4, \n",
    "        hidden_layer_sizes=(160,),\n",
    "        max_iter=260, \n",
    "        solver='sgd'),\n",
    "     ['best_features_MLP', best_features_MLP]\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure number of fold\n",
    "NFOLDS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DB Sqlite connection\n",
    "import sqlite3\n",
    "db = \"/Users/thibaultclement/Project/ligue1-predict/src/notebook/data/db/soccer_predict.sqlite\"\n",
    "conn = sqlite3.connect(db)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37907, 190)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all prematch data\n",
    "df_all = pd.read_sql_query(\"SELECT * FROM pre_matchs ORDER BY INFO_Date ASC;\", conn)\n",
    "df_all = (df_all[df_all.columns.drop(['index'])])\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30912, 190)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all game between June (include) and October (include)\n",
    "df_all['INFO_Date'] = pd.to_datetime(df_all['INFO_Date'])\n",
    "df_all['INFO_Date'].dt.month\n",
    "df_all = df_all[(df_all['INFO_Date'].dt.month < 6) | (df_all['INFO_Date'].dt.month >= 10)]\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a INFO_WIN column containing the gain if you bet the good result\n",
    "df_all['INFO_WIN'] = 0\n",
    "df_all.loc[df_all.INFO_FTR == 'H', 'INFO_WIN'] = df_all[odd_H]\n",
    "df_all.loc[df_all.INFO_FTR == 'A', 'INFO_WIN'] = df_all[odd_A]\n",
    "df_all.loc[df_all.INFO_FTR == 'D', 'INFO_WIN'] = df_all[odd_D]\n",
    "df_all['INFO_WIN_P'] = 0\n",
    "df_all.loc[df_all.INFO_FTR == 'H', 'INFO_WIN_P'] = df_all['INFO_PSH']\n",
    "df_all.loc[df_all.INFO_FTR == 'A', 'INFO_WIN_P'] = df_all['INFO_PSA']\n",
    "df_all.loc[df_all.INFO_FTR == 'D', 'INFO_WIN_P'] = df_all['INFO_PSD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(league, season, calibration, historical_training_year, features):\n",
    "    # Filter by league\n",
    "    df = df_all[(df_all['INFO_Div'] == league)]\n",
    "    # Keep season for test and filter by number of historical season used to train\n",
    "    date_start_learn = datetime.date(season-historical_training_year, 8, 1)\n",
    "    date_end_learn = datetime.date(season, 8, 1)\n",
    "    date_start_test_season = datetime.date(season, 8, 1)\n",
    "    date_end_test_season = datetime.date(season+1, 8, 1)\n",
    "    df_test = df[(df['INFO_Date'] > date_start_test_season)]\n",
    "    df_test = df_test[(df_test['INFO_Date'] < date_end_test_season)]\n",
    "    df = df[(df['INFO_Date'] > date_start_learn)]\n",
    "    df = df[(df['INFO_Date'] < date_end_learn)]\n",
    "    # reset index\n",
    "    df = df.reset_index()\n",
    "    df_test = df_test.reset_index()\n",
    "    # Encode Label\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df[target])\n",
    "    # Filter by feature used to train\n",
    "    X = pd.get_dummies(df[features])\n",
    "    #y = le.transform(df[target])\n",
    "    y = df[target]\n",
    "    X_test_season = pd.get_dummies(df_test[features])\n",
    "    #y_test_season = le.transform(df_test[target])\n",
    "    y_test_season = df_test[target]\n",
    "    # Impute of missing values (NaN) with the mean\n",
    "    # TODO drop NaN instead of replacing ith means \n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    imp = imp.fit(X)\n",
    "    X = imp.transform(X)\n",
    "    X_test_season = imp.transform(X_test_season)\n",
    "    # Standardize features\n",
    "    if calibration:\n",
    "        sc_X = StandardScaler().fit(X)\n",
    "        X = sc_X.transform(X)\n",
    "        X_test_season = sc_X.transform(X_test_season)\n",
    "    return df, df_test, X, y, X_test_season, y_test_season, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, x_test, kf_param):\n",
    "    oof_train = np.zeros((x_train.shape[0],3))\n",
    "    oof_test = np.zeros((x_test.shape[0],3))\n",
    "    oof_test_skf = np.empty((NFOLDS, x_test.shape[0], 3))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf_param):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train_df[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        # Calibrate model\n",
    "        clf.fit(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict_proba(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict_proba(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train, oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_layer_columns(layer, classes):\n",
    "    cols = []\n",
    "    for clf_name, preprocess, calibration, historical_training_year, clf, features in layer:\n",
    "        for result in classes:\n",
    "            cols.append(clf_name+result)\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_layer1_df(X, X_test_season, base_layer, cols):\n",
    "    X_train_layer1 = np.zeros((X.shape[0], len(base_layer)*3))\n",
    "    X_train_layer1 = pd.DataFrame(X_train_layer1, columns=cols)\n",
    "    X_test_layer1 = np.zeros((X_test_season.shape[0], len(base_layer)*3))\n",
    "    X_test_layer1 = pd.DataFrame(X_test_layer1, columns=cols)\n",
    "    return X_train_layer1, X_test_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_score(y, pred, probs):\n",
    "    # Compute cross-entropy score\n",
    "    ll = log_loss(y, probs)\n",
    "    # Compute accuracy score\n",
    "    acc = accuracy_score(y, pred)\n",
    "    # Compute precision score\n",
    "    prec = precision_score(y, pred, average=None)\n",
    "    prec_A = prec[0]\n",
    "    prec_D = prec[1]\n",
    "    prec_H = prec[2]\n",
    "    # Compute recall score\n",
    "    rec = recall_score(y, pred, average=None)\n",
    "    rec_A = rec[0]\n",
    "    rec_D = rec[1]\n",
    "    rec_H = rec[2]\n",
    "    # Compute F1 score\n",
    "    f1 = f1_score(y, pred, average=None)\n",
    "    f1_A = f1[0]\n",
    "    f1_D = f1[1]\n",
    "    f1_H = f1[2]\n",
    "    return {\n",
    "        'll': ll, \n",
    "        'acc': acc, \n",
    "        'prec_A': prec_A, \n",
    "        'prec_D': prec_D, \n",
    "        'prec_H': prec_H, \n",
    "        'rec_A': rec_A, \n",
    "        'rec_D': rec_D, \n",
    "        'rec_H': rec_H, \n",
    "        'f1_A': f1_A, \n",
    "        'f1_D': f1_D, \n",
    "        'f1_H': f1_H\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_money(df_test, pred_season, prob_season):\n",
    "    # Join odd and prediction together\n",
    "    df_test_season = df_test\n",
    "    df_test_season['probs_A'] = prob_season[:,0]\n",
    "    df_test_season['probs_D'] = prob_season[:,1]\n",
    "    df_test_season['probs_H'] = prob_season[:,2]\n",
    "    df_test_season['probs'] = df_test_season[['probs_A','probs_D','probs_H']].max(axis=1)\n",
    "    #df_test_season['pred'] = le.inverse_transform(pred_season)\n",
    "    df_test_season['pred'] = pred_season\n",
    "    df_test_season['WIN'] = -1\n",
    "    df_test_season.loc[df_test_season.INFO_FTR == df_test_season.pred, 'WIN'] = df_test_season['INFO_WIN']-1\n",
    "    df_test_season['WIN_P'] = -1\n",
    "    df_test_season.loc[df_test_season.INFO_FTR == df_test_season.pred, 'WIN_P'] = df_test_season['INFO_WIN_P']-1\n",
    "    df_test_season['INFO_ODD'] = 0\n",
    "    df_test_season.loc[df_test_season.pred == 'A', 'INFO_ODD_BET'] = df_test_season[odd_A]\n",
    "    df_test_season.loc[df_test_season.pred == 'D', 'INFO_ODD_BET'] = df_test_season[odd_D]\n",
    "    df_test_season.loc[df_test_season.pred == 'H', 'INFO_ODD_BET'] = df_test_season[odd_H]\n",
    "    df_test_season['prob_less_bet'] = 0\n",
    "    df_test_season.loc[df_test_season.pred == 'A', 'prob_less_bet'] = df_test_season['probs'] - df_test_season[odd_A].apply(lambda x: 1/x)\n",
    "    df_test_season.loc[df_test_season.pred == 'D', 'prob_less_bet'] = df_test_season['probs'] - df_test_season[odd_D].apply(lambda x: 1/x)\n",
    "    df_test_season.loc[df_test_season.pred == 'H', 'prob_less_bet'] = df_test_season['probs'] - df_test_season[odd_H].apply(lambda x: 1/x)\n",
    "    # calculate money I can get following different scenario\n",
    "    # Bet on all\n",
    "    bet_all = df_test_season.WIN.mean()\n",
    "    # Bet under 1.9\n",
    "    bet_lte_19 = df_test_season[df_test_season['INFO_ODD_BET'] < 1.9].WIN.mean()\n",
    "    # Bet under 4\n",
    "    bet_lte_4 = df_test_season[df_test_season['INFO_ODD_BET'] < 4].WIN.mean()\n",
    "    # Bet between 1.9 and 4\n",
    "    bet_btw_19_4 = df_test_season[(df_test_season['INFO_ODD_BET'] > 1.9) & (df_test_season['INFO_ODD_BET'] < 4)].WIN.mean()\n",
    "    # Bet between 1.9 and 5\n",
    "    bet_btw_19_5 = df_test_season[(df_test_season['INFO_ODD_BET'] > 1.9) & (df_test_season['INFO_ODD_BET'] < 5)].WIN.mean()\n",
    "    # Bet between 1.5 and 4\n",
    "    bet_btw_15_4 = df_test_season[(df_test_season['INFO_ODD_BET'] > 1.5) & (df_test_season['INFO_ODD_BET'] < 4)].WIN.mean()\n",
    "    # Bet between 1.5 and 5\n",
    "    bet_btw_15_5 = df_test_season[(df_test_season['INFO_ODD_BET'] > 1.5) & (df_test_season['INFO_ODD_BET'] < 5)].WIN.mean()\n",
    "    # Bet prob higher than 50%\n",
    "    bet_pred_gte_50 = df_test_season[df_test_season.probs > 0.5].WIN.mean()\n",
    "    # Bet prob higher than 60%\n",
    "    bet_pred_gte_60 = df_test_season[df_test_season.probs > 0.6].WIN.mean()\n",
    "    # Bet prob higher than 70%\n",
    "    bet_pred_gte_70 = df_test_season[df_test_season.probs > 0.7].WIN.mean()\n",
    "    return {\n",
    "        'bet_all': bet_all,\n",
    "        'bet_lte_19': bet_lte_19,\n",
    "        'bet_lte_4': bet_lte_4,\n",
    "        'bet_btw_19_4': bet_btw_19_4,\n",
    "        'bet_btw_19_5': bet_btw_19_5,\n",
    "        'bet_btw_15_4': bet_btw_15_4,\n",
    "        'bet_btw_15_5': bet_btw_15_5,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop on season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Init dataframe\n",
    "result_df = pd.DataFrame(columns=[\n",
    "    'league', \n",
    "    'season', \n",
    "    'historical_training_year', \n",
    "    'C',\n",
    "    'penalty',\n",
    "    'class_weight',\n",
    "    'solver',\n",
    "    'max_iter',\n",
    "    'll',\n",
    "    'acc',\n",
    "    'prec_A',\n",
    "    'prec_D',\n",
    "    'prec_H',\n",
    "    'rec_A',\n",
    "    'rec_D',\n",
    "    'rec_H',\n",
    "    'f1_A',\n",
    "    'f1_D',\n",
    "    'f1_H',\n",
    "    'bet_all',\n",
    "    'bet_lte_19',\n",
    "    'bet_lte_4',\n",
    "    'bet_btw_19_4',\n",
    "    'bet_btw_19_5',\n",
    "    'bet_btw_15_4',\n",
    "    'bet_btw_15_5'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1 2014 XGBoost 9\n",
      "D1 2014 NB 9\n",
      "D1 2014 MLP 9\n",
      "D1 2015 XGBoost 9\n",
      "D1 2015 NB 9\n",
      "D1 2015 MLP 9\n",
      "D1 2016 XGBoost 9\n",
      "D1 2016 NB 9\n",
      "D1 2016 MLP 9\n"
     ]
    }
   ],
   "source": [
    "for season in season_list:\n",
    "    \n",
    "    # Prepare the layer 1\n",
    "    df, df_test, X, y, X_test_season, y_test_season, le = get_dataset(league, season, False, 9, all_features)\n",
    "    cols = get_layer_columns(base_layer, classes)\n",
    "    X_train_layer1, X_test_season_layer1 = get_layer1_df(X, X_test_season, base_layer, cols)\n",
    "    \n",
    "    # Process base layer\n",
    "    for clf_name, preprocess, calibration, historical_training_year, classifier, features in base_layer:\n",
    "        print league,str(season),clf_name,str(historical_training_year)\n",
    "        \n",
    "        #Get the dataset\n",
    "        df, df_test, X, y, X_test_season, y_test_season, le = get_dataset(league, season, calibration, historical_training_year, features[1])\n",
    "        \n",
    "        # Check if we need to recalibrate the prediction\n",
    "        if calibration == 'sigmoid':\n",
    "            clf = CalibratedClassifierCV(classifier, cv=4, method='sigmoid')\n",
    "        elif calibration == 'isotonic':\n",
    "            clf = CalibratedClassifierCV(classifier, cv=4, method='isotonic')\n",
    "        elif calibration == 'no':\n",
    "            clf = classifier\n",
    "        \n",
    "        # Calibrate the model\n",
    "        clf.fit(X, y)\n",
    "        \n",
    "        # Predict probabilities of the test season\n",
    "        y_probs_train_layer1 = clf.predict_proba(X)\n",
    "        y_probs_test_season_layer1 = clf.predict_proba(X_test_season)\n",
    "        \n",
    "        # Add this model prediction to layer 1 dataset\n",
    "        X_train_layer1.loc[:, [clf_name+result for result in classes]] = y_probs_train_layer1\n",
    "        X_test_season_layer1.loc[:, [clf_name+result for result in classes]] = y_probs_test_season_layer1\n",
    "    \n",
    "    #Process Layer 1\n",
    "    clf_1 = LogisticRegression(\n",
    "        C=best_params['C'],\n",
    "        penalty=best_params['penalty'],\n",
    "        class_weight=best_params['class_weight'],\n",
    "        solver=best_params['solver'],\n",
    "        max_iter=best_params['max_iter'],\n",
    "        multi_class=best_params['multi_class'],\n",
    "    )\n",
    "    clf_1.fit(X_train_layer1, y)\n",
    "    # Predict target values\n",
    "    y_pred = clf_1.predict(X_test_season_layer1)\n",
    "    # Predict probabilities\n",
    "    y_probs = clf_1.predict_proba(X_test_season_layer1)\n",
    "    # get scores\n",
    "    score_dict = get_score(y_test_season, y_pred, y_probs)\n",
    "    # get money earned\n",
    "    money_dict = get_money(df_test, y_pred, y_probs)\n",
    "    # Add all info to result dataframe\n",
    "    result_df.loc[len(result_df.index)] = [\n",
    "        league, \n",
    "        season, \n",
    "        historical_training_year, \n",
    "        best_params['C'],\n",
    "        best_params['penalty'],\n",
    "        best_params['class_weight'],\n",
    "        best_params['solver'],\n",
    "        best_params['max_iter'],\n",
    "        score_dict['ll'],\n",
    "        score_dict['acc'],\n",
    "        score_dict['prec_A'],\n",
    "        score_dict['prec_D'],\n",
    "        score_dict['prec_H'],\n",
    "        score_dict['rec_A'],\n",
    "        score_dict['rec_D'],\n",
    "        score_dict['rec_H'],\n",
    "        score_dict['f1_A'],\n",
    "        score_dict['f1_D'],\n",
    "        score_dict['f1_H'],\n",
    "        money_dict['bet_all'],\n",
    "        money_dict['bet_lte_19'],\n",
    "        money_dict['bet_lte_4'],\n",
    "        money_dict['bet_btw_19_4'],\n",
    "        money_dict['bet_btw_19_5'],\n",
    "        money_dict['bet_btw_15_4'],\n",
    "        money_dict['bet_btw_15_5']\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df.to_csv('./report/STACKING_1_FTR_D1_BEST_HYPERPARAM_LOG_LOSS-VALIDATION.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final result\n",
    "Best is with ??? and ??? years of history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>league</th>\n",
       "      <th>season</th>\n",
       "      <th>historical_training_year</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>solver</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>ll</th>\n",
       "      <th>acc</th>\n",
       "      <th>...</th>\n",
       "      <th>f1_A</th>\n",
       "      <th>f1_D</th>\n",
       "      <th>f1_H</th>\n",
       "      <th>bet_all</th>\n",
       "      <th>bet_lte_19</th>\n",
       "      <th>bet_lte_4</th>\n",
       "      <th>bet_btw_19_4</th>\n",
       "      <th>bet_btw_19_5</th>\n",
       "      <th>bet_btw_15_4</th>\n",
       "      <th>bet_btw_15_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>8.291</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "      <td>sag</td>\n",
       "      <td>270</td>\n",
       "      <td>1.342474</td>\n",
       "      <td>0.456349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419580</td>\n",
       "      <td>0.206186</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>-0.005833</td>\n",
       "      <td>-0.060244</td>\n",
       "      <td>-0.008966</td>\n",
       "      <td>0.025906</td>\n",
       "      <td>0.031739</td>\n",
       "      <td>0.015351</td>\n",
       "      <td>0.020761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>8.291</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "      <td>sag</td>\n",
       "      <td>270</td>\n",
       "      <td>1.455315</td>\n",
       "      <td>0.473251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.288660</td>\n",
       "      <td>0.572614</td>\n",
       "      <td>0.016914</td>\n",
       "      <td>-0.015060</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.012153</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>-0.005580</td>\n",
       "      <td>0.005521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D1</td>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>8.291</td>\n",
       "      <td>l2</td>\n",
       "      <td>None</td>\n",
       "      <td>sag</td>\n",
       "      <td>270</td>\n",
       "      <td>1.542562</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316456</td>\n",
       "      <td>0.192982</td>\n",
       "      <td>0.443548</td>\n",
       "      <td>-0.202308</td>\n",
       "      <td>-0.326528</td>\n",
       "      <td>-0.113829</td>\n",
       "      <td>-0.011733</td>\n",
       "      <td>-0.090552</td>\n",
       "      <td>-0.095319</td>\n",
       "      <td>-0.153831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  league season historical_training_year      C penalty class_weight solver  \\\n",
       "0     D1   2014                        9  8.291      l2         None    sag   \n",
       "1     D1   2015                        9  8.291      l2         None    sag   \n",
       "2     D1   2016                        9  8.291      l2         None    sag   \n",
       "\n",
       "  max_iter        ll       acc      ...           f1_A      f1_D      f1_H  \\\n",
       "0      270  1.342474  0.456349      ...       0.419580  0.206186  0.568182   \n",
       "1      270  1.455315  0.473251      ...       0.432432  0.288660  0.572614   \n",
       "2      270  1.542562  0.350000      ...       0.316456  0.192982  0.443548   \n",
       "\n",
       "    bet_all  bet_lte_19  bet_lte_4  bet_btw_19_4  bet_btw_19_5  bet_btw_15_4  \\\n",
       "0 -0.005833   -0.060244  -0.008966      0.025906      0.031739      0.015351   \n",
       "1  0.016914   -0.015060   0.002203      0.012153      0.024645     -0.005580   \n",
       "2 -0.202308   -0.326528  -0.113829     -0.011733     -0.090552     -0.095319   \n",
       "\n",
       "   bet_btw_15_5  \n",
       "0      0.020761  \n",
       "1      0.005521  \n",
       "2     -0.153831  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XGBoostA</th>\n",
       "      <th>XGBoostD</th>\n",
       "      <th>XGBoostH</th>\n",
       "      <th>NBA</th>\n",
       "      <th>NBD</th>\n",
       "      <th>NBH</th>\n",
       "      <th>MLPA</th>\n",
       "      <th>MLPD</th>\n",
       "      <th>MLPH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.279969</td>\n",
       "      <td>0.243720</td>\n",
       "      <td>0.476311</td>\n",
       "      <td>0.144821</td>\n",
       "      <td>0.260931</td>\n",
       "      <td>0.594248</td>\n",
       "      <td>0.306964</td>\n",
       "      <td>0.256063</td>\n",
       "      <td>0.436973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.432669</td>\n",
       "      <td>0.248423</td>\n",
       "      <td>0.318907</td>\n",
       "      <td>0.938504</td>\n",
       "      <td>0.055820</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>0.445320</td>\n",
       "      <td>0.238716</td>\n",
       "      <td>0.315964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.439778</td>\n",
       "      <td>0.265275</td>\n",
       "      <td>0.294947</td>\n",
       "      <td>0.956420</td>\n",
       "      <td>0.042407</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.460049</td>\n",
       "      <td>0.260201</td>\n",
       "      <td>0.279750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.234580</td>\n",
       "      <td>0.327127</td>\n",
       "      <td>0.438293</td>\n",
       "      <td>0.080766</td>\n",
       "      <td>0.268106</td>\n",
       "      <td>0.651128</td>\n",
       "      <td>0.181941</td>\n",
       "      <td>0.226449</td>\n",
       "      <td>0.591610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.255676</td>\n",
       "      <td>0.241452</td>\n",
       "      <td>0.502872</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.999493</td>\n",
       "      <td>0.143772</td>\n",
       "      <td>0.176581</td>\n",
       "      <td>0.679647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.275133</td>\n",
       "      <td>0.240479</td>\n",
       "      <td>0.484389</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>0.029953</td>\n",
       "      <td>0.966100</td>\n",
       "      <td>0.236894</td>\n",
       "      <td>0.221570</td>\n",
       "      <td>0.541536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.284215</td>\n",
       "      <td>0.287574</td>\n",
       "      <td>0.428211</td>\n",
       "      <td>0.300224</td>\n",
       "      <td>0.589584</td>\n",
       "      <td>0.110192</td>\n",
       "      <td>0.371066</td>\n",
       "      <td>0.284059</td>\n",
       "      <td>0.344875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.164030</td>\n",
       "      <td>0.193751</td>\n",
       "      <td>0.642219</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.999369</td>\n",
       "      <td>0.171993</td>\n",
       "      <td>0.183573</td>\n",
       "      <td>0.644434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.315732</td>\n",
       "      <td>0.272952</td>\n",
       "      <td>0.411315</td>\n",
       "      <td>0.418144</td>\n",
       "      <td>0.322031</td>\n",
       "      <td>0.259825</td>\n",
       "      <td>0.332488</td>\n",
       "      <td>0.247499</td>\n",
       "      <td>0.420013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.328320</td>\n",
       "      <td>0.278061</td>\n",
       "      <td>0.393619</td>\n",
       "      <td>0.371676</td>\n",
       "      <td>0.518101</td>\n",
       "      <td>0.110223</td>\n",
       "      <td>0.358275</td>\n",
       "      <td>0.274912</td>\n",
       "      <td>0.366813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.445461</td>\n",
       "      <td>0.277148</td>\n",
       "      <td>0.277391</td>\n",
       "      <td>0.988160</td>\n",
       "      <td>0.011701</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.502995</td>\n",
       "      <td>0.240948</td>\n",
       "      <td>0.256057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.253319</td>\n",
       "      <td>0.241795</td>\n",
       "      <td>0.504886</td>\n",
       "      <td>0.789918</td>\n",
       "      <td>0.092065</td>\n",
       "      <td>0.118017</td>\n",
       "      <td>0.317978</td>\n",
       "      <td>0.227986</td>\n",
       "      <td>0.454036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.335064</td>\n",
       "      <td>0.396986</td>\n",
       "      <td>0.267949</td>\n",
       "      <td>0.986718</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.479347</td>\n",
       "      <td>0.249916</td>\n",
       "      <td>0.270738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.247686</td>\n",
       "      <td>0.313734</td>\n",
       "      <td>0.438580</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.016566</td>\n",
       "      <td>0.981793</td>\n",
       "      <td>0.199541</td>\n",
       "      <td>0.219975</td>\n",
       "      <td>0.580485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.299722</td>\n",
       "      <td>0.269067</td>\n",
       "      <td>0.431211</td>\n",
       "      <td>0.343539</td>\n",
       "      <td>0.513204</td>\n",
       "      <td>0.143257</td>\n",
       "      <td>0.354840</td>\n",
       "      <td>0.286715</td>\n",
       "      <td>0.358444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.203386</td>\n",
       "      <td>0.198462</td>\n",
       "      <td>0.598152</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.011020</td>\n",
       "      <td>0.987226</td>\n",
       "      <td>0.209811</td>\n",
       "      <td>0.213020</td>\n",
       "      <td>0.577170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.315508</td>\n",
       "      <td>0.362139</td>\n",
       "      <td>0.322353</td>\n",
       "      <td>0.358652</td>\n",
       "      <td>0.480015</td>\n",
       "      <td>0.161332</td>\n",
       "      <td>0.341809</td>\n",
       "      <td>0.262727</td>\n",
       "      <td>0.395464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.263025</td>\n",
       "      <td>0.337302</td>\n",
       "      <td>0.399673</td>\n",
       "      <td>0.088241</td>\n",
       "      <td>0.316111</td>\n",
       "      <td>0.595649</td>\n",
       "      <td>0.193819</td>\n",
       "      <td>0.241576</td>\n",
       "      <td>0.564606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.284626</td>\n",
       "      <td>0.318858</td>\n",
       "      <td>0.396516</td>\n",
       "      <td>0.286980</td>\n",
       "      <td>0.367459</td>\n",
       "      <td>0.345561</td>\n",
       "      <td>0.324375</td>\n",
       "      <td>0.248370</td>\n",
       "      <td>0.427256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.255841</td>\n",
       "      <td>0.327899</td>\n",
       "      <td>0.416260</td>\n",
       "      <td>0.113930</td>\n",
       "      <td>0.447552</td>\n",
       "      <td>0.438518</td>\n",
       "      <td>0.233962</td>\n",
       "      <td>0.251556</td>\n",
       "      <td>0.514482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.246894</td>\n",
       "      <td>0.425828</td>\n",
       "      <td>0.327278</td>\n",
       "      <td>0.513494</td>\n",
       "      <td>0.383758</td>\n",
       "      <td>0.102747</td>\n",
       "      <td>0.364117</td>\n",
       "      <td>0.276236</td>\n",
       "      <td>0.359647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.277024</td>\n",
       "      <td>0.231087</td>\n",
       "      <td>0.491889</td>\n",
       "      <td>0.209163</td>\n",
       "      <td>0.333376</td>\n",
       "      <td>0.457460</td>\n",
       "      <td>0.304041</td>\n",
       "      <td>0.240378</td>\n",
       "      <td>0.455581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.206349</td>\n",
       "      <td>0.296840</td>\n",
       "      <td>0.496810</td>\n",
       "      <td>0.006102</td>\n",
       "      <td>0.014645</td>\n",
       "      <td>0.979253</td>\n",
       "      <td>0.240279</td>\n",
       "      <td>0.210889</td>\n",
       "      <td>0.548832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.303482</td>\n",
       "      <td>0.266038</td>\n",
       "      <td>0.430480</td>\n",
       "      <td>0.369994</td>\n",
       "      <td>0.289684</td>\n",
       "      <td>0.340322</td>\n",
       "      <td>0.325885</td>\n",
       "      <td>0.244550</td>\n",
       "      <td>0.429565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.293066</td>\n",
       "      <td>0.244406</td>\n",
       "      <td>0.462528</td>\n",
       "      <td>0.414910</td>\n",
       "      <td>0.467712</td>\n",
       "      <td>0.117378</td>\n",
       "      <td>0.367880</td>\n",
       "      <td>0.262479</td>\n",
       "      <td>0.369642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.377864</td>\n",
       "      <td>0.352401</td>\n",
       "      <td>0.269735</td>\n",
       "      <td>0.969561</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.475686</td>\n",
       "      <td>0.244787</td>\n",
       "      <td>0.279526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.509353</td>\n",
       "      <td>0.270479</td>\n",
       "      <td>0.220168</td>\n",
       "      <td>0.977487</td>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.516844</td>\n",
       "      <td>0.246376</td>\n",
       "      <td>0.236781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.357691</td>\n",
       "      <td>0.256916</td>\n",
       "      <td>0.385392</td>\n",
       "      <td>0.947628</td>\n",
       "      <td>0.049328</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.416474</td>\n",
       "      <td>0.251431</td>\n",
       "      <td>0.332094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.190215</td>\n",
       "      <td>0.252432</td>\n",
       "      <td>0.557353</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.154699</td>\n",
       "      <td>0.178875</td>\n",
       "      <td>0.666426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.234881</td>\n",
       "      <td>0.249851</td>\n",
       "      <td>0.515268</td>\n",
       "      <td>0.018129</td>\n",
       "      <td>0.130385</td>\n",
       "      <td>0.851486</td>\n",
       "      <td>0.224384</td>\n",
       "      <td>0.226193</td>\n",
       "      <td>0.549423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>0.298115</td>\n",
       "      <td>0.291238</td>\n",
       "      <td>0.410647</td>\n",
       "      <td>0.186024</td>\n",
       "      <td>0.435979</td>\n",
       "      <td>0.377997</td>\n",
       "      <td>0.312394</td>\n",
       "      <td>0.257603</td>\n",
       "      <td>0.430002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>0.257599</td>\n",
       "      <td>0.264858</td>\n",
       "      <td>0.477543</td>\n",
       "      <td>0.068356</td>\n",
       "      <td>0.258556</td>\n",
       "      <td>0.673088</td>\n",
       "      <td>0.301052</td>\n",
       "      <td>0.264556</td>\n",
       "      <td>0.434392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>0.169832</td>\n",
       "      <td>0.166591</td>\n",
       "      <td>0.663577</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>0.144992</td>\n",
       "      <td>0.182766</td>\n",
       "      <td>0.672242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>0.303973</td>\n",
       "      <td>0.322276</td>\n",
       "      <td>0.373751</td>\n",
       "      <td>0.480862</td>\n",
       "      <td>0.433946</td>\n",
       "      <td>0.085192</td>\n",
       "      <td>0.380082</td>\n",
       "      <td>0.273804</td>\n",
       "      <td>0.346114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188</th>\n",
       "      <td>0.241275</td>\n",
       "      <td>0.274224</td>\n",
       "      <td>0.484500</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.999893</td>\n",
       "      <td>0.163708</td>\n",
       "      <td>0.170482</td>\n",
       "      <td>0.665810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>0.293426</td>\n",
       "      <td>0.310174</td>\n",
       "      <td>0.396400</td>\n",
       "      <td>0.137162</td>\n",
       "      <td>0.526503</td>\n",
       "      <td>0.336335</td>\n",
       "      <td>0.238899</td>\n",
       "      <td>0.271880</td>\n",
       "      <td>0.489221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>0.161879</td>\n",
       "      <td>0.161675</td>\n",
       "      <td>0.676446</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.012487</td>\n",
       "      <td>0.984796</td>\n",
       "      <td>0.218245</td>\n",
       "      <td>0.192313</td>\n",
       "      <td>0.589442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>0.332681</td>\n",
       "      <td>0.271785</td>\n",
       "      <td>0.395534</td>\n",
       "      <td>0.238517</td>\n",
       "      <td>0.607189</td>\n",
       "      <td>0.154295</td>\n",
       "      <td>0.333902</td>\n",
       "      <td>0.267593</td>\n",
       "      <td>0.398505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>0.240391</td>\n",
       "      <td>0.257538</td>\n",
       "      <td>0.502071</td>\n",
       "      <td>0.275036</td>\n",
       "      <td>0.311994</td>\n",
       "      <td>0.412970</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>0.239918</td>\n",
       "      <td>0.466481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193</th>\n",
       "      <td>0.251840</td>\n",
       "      <td>0.259404</td>\n",
       "      <td>0.488756</td>\n",
       "      <td>0.033504</td>\n",
       "      <td>0.255167</td>\n",
       "      <td>0.711329</td>\n",
       "      <td>0.211790</td>\n",
       "      <td>0.234807</td>\n",
       "      <td>0.553404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2194</th>\n",
       "      <td>0.252822</td>\n",
       "      <td>0.318614</td>\n",
       "      <td>0.428565</td>\n",
       "      <td>0.043346</td>\n",
       "      <td>0.240230</td>\n",
       "      <td>0.716425</td>\n",
       "      <td>0.235277</td>\n",
       "      <td>0.250752</td>\n",
       "      <td>0.513971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>0.236266</td>\n",
       "      <td>0.220392</td>\n",
       "      <td>0.543343</td>\n",
       "      <td>0.051325</td>\n",
       "      <td>0.156834</td>\n",
       "      <td>0.791841</td>\n",
       "      <td>0.271037</td>\n",
       "      <td>0.240616</td>\n",
       "      <td>0.488347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>0.403086</td>\n",
       "      <td>0.280637</td>\n",
       "      <td>0.316277</td>\n",
       "      <td>0.956532</td>\n",
       "      <td>0.038460</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>0.440104</td>\n",
       "      <td>0.249120</td>\n",
       "      <td>0.310776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>0.242951</td>\n",
       "      <td>0.324894</td>\n",
       "      <td>0.432155</td>\n",
       "      <td>0.107808</td>\n",
       "      <td>0.540546</td>\n",
       "      <td>0.351645</td>\n",
       "      <td>0.282333</td>\n",
       "      <td>0.269513</td>\n",
       "      <td>0.448154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>0.317501</td>\n",
       "      <td>0.267064</td>\n",
       "      <td>0.415436</td>\n",
       "      <td>0.252530</td>\n",
       "      <td>0.390790</td>\n",
       "      <td>0.356680</td>\n",
       "      <td>0.310645</td>\n",
       "      <td>0.244314</td>\n",
       "      <td>0.445041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>0.286605</td>\n",
       "      <td>0.279049</td>\n",
       "      <td>0.434346</td>\n",
       "      <td>0.417142</td>\n",
       "      <td>0.462990</td>\n",
       "      <td>0.119868</td>\n",
       "      <td>0.324746</td>\n",
       "      <td>0.253263</td>\n",
       "      <td>0.421991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>0.276597</td>\n",
       "      <td>0.238588</td>\n",
       "      <td>0.484814</td>\n",
       "      <td>0.078668</td>\n",
       "      <td>0.306853</td>\n",
       "      <td>0.614480</td>\n",
       "      <td>0.283220</td>\n",
       "      <td>0.265017</td>\n",
       "      <td>0.451763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>0.511912</td>\n",
       "      <td>0.229510</td>\n",
       "      <td>0.258578</td>\n",
       "      <td>0.890850</td>\n",
       "      <td>0.083922</td>\n",
       "      <td>0.025228</td>\n",
       "      <td>0.369258</td>\n",
       "      <td>0.262658</td>\n",
       "      <td>0.368083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>0.184468</td>\n",
       "      <td>0.180429</td>\n",
       "      <td>0.635103</td>\n",
       "      <td>0.017176</td>\n",
       "      <td>0.009481</td>\n",
       "      <td>0.973344</td>\n",
       "      <td>0.239473</td>\n",
       "      <td>0.196858</td>\n",
       "      <td>0.563669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203</th>\n",
       "      <td>0.311507</td>\n",
       "      <td>0.321298</td>\n",
       "      <td>0.367195</td>\n",
       "      <td>0.176611</td>\n",
       "      <td>0.510425</td>\n",
       "      <td>0.312964</td>\n",
       "      <td>0.318600</td>\n",
       "      <td>0.262034</td>\n",
       "      <td>0.419366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>0.417334</td>\n",
       "      <td>0.255647</td>\n",
       "      <td>0.327019</td>\n",
       "      <td>0.543531</td>\n",
       "      <td>0.182719</td>\n",
       "      <td>0.273750</td>\n",
       "      <td>0.366181</td>\n",
       "      <td>0.258676</td>\n",
       "      <td>0.375143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>0.316874</td>\n",
       "      <td>0.281194</td>\n",
       "      <td>0.401933</td>\n",
       "      <td>0.112609</td>\n",
       "      <td>0.446693</td>\n",
       "      <td>0.440698</td>\n",
       "      <td>0.276634</td>\n",
       "      <td>0.266909</td>\n",
       "      <td>0.456458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>0.165851</td>\n",
       "      <td>0.179541</td>\n",
       "      <td>0.654608</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.005135</td>\n",
       "      <td>0.994698</td>\n",
       "      <td>0.166319</td>\n",
       "      <td>0.201630</td>\n",
       "      <td>0.632051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>0.382897</td>\n",
       "      <td>0.257500</td>\n",
       "      <td>0.359603</td>\n",
       "      <td>0.394799</td>\n",
       "      <td>0.466394</td>\n",
       "      <td>0.138807</td>\n",
       "      <td>0.322927</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.396773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>0.217100</td>\n",
       "      <td>0.279304</td>\n",
       "      <td>0.503596</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.008471</td>\n",
       "      <td>0.990634</td>\n",
       "      <td>0.182094</td>\n",
       "      <td>0.198796</td>\n",
       "      <td>0.619110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>0.328693</td>\n",
       "      <td>0.245277</td>\n",
       "      <td>0.426031</td>\n",
       "      <td>0.064991</td>\n",
       "      <td>0.282082</td>\n",
       "      <td>0.652927</td>\n",
       "      <td>0.273037</td>\n",
       "      <td>0.241846</td>\n",
       "      <td>0.485117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>0.212183</td>\n",
       "      <td>0.225518</td>\n",
       "      <td>0.562299</td>\n",
       "      <td>0.227521</td>\n",
       "      <td>0.231462</td>\n",
       "      <td>0.541017</td>\n",
       "      <td>0.283153</td>\n",
       "      <td>0.236524</td>\n",
       "      <td>0.480324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2211</th>\n",
       "      <td>0.258600</td>\n",
       "      <td>0.298005</td>\n",
       "      <td>0.443395</td>\n",
       "      <td>0.057386</td>\n",
       "      <td>0.308125</td>\n",
       "      <td>0.634489</td>\n",
       "      <td>0.230701</td>\n",
       "      <td>0.252946</td>\n",
       "      <td>0.516353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2212</th>\n",
       "      <td>0.181685</td>\n",
       "      <td>0.233630</td>\n",
       "      <td>0.584684</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.024783</td>\n",
       "      <td>0.972972</td>\n",
       "      <td>0.172696</td>\n",
       "      <td>0.216079</td>\n",
       "      <td>0.611225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2213</th>\n",
       "      <td>0.299779</td>\n",
       "      <td>0.240988</td>\n",
       "      <td>0.459233</td>\n",
       "      <td>0.323699</td>\n",
       "      <td>0.383534</td>\n",
       "      <td>0.292767</td>\n",
       "      <td>0.313710</td>\n",
       "      <td>0.261860</td>\n",
       "      <td>0.424431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2214 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      XGBoostA  XGBoostD  XGBoostH       NBA       NBD       NBH      MLPA  \\\n",
       "0     0.279969  0.243720  0.476311  0.144821  0.260931  0.594248  0.306964   \n",
       "1     0.432669  0.248423  0.318907  0.938504  0.055820  0.005676  0.445320   \n",
       "2     0.439778  0.265275  0.294947  0.956420  0.042407  0.001173  0.460049   \n",
       "3     0.234580  0.327127  0.438293  0.080766  0.268106  0.651128  0.181941   \n",
       "4     0.255676  0.241452  0.502872  0.000013  0.000494  0.999493  0.143772   \n",
       "5     0.275133  0.240479  0.484389  0.003947  0.029953  0.966100  0.236894   \n",
       "6     0.284215  0.287574  0.428211  0.300224  0.589584  0.110192  0.371066   \n",
       "7     0.164030  0.193751  0.642219  0.000047  0.000584  0.999369  0.171993   \n",
       "8     0.315732  0.272952  0.411315  0.418144  0.322031  0.259825  0.332488   \n",
       "9     0.328320  0.278061  0.393619  0.371676  0.518101  0.110223  0.358275   \n",
       "10    0.445461  0.277148  0.277391  0.988160  0.011701  0.000139  0.502995   \n",
       "11    0.253319  0.241795  0.504886  0.789918  0.092065  0.118017  0.317978   \n",
       "12    0.335064  0.396986  0.267949  0.986718  0.013090  0.000192  0.479347   \n",
       "13    0.247686  0.313734  0.438580  0.001642  0.016566  0.981793  0.199541   \n",
       "14    0.299722  0.269067  0.431211  0.343539  0.513204  0.143257  0.354840   \n",
       "15    0.203386  0.198462  0.598152  0.001754  0.011020  0.987226  0.209811   \n",
       "16    0.315508  0.362139  0.322353  0.358652  0.480015  0.161332  0.341809   \n",
       "17    0.263025  0.337302  0.399673  0.088241  0.316111  0.595649  0.193819   \n",
       "18    0.284626  0.318858  0.396516  0.286980  0.367459  0.345561  0.324375   \n",
       "19    0.255841  0.327899  0.416260  0.113930  0.447552  0.438518  0.233962   \n",
       "20    0.246894  0.425828  0.327278  0.513494  0.383758  0.102747  0.364117   \n",
       "21    0.277024  0.231087  0.491889  0.209163  0.333376  0.457460  0.304041   \n",
       "22    0.206349  0.296840  0.496810  0.006102  0.014645  0.979253  0.240279   \n",
       "23    0.303482  0.266038  0.430480  0.369994  0.289684  0.340322  0.325885   \n",
       "24    0.293066  0.244406  0.462528  0.414910  0.467712  0.117378  0.367880   \n",
       "25    0.377864  0.352401  0.269735  0.969561  0.029300  0.001139  0.475686   \n",
       "26    0.509353  0.270479  0.220168  0.977487  0.022354  0.000159  0.516844   \n",
       "27    0.357691  0.256916  0.385392  0.947628  0.049328  0.003044  0.416474   \n",
       "28    0.190215  0.252432  0.557353  0.000010  0.000289  0.999700  0.154699   \n",
       "29    0.234881  0.249851  0.515268  0.018129  0.130385  0.851486  0.224384   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2184  0.298115  0.291238  0.410647  0.186024  0.435979  0.377997  0.312394   \n",
       "2185  0.257599  0.264858  0.477543  0.068356  0.258556  0.673088  0.301052   \n",
       "2186  0.169832  0.166591  0.663577  0.000055  0.001168  0.998777  0.144992   \n",
       "2187  0.303973  0.322276  0.373751  0.480862  0.433946  0.085192  0.380082   \n",
       "2188  0.241275  0.274224  0.484500  0.000004  0.000102  0.999893  0.163708   \n",
       "2189  0.293426  0.310174  0.396400  0.137162  0.526503  0.336335  0.238899   \n",
       "2190  0.161879  0.161675  0.676446  0.002716  0.012487  0.984796  0.218245   \n",
       "2191  0.332681  0.271785  0.395534  0.238517  0.607189  0.154295  0.333902   \n",
       "2192  0.240391  0.257538  0.502071  0.275036  0.311994  0.412970  0.293600   \n",
       "2193  0.251840  0.259404  0.488756  0.033504  0.255167  0.711329  0.211790   \n",
       "2194  0.252822  0.318614  0.428565  0.043346  0.240230  0.716425  0.235277   \n",
       "2195  0.236266  0.220392  0.543343  0.051325  0.156834  0.791841  0.271037   \n",
       "2196  0.403086  0.280637  0.316277  0.956532  0.038460  0.005009  0.440104   \n",
       "2197  0.242951  0.324894  0.432155  0.107808  0.540546  0.351645  0.282333   \n",
       "2198  0.317501  0.267064  0.415436  0.252530  0.390790  0.356680  0.310645   \n",
       "2199  0.286605  0.279049  0.434346  0.417142  0.462990  0.119868  0.324746   \n",
       "2200  0.276597  0.238588  0.484814  0.078668  0.306853  0.614480  0.283220   \n",
       "2201  0.511912  0.229510  0.258578  0.890850  0.083922  0.025228  0.369258   \n",
       "2202  0.184468  0.180429  0.635103  0.017176  0.009481  0.973344  0.239473   \n",
       "2203  0.311507  0.321298  0.367195  0.176611  0.510425  0.312964  0.318600   \n",
       "2204  0.417334  0.255647  0.327019  0.543531  0.182719  0.273750  0.366181   \n",
       "2205  0.316874  0.281194  0.401933  0.112609  0.446693  0.440698  0.276634   \n",
       "2206  0.165851  0.179541  0.654608  0.000167  0.005135  0.994698  0.166319   \n",
       "2207  0.382897  0.257500  0.359603  0.394799  0.466394  0.138807  0.322927   \n",
       "2208  0.217100  0.279304  0.503596  0.000895  0.008471  0.990634  0.182094   \n",
       "2209  0.328693  0.245277  0.426031  0.064991  0.282082  0.652927  0.273037   \n",
       "2210  0.212183  0.225518  0.562299  0.227521  0.231462  0.541017  0.283153   \n",
       "2211  0.258600  0.298005  0.443395  0.057386  0.308125  0.634489  0.230701   \n",
       "2212  0.181685  0.233630  0.584684  0.002245  0.024783  0.972972  0.172696   \n",
       "2213  0.299779  0.240988  0.459233  0.323699  0.383534  0.292767  0.313710   \n",
       "\n",
       "          MLPD      MLPH  \n",
       "0     0.256063  0.436973  \n",
       "1     0.238716  0.315964  \n",
       "2     0.260201  0.279750  \n",
       "3     0.226449  0.591610  \n",
       "4     0.176581  0.679647  \n",
       "5     0.221570  0.541536  \n",
       "6     0.284059  0.344875  \n",
       "7     0.183573  0.644434  \n",
       "8     0.247499  0.420013  \n",
       "9     0.274912  0.366813  \n",
       "10    0.240948  0.256057  \n",
       "11    0.227986  0.454036  \n",
       "12    0.249916  0.270738  \n",
       "13    0.219975  0.580485  \n",
       "14    0.286715  0.358444  \n",
       "15    0.213020  0.577170  \n",
       "16    0.262727  0.395464  \n",
       "17    0.241576  0.564606  \n",
       "18    0.248370  0.427256  \n",
       "19    0.251556  0.514482  \n",
       "20    0.276236  0.359647  \n",
       "21    0.240378  0.455581  \n",
       "22    0.210889  0.548832  \n",
       "23    0.244550  0.429565  \n",
       "24    0.262479  0.369642  \n",
       "25    0.244787  0.279526  \n",
       "26    0.246376  0.236781  \n",
       "27    0.251431  0.332094  \n",
       "28    0.178875  0.666426  \n",
       "29    0.226193  0.549423  \n",
       "...        ...       ...  \n",
       "2184  0.257603  0.430002  \n",
       "2185  0.264556  0.434392  \n",
       "2186  0.182766  0.672242  \n",
       "2187  0.273804  0.346114  \n",
       "2188  0.170482  0.665810  \n",
       "2189  0.271880  0.489221  \n",
       "2190  0.192313  0.589442  \n",
       "2191  0.267593  0.398505  \n",
       "2192  0.239918  0.466481  \n",
       "2193  0.234807  0.553404  \n",
       "2194  0.250752  0.513971  \n",
       "2195  0.240616  0.488347  \n",
       "2196  0.249120  0.310776  \n",
       "2197  0.269513  0.448154  \n",
       "2198  0.244314  0.445041  \n",
       "2199  0.253263  0.421991  \n",
       "2200  0.265017  0.451763  \n",
       "2201  0.262658  0.368083  \n",
       "2202  0.196858  0.563669  \n",
       "2203  0.262034  0.419366  \n",
       "2204  0.258676  0.375143  \n",
       "2205  0.266909  0.456458  \n",
       "2206  0.201630  0.632051  \n",
       "2207  0.280300  0.396773  \n",
       "2208  0.198796  0.619110  \n",
       "2209  0.241846  0.485117  \n",
       "2210  0.236524  0.480324  \n",
       "2211  0.252946  0.516353  \n",
       "2212  0.216079  0.611225  \n",
       "2213  0.261860  0.424431  \n",
       "\n",
       "[2214 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
