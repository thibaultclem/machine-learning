{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the library\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "import matplotlib.pyplot as plt # plotting library\n",
    "import seaborn as sns # visualization library based on matplotlib\n",
    "from IPython.display import display # Manage multiple output per cell\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "league = 'E0'\n",
    "season = 2016\n",
    "historical_training_year = 9\n",
    "model_name = 'XGBoost_FTR_'+league+'_BEST_PARAM_LOG_LOSS'\n",
    "odd_H = 'INFO_BbAvH'\n",
    "odd_A = 'INFO_BbAvA'\n",
    "odd_D = 'INFO_BbAvD'\n",
    "target = 'INFO_FTR'\n",
    "start_date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features = [\"A_MEANS_FIVE_AC\",\"A_MEANS_FIVE_AF\",\"A_MEANS_FIVE_AR\",\"A_MEANS_FIVE_AS\",\"A_MEANS_FIVE_AST\",\"A_MEANS_FIVE_AY\",\"A_MEANS_FIVE_FTAG\",\"A_MEANS_FIVE_FTHG\",\"A_MEANS_FIVE_FTR_A\",\"A_MEANS_FIVE_FTR_D\",\"A_MEANS_FIVE_FTR_H\",\"A_MEANS_FIVE_HC\",\"A_MEANS_FIVE_HF\",\"A_MEANS_FIVE_HR\",\"A_MEANS_FIVE_HS\",\"A_MEANS_FIVE_HST\",\"A_MEANS_FIVE_HTAG\",\"A_MEANS_FIVE_HTHG\",\"A_MEANS_FIVE_HTR_A\",\"A_MEANS_FIVE_HTR_D\",\"A_MEANS_FIVE_HTR_H\",\"A_MEANS_FIVE_HY\",\"H_MEANS_FIVE_AC\",\"H_MEANS_FIVE_AF\",\"H_MEANS_FIVE_AR\",\"H_MEANS_FIVE_AS\",\"H_MEANS_FIVE_AST\",\"H_MEANS_FIVE_AY\",\"H_MEANS_FIVE_FTAG\",\"H_MEANS_FIVE_FTHG\",\"H_MEANS_FIVE_FTR_A\",\"H_MEANS_FIVE_FTR_D\",\"H_MEANS_FIVE_FTR_H\",\"H_MEANS_FIVE_HC\",\"H_MEANS_FIVE_HF\",\"H_MEANS_FIVE_HR\",\"H_MEANS_FIVE_HS\",\"H_MEANS_FIVE_HST\",\"H_MEANS_FIVE_HTAG\",\"H_MEANS_FIVE_HTHG\",\"H_MEANS_FIVE_HTR_A\",\"H_MEANS_FIVE_HTR_D\",\"H_MEANS_FIVE_HTR_H\",\"H_MEANS_FIVE_HY\",\"A_MEANS_THREE_AC\",\"A_MEANS_THREE_AF\",\"A_MEANS_THREE_AR\",\"A_MEANS_THREE_AS\",\"A_MEANS_THREE_AST\",\"A_MEANS_THREE_AY\",\"A_MEANS_THREE_FTAG\",\"A_MEANS_THREE_FTHG\",\"A_MEANS_THREE_FTR_A\",\"A_MEANS_THREE_FTR_D\",\"A_MEANS_THREE_FTR_H\",\"A_MEANS_THREE_HC\",\"A_MEANS_THREE_HF\",\"A_MEANS_THREE_HR\",\"A_MEANS_THREE_HS\",\"A_MEANS_THREE_HST\",\"A_MEANS_THREE_HTAG\",\"A_MEANS_THREE_HTHG\",\"A_MEANS_THREE_HTR_A\",\"A_MEANS_THREE_HTR_D\",\"A_MEANS_THREE_HTR_H\",\"A_MEANS_THREE_HY\",\"H_MEANS_THREE_AC\",\"H_MEANS_THREE_AF\",\"H_MEANS_THREE_AR\",\"H_MEANS_THREE_AS\",\"H_MEANS_THREE_AST\",\"H_MEANS_THREE_AY\",\"H_MEANS_THREE_FTAG\",\"H_MEANS_THREE_FTHG\",\"H_MEANS_THREE_FTR_A\",\"H_MEANS_THREE_FTR_D\",\"H_MEANS_THREE_FTR_H\",\"H_MEANS_THREE_HC\",\"H_MEANS_THREE_HF\",\"H_MEANS_THREE_HR\",\"H_MEANS_THREE_HS\",\"H_MEANS_THREE_HST\",\"H_MEANS_THREE_HTAG\",\"H_MEANS_THREE_HTHG\",\"H_MEANS_THREE_HTR_A\",\"H_MEANS_THREE_HTR_D\",\"H_MEANS_THREE_HTR_H\",\"H_MEANS_THREE_HY\",\"A_STD_FIVE_AC\",\"A_STD_FIVE_AF\",\"A_STD_FIVE_AR\",\"A_STD_FIVE_AS\",\"A_STD_FIVE_AST\",\"A_STD_FIVE_AY\",\"A_STD_FIVE_FTAG\",\"A_STD_FIVE_FTHG\",\"A_STD_FIVE_FTR_A\",\"A_STD_FIVE_FTR_D\",\"A_STD_FIVE_FTR_H\",\"A_STD_FIVE_HC\",\"A_STD_FIVE_HF\",\"A_STD_FIVE_HR\",\"A_STD_FIVE_HS\",\"A_STD_FIVE_HST\",\"A_STD_FIVE_HTAG\",\"A_STD_FIVE_HTHG\",\"A_STD_FIVE_HTR_A\",\"A_STD_FIVE_HTR_D\",\"A_STD_FIVE_HTR_H\",\"A_STD_FIVE_HY\",\"H_STD_FIVE_AC\",\"H_STD_FIVE_AF\",\"H_STD_FIVE_AR\",\"H_STD_FIVE_AS\",\"H_STD_FIVE_AST\",\"H_STD_FIVE_AY\",\"H_STD_FIVE_FTAG\",\"H_STD_FIVE_FTHG\",\"H_STD_FIVE_FTR_A\",\"H_STD_FIVE_FTR_D\",\"H_STD_FIVE_FTR_H\",\"H_STD_FIVE_HC\",\"H_STD_FIVE_HF\",\"H_STD_FIVE_HR\",\"H_STD_FIVE_HS\",\"H_STD_FIVE_HST\",\"H_STD_FIVE_HTAG\",\"H_STD_FIVE_HTHG\",\"H_STD_FIVE_HTR_A\",\"H_STD_FIVE_HTR_D\",\"H_STD_FIVE_HTR_H\",\"H_STD_FIVE_HY\",\"A_STD_THREE_AC\",\"A_STD_THREE_AF\",\"A_STD_THREE_AR\",\"A_STD_THREE_AS\",\"A_STD_THREE_AST\",\"A_STD_THREE_AY\",\"A_STD_THREE_FTAG\",\"A_STD_THREE_FTHG\",\"A_STD_THREE_FTR_A\",\"A_STD_THREE_FTR_D\",\"A_STD_THREE_FTR_H\",\"A_STD_THREE_HC\",\"A_STD_THREE_HF\",\"A_STD_THREE_HR\",\"A_STD_THREE_HS\",\"A_STD_THREE_HST\",\"A_STD_THREE_HTAG\",\"A_STD_THREE_HTHG\",\"A_STD_THREE_HTR_A\",\"A_STD_THREE_HTR_D\",\"A_STD_THREE_HTR_H\",\"A_STD_THREE_HY\",\"H_STD_THREE_AC\",\"H_STD_THREE_AF\",\"H_STD_THREE_AR\",\"H_STD_THREE_AS\",\"H_STD_THREE_AST\",\"H_STD_THREE_AY\",\"H_STD_THREE_FTAG\",\"H_STD_THREE_FTHG\",\"H_STD_THREE_FTR_A\",\"H_STD_THREE_FTR_D\",\"H_STD_THREE_FTR_H\",\"H_STD_THREE_HC\",\"H_STD_THREE_HF\",\"H_STD_THREE_HR\",\"H_STD_THREE_HS\",\"H_STD_THREE_HST\",\"H_STD_THREE_HTAG\",\"H_STD_THREE_HTHG\",\"H_STD_THREE_HTR_A\",\"H_STD_THREE_HTR_D\",\"H_STD_THREE_HTR_H\",\"H_STD_THREE_HY\"]\n",
    "features_list = all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DB Sqlite connection\n",
    "import sqlite3\n",
    "db = \"/Users/thibaultclement/Project/ligue1-predict/src/notebook/data/db/soccer_predict.sqlite\"\n",
    "conn = sqlite3.connect(db)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37907, 190)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all prematch data\n",
    "df = pd.read_sql_query(\"SELECT * FROM pre_matchs ORDER BY INFO_Date ASC;\", conn)\n",
    "df = (df[df.columns.drop(['index'])])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26988, 190)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all game between June (include) and October (include)\n",
    "df['INFO_Date'] = pd.to_datetime(df['INFO_Date'])\n",
    "df['INFO_Date'].dt.month\n",
    "df = df[(df['INFO_Date'].dt.month < 6) | (df['INFO_Date'].dt.month > 10)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a INFO_WIN column containing the gain if you bet the good result\n",
    "df['INFO_WIN'] = 0\n",
    "df.loc[df.INFO_FTR == 'H', 'INFO_WIN'] = df[odd_H]\n",
    "df.loc[df.INFO_FTR == 'A', 'INFO_WIN'] = df[odd_A]\n",
    "df.loc[df.INFO_FTR == 'D', 'INFO_WIN'] = df[odd_D]\n",
    "df['INFO_WIN_P'] = 0\n",
    "df.loc[df.INFO_FTR == 'H', 'INFO_WIN_P'] = df['INFO_PSH']\n",
    "df.loc[df.INFO_FTR == 'A', 'INFO_WIN_P'] = df['INFO_PSA']\n",
    "df.loc[df.INFO_FTR == 'D', 'INFO_WIN_P'] = df['INFO_PSD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select a particular league\n",
    "df = df[(df['INFO_Div'] == league)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep season for further test and don't use it for traning\n",
    "import datetime\n",
    "date_start_learn = datetime.date(season-historical_training_year, 8, 1)\n",
    "date_end_learn = datetime.date(season-1, 8, 1)\n",
    "date_start_current_season = datetime.date(season, 8, 1)\n",
    "date_end_current_season = datetime.date(season+1, 8, 1)\n",
    "df_current_season = df[(df['INFO_Date'] > date_start_current_season)]\n",
    "df_current_season = df_current_season[(df_current_season['INFO_Date'] < date_end_current_season)]\n",
    "df = df[(df['INFO_Date'] > date_start_learn)]\n",
    "df = df[(df['INFO_Date'] < date_end_learn)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode string label\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df[target])\n",
    "# Prepare the dataset\n",
    "X = pd.get_dummies(df[features_list])\n",
    "y = le.transform(df[target]) \n",
    "X_current_season = pd.get_dummies(df_current_season[features_list])\n",
    "y_current_season = le.transform(df_current_season[target]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 0)## Tuning Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import model\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "classifier = XGBClassifier(nthread=4, seed=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 30 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 13.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "798.8085589408875"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Randomized to find the best hyper-parameters for our Model\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics.classification import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "parameters = {\n",
    "    'learning_rate': [0.01],\n",
    "    'n_estimators': np.arange(100, 600, 30).tolist(),\n",
    "    'max_depth': np.arange(3, 9).tolist(),\n",
    "    'min_child_weight': np.arange(1, 9).tolist(),\n",
    "    'gamma': np.arange(0.01,1,0.03).tolist(),\n",
    "    'subsample': np.arange(0.5,1,0.05).tolist(),\n",
    "    'colsample_bytree': np.arange(0.5,1,0.05).tolist(),\n",
    "    'objective': ['multi:softprob'],\n",
    "    'scale_pos_weight': [1],\n",
    "    'reg_alpha':np.arange(0,1,0.03).tolist()\n",
    "}\n",
    "\n",
    "grid_search = RandomizedSearchCV(\n",
    "    estimator=classifier,\n",
    "    param_distributions=parameters,\n",
    "    #scoring=make_scorer(log_loss, greater_is_better=False, needs_proba=True),\n",
    "    scoring='accuracy',\n",
    "    cv=8,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    n_iter=30)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5393258426966292"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract best score calculated with the GridSearchCV\n",
    "best_score = grid_search.best_score_\n",
    "display(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.9000000000000004,\n",
       " 'gamma': 0.58,\n",
       " 'learning_rate': 0.01,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 7,\n",
       " 'n_estimators': 250,\n",
       " 'objective': 'multi:softprob',\n",
       " 'reg_alpha': 0.54,\n",
       " 'scale_pos_weight': 1,\n",
       " 'subsample': 0.6000000000000001}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract best hyper-parameter calculated with the GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>...</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.399557</td>\n",
       "      <td>0.012444</td>\n",
       "      <td>0.532459</td>\n",
       "      <td>0.565455</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.558488</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.565621</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.562678</td>\n",
       "      <td>0.046896</td>\n",
       "      <td>0.006879</td>\n",
       "      <td>0.015987</td>\n",
       "      <td>0.004647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.366827</td>\n",
       "      <td>0.010683</td>\n",
       "      <td>0.534956</td>\n",
       "      <td>0.633317</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.620542</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.636947</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>0.633903</td>\n",
       "      <td>0.086661</td>\n",
       "      <td>0.003277</td>\n",
       "      <td>0.012901</td>\n",
       "      <td>0.011217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.376678</td>\n",
       "      <td>0.016603</td>\n",
       "      <td>0.534956</td>\n",
       "      <td>0.632334</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.623395</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.633381</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.079614</td>\n",
       "      <td>0.006420</td>\n",
       "      <td>0.019003</td>\n",
       "      <td>0.005878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.275892</td>\n",
       "      <td>0.052117</td>\n",
       "      <td>0.531835</td>\n",
       "      <td>0.999287</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.998573</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.999288</td>\n",
       "      <td>0.128408</td>\n",
       "      <td>0.018777</td>\n",
       "      <td>0.016888</td>\n",
       "      <td>0.000798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.410967</td>\n",
       "      <td>0.015514</td>\n",
       "      <td>0.529963</td>\n",
       "      <td>0.623595</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.609130</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.616976</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.630342</td>\n",
       "      <td>0.068891</td>\n",
       "      <td>0.003975</td>\n",
       "      <td>0.016133</td>\n",
       "      <td>0.008738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28.857051</td>\n",
       "      <td>0.071036</td>\n",
       "      <td>0.529338</td>\n",
       "      <td>0.992955</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.993581</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.992867</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.993590</td>\n",
       "      <td>0.711842</td>\n",
       "      <td>0.041157</td>\n",
       "      <td>0.022888</td>\n",
       "      <td>0.000754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20.440096</td>\n",
       "      <td>0.047294</td>\n",
       "      <td>0.531835</td>\n",
       "      <td>0.973694</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.970756</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.975036</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.065160</td>\n",
       "      <td>0.024326</td>\n",
       "      <td>0.023233</td>\n",
       "      <td>0.001830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.712006</td>\n",
       "      <td>0.007858</td>\n",
       "      <td>0.533708</td>\n",
       "      <td>0.564474</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.559201</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.562054</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.566239</td>\n",
       "      <td>0.046309</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.017019</td>\n",
       "      <td>0.004590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.471233</td>\n",
       "      <td>0.046119</td>\n",
       "      <td>0.529963</td>\n",
       "      <td>0.960227</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.958631</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.965763</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>0.964387</td>\n",
       "      <td>0.146136</td>\n",
       "      <td>0.022489</td>\n",
       "      <td>0.021133</td>\n",
       "      <td>0.004012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.193141</td>\n",
       "      <td>0.019256</td>\n",
       "      <td>0.534956</td>\n",
       "      <td>0.645264</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.647646</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.641940</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.650997</td>\n",
       "      <td>0.050271</td>\n",
       "      <td>0.005951</td>\n",
       "      <td>0.019555</td>\n",
       "      <td>0.003615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23.673239</td>\n",
       "      <td>0.039312</td>\n",
       "      <td>0.529963</td>\n",
       "      <td>0.980827</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.977889</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.978602</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>0.984330</td>\n",
       "      <td>0.100150</td>\n",
       "      <td>0.017251</td>\n",
       "      <td>0.022990</td>\n",
       "      <td>0.002274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.265442</td>\n",
       "      <td>0.015792</td>\n",
       "      <td>0.534956</td>\n",
       "      <td>0.685925</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.671897</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.678317</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>0.702991</td>\n",
       "      <td>0.061780</td>\n",
       "      <td>0.008840</td>\n",
       "      <td>0.016489</td>\n",
       "      <td>0.011168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.221845</td>\n",
       "      <td>0.028247</td>\n",
       "      <td>0.536205</td>\n",
       "      <td>0.946316</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.945792</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.948645</td>\n",
       "      <td>0.550505</td>\n",
       "      <td>0.950142</td>\n",
       "      <td>0.048685</td>\n",
       "      <td>0.013472</td>\n",
       "      <td>0.021272</td>\n",
       "      <td>0.003760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.097931</td>\n",
       "      <td>0.013768</td>\n",
       "      <td>0.531835</td>\n",
       "      <td>0.596486</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.589158</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.592011</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>0.603276</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>0.012631</td>\n",
       "      <td>0.005801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.215882</td>\n",
       "      <td>0.019782</td>\n",
       "      <td>0.531835</td>\n",
       "      <td>0.686907</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.677603</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.686163</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.697293</td>\n",
       "      <td>0.064899</td>\n",
       "      <td>0.006002</td>\n",
       "      <td>0.016712</td>\n",
       "      <td>0.006268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21.872894</td>\n",
       "      <td>0.065635</td>\n",
       "      <td>0.529338</td>\n",
       "      <td>0.997413</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.998573</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.996434</td>\n",
       "      <td>0.510101</td>\n",
       "      <td>0.999288</td>\n",
       "      <td>0.652614</td>\n",
       "      <td>0.021890</td>\n",
       "      <td>0.017733</td>\n",
       "      <td>0.001746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.150156</td>\n",
       "      <td>0.028035</td>\n",
       "      <td>0.534332</td>\n",
       "      <td>0.856962</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.853067</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.865193</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.062155</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>0.018707</td>\n",
       "      <td>0.005425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>34.207132</td>\n",
       "      <td>0.081387</td>\n",
       "      <td>0.529963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.550</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.135941</td>\n",
       "      <td>0.028597</td>\n",
       "      <td>0.015687</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.987786</td>\n",
       "      <td>0.021661</td>\n",
       "      <td>0.539326</td>\n",
       "      <td>0.725608</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.719686</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.728245</td>\n",
       "      <td>0.550505</td>\n",
       "      <td>0.738604</td>\n",
       "      <td>0.086819</td>\n",
       "      <td>0.021003</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.007151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8.620298</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>0.532459</td>\n",
       "      <td>0.605850</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.602710</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.602710</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>0.608262</td>\n",
       "      <td>0.089523</td>\n",
       "      <td>0.006052</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.005044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.144225</td>\n",
       "      <td>0.015879</td>\n",
       "      <td>0.534332</td>\n",
       "      <td>0.719189</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.714693</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.721113</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>0.727920</td>\n",
       "      <td>0.079730</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>0.016080</td>\n",
       "      <td>0.005680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.084628</td>\n",
       "      <td>0.018889</td>\n",
       "      <td>0.534332</td>\n",
       "      <td>0.659443</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.649786</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.661912</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>0.663818</td>\n",
       "      <td>0.096702</td>\n",
       "      <td>0.012283</td>\n",
       "      <td>0.015660</td>\n",
       "      <td>0.006125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.215728</td>\n",
       "      <td>0.041307</td>\n",
       "      <td>0.531211</td>\n",
       "      <td>0.868378</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.863766</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.867332</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>0.873932</td>\n",
       "      <td>0.080191</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.016637</td>\n",
       "      <td>0.004049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.842436</td>\n",
       "      <td>0.038641</td>\n",
       "      <td>0.528090</td>\n",
       "      <td>0.931959</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.937946</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.926534</td>\n",
       "      <td>0.520202</td>\n",
       "      <td>0.935185</td>\n",
       "      <td>0.123658</td>\n",
       "      <td>0.022361</td>\n",
       "      <td>0.019492</td>\n",
       "      <td>0.003639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.690916</td>\n",
       "      <td>0.025236</td>\n",
       "      <td>0.536829</td>\n",
       "      <td>0.990011</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.988588</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.988588</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.993590</td>\n",
       "      <td>0.095562</td>\n",
       "      <td>0.006729</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.003030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.685001</td>\n",
       "      <td>0.013671</td>\n",
       "      <td>0.539326</td>\n",
       "      <td>0.759584</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.751070</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.766049</td>\n",
       "      <td>0.550505</td>\n",
       "      <td>0.764957</td>\n",
       "      <td>0.049338</td>\n",
       "      <td>0.011492</td>\n",
       "      <td>0.013614</td>\n",
       "      <td>0.009092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>24.390031</td>\n",
       "      <td>0.073218</td>\n",
       "      <td>0.524345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.540</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.204271</td>\n",
       "      <td>0.032286</td>\n",
       "      <td>0.013429</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.509938</td>\n",
       "      <td>0.017650</td>\n",
       "      <td>0.534956</td>\n",
       "      <td>0.888532</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.879458</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.884451</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.896724</td>\n",
       "      <td>0.074829</td>\n",
       "      <td>0.008207</td>\n",
       "      <td>0.022480</td>\n",
       "      <td>0.008558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.952758</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.529338</td>\n",
       "      <td>0.820043</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.811698</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.823110</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.829060</td>\n",
       "      <td>0.090896</td>\n",
       "      <td>0.009115</td>\n",
       "      <td>0.014206</td>\n",
       "      <td>0.007195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.127728</td>\n",
       "      <td>0.012567</td>\n",
       "      <td>0.535581</td>\n",
       "      <td>0.811216</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.805278</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.811698</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.225725</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.020396</td>\n",
       "      <td>0.006311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0        3.399557         0.012444         0.532459          0.565455   \n",
       "1        5.366827         0.010683         0.534956          0.633317   \n",
       "2        9.376678         0.016603         0.534956          0.632334   \n",
       "3       16.275892         0.052117         0.531835          0.999287   \n",
       "4        6.410967         0.015514         0.529963          0.623595   \n",
       "5       28.857051         0.071036         0.529338          0.992955   \n",
       "6       20.440096         0.047294         0.531835          0.973694   \n",
       "7        1.712006         0.007858         0.533708          0.564474   \n",
       "8       16.471233         0.046119         0.529963          0.960227   \n",
       "9        8.193141         0.019256         0.534956          0.645264   \n",
       "10      23.673239         0.039312         0.529963          0.980827   \n",
       "11       8.265442         0.015792         0.534956          0.685925   \n",
       "12      10.221845         0.028247         0.536205          0.946316   \n",
       "13       7.097931         0.013768         0.531835          0.596486   \n",
       "14      14.215882         0.019782         0.531835          0.686907   \n",
       "15      21.872894         0.065635         0.529338          0.997413   \n",
       "16      14.150156         0.028035         0.534332          0.856962   \n",
       "17      34.207132         0.081387         0.529963          1.000000   \n",
       "18      10.987786         0.021661         0.539326          0.725608   \n",
       "19       8.620298         0.014388         0.532459          0.605850   \n",
       "20       8.144225         0.015879         0.534332          0.719189   \n",
       "21       9.084628         0.018889         0.534332          0.659443   \n",
       "22      10.215728         0.041307         0.531211          0.868378   \n",
       "23      18.842436         0.038641         0.528090          0.931959   \n",
       "24      19.690916         0.025236         0.536829          0.990011   \n",
       "25       3.685001         0.013671         0.539326          0.759584   \n",
       "26      24.390031         0.073218         0.524345          1.000000   \n",
       "27       6.509938         0.017650         0.534956          0.888532   \n",
       "28      11.952758         0.021634         0.529338          0.820043   \n",
       "29       7.127728         0.012567         0.535581          0.811216   \n",
       "\n",
       "   param_colsample_bytree param_gamma param_learning_rate param_max_depth  \\\n",
       "0                     0.7        0.49                0.01               3   \n",
       "1                     0.9        0.25                0.01               5   \n",
       "2                    0.75        0.43                0.01               3   \n",
       "3                    0.55        0.61                0.01               8   \n",
       "4                     0.5        0.88                0.01               3   \n",
       "5                    0.75        0.16                0.01               7   \n",
       "6                    0.55        0.73                0.01               6   \n",
       "7                    0.55        0.67                0.01               3   \n",
       "8                     0.5        0.73                0.01               8   \n",
       "9                    0.55        0.76                0.01               3   \n",
       "10                    0.7         0.4                0.01               7   \n",
       "11                   0.75        0.37                0.01               5   \n",
       "12                   0.55        0.13                0.01               8   \n",
       "13                   0.85        0.88                0.01               3   \n",
       "14                    0.9        0.04                0.01               3   \n",
       "15                    0.7        0.01                0.01               6   \n",
       "16                    0.6        0.91                0.01               5   \n",
       "17                   0.95        0.28                0.01               8   \n",
       "18                    0.9        0.58                0.01               5   \n",
       "19                    0.9        0.49                0.01               3   \n",
       "20                   0.65         0.4                0.01               5   \n",
       "21                   0.85        0.19                0.01               4   \n",
       "22                    0.6        0.01                0.01               5   \n",
       "23                    0.9        0.31                0.01               5   \n",
       "24                    0.8        0.34                0.01               6   \n",
       "25                   0.55        0.07                0.01               6   \n",
       "26                    0.6        0.19                0.01               7   \n",
       "27                   0.75        0.88                0.01               8   \n",
       "28                   0.65        0.28                0.01               5   \n",
       "29                    0.7        0.07                0.01               8   \n",
       "\n",
       "   param_min_child_weight param_n_estimators       ...         \\\n",
       "0                       8                160       ...          \n",
       "1                       8                130       ...          \n",
       "2                       7                430       ...          \n",
       "3                       1                310       ...          \n",
       "4                       4                430       ...          \n",
       "5                       3                520       ...          \n",
       "6                       6                580       ...          \n",
       "7                       2                100       ...          \n",
       "8                       5                400       ...          \n",
       "9                       2                460       ...          \n",
       "10                      8                490       ...          \n",
       "11                      8                220       ...          \n",
       "12                      4                220       ...          \n",
       "13                      6                310       ...          \n",
       "14                      6                550       ...          \n",
       "15                      1                490       ...          \n",
       "16                      7                460       ...          \n",
       "17                      1                430       ...          \n",
       "18                      7                250       ...          \n",
       "19                      8                340       ...          \n",
       "20                      7                250       ...          \n",
       "21                      6                280       ...          \n",
       "22                      1                340       ...          \n",
       "23                      2                460       ...          \n",
       "24                      1                400       ...          \n",
       "25                      1                100       ...          \n",
       "26                      1                520       ...          \n",
       "27                      1                100       ...          \n",
       "28                      6                370       ...          \n",
       "29                      7                130       ...          \n",
       "\n",
       "   split5_test_score split5_train_score split6_test_score split6_train_score  \\\n",
       "0              0.545           0.558488             0.555           0.565621   \n",
       "1              0.555           0.620542             0.545           0.636947   \n",
       "2              0.560           0.623395             0.535           0.633381   \n",
       "3              0.555           1.000000             0.530           0.998573   \n",
       "4              0.545           0.609130             0.535           0.616976   \n",
       "5              0.565           0.993581             0.550           0.992867   \n",
       "6              0.580           0.970756             0.540           0.975036   \n",
       "7              0.530           0.559201             0.560           0.562054   \n",
       "8              0.560           0.958631             0.545           0.965763   \n",
       "9              0.560           0.647646             0.550           0.641940   \n",
       "10             0.575           0.977889             0.540           0.978602   \n",
       "11             0.555           0.671897             0.540           0.678317   \n",
       "12             0.565           0.945792             0.545           0.948645   \n",
       "13             0.545           0.589158             0.540           0.592011   \n",
       "14             0.555           0.677603             0.535           0.686163   \n",
       "15             0.570           0.998573             0.535           0.996434   \n",
       "16             0.570           0.853067             0.540           0.865193   \n",
       "17             0.555           1.000000             0.550           1.000000   \n",
       "18             0.560           0.719686             0.545           0.728245   \n",
       "19             0.550           0.602710             0.535           0.602710   \n",
       "20             0.555           0.714693             0.530           0.721113   \n",
       "21             0.550           0.649786             0.535           0.661912   \n",
       "22             0.555           0.863766             0.545           0.867332   \n",
       "23             0.565           0.937946             0.535           0.926534   \n",
       "24             0.575           0.988588             0.550           0.988588   \n",
       "25             0.550           0.751070             0.555           0.766049   \n",
       "26             0.545           1.000000             0.540           1.000000   \n",
       "27             0.560           0.879458             0.565           0.884451   \n",
       "28             0.555           0.811698             0.520           0.823110   \n",
       "29             0.555           0.805278             0.545           0.811698   \n",
       "\n",
       "   split7_test_score  split7_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.530303            0.562678      0.046896        0.006879   \n",
       "1           0.540404            0.633903      0.086661        0.003277   \n",
       "2           0.535354            0.638889      0.079614        0.006420   \n",
       "3           0.525253            0.999288      0.128408        0.018777   \n",
       "4           0.545455            0.630342      0.068891        0.003975   \n",
       "5           0.525253            0.993590      0.711842        0.041157   \n",
       "6           0.530303            0.972222      0.065160        0.024326   \n",
       "7           0.545455            0.566239      0.046309        0.003147   \n",
       "8           0.535354            0.964387      0.146136        0.022489   \n",
       "9           0.525253            0.650997      0.050271        0.005951   \n",
       "10          0.535354            0.984330      0.100150        0.017251   \n",
       "11          0.540404            0.702991      0.061780        0.008840   \n",
       "12          0.550505            0.950142      0.048685        0.013472   \n",
       "13          0.540404            0.603276      0.022491        0.004347   \n",
       "14          0.525253            0.697293      0.064899        0.006002   \n",
       "15          0.510101            0.999288      0.652614        0.021890   \n",
       "16          0.535354            0.865385      0.062155        0.012954   \n",
       "17          0.515152            1.000000      0.135941        0.028597   \n",
       "18          0.550505            0.738604      0.086819        0.021003   \n",
       "19          0.540404            0.608262      0.089523        0.006052   \n",
       "20          0.540404            0.727920      0.079730        0.003918   \n",
       "21          0.535354            0.663818      0.096702        0.012283   \n",
       "22          0.535354            0.873932      0.080191        0.029803   \n",
       "23          0.520202            0.935185      0.123658        0.022361   \n",
       "24          0.530303            0.993590      0.095562        0.006729   \n",
       "25          0.550505            0.764957      0.049338        0.011492   \n",
       "26          0.515152            1.000000      0.204271        0.032286   \n",
       "27          0.530303            0.896724      0.074829        0.008207   \n",
       "28          0.525253            0.829060      0.090896        0.009115   \n",
       "29          0.530303            0.820513      0.225725        0.003386   \n",
       "\n",
       "    std_test_score  std_train_score  \n",
       "0         0.015987         0.004647  \n",
       "1         0.012901         0.011217  \n",
       "2         0.019003         0.005878  \n",
       "3         0.016888         0.000798  \n",
       "4         0.016133         0.008738  \n",
       "5         0.022888         0.000754  \n",
       "6         0.023233         0.001830  \n",
       "7         0.017019         0.004590  \n",
       "8         0.021133         0.004012  \n",
       "9         0.019555         0.003615  \n",
       "10        0.022990         0.002274  \n",
       "11        0.016489         0.011168  \n",
       "12        0.021272         0.003760  \n",
       "13        0.012631         0.005801  \n",
       "14        0.016712         0.006268  \n",
       "15        0.017733         0.001746  \n",
       "16        0.018707         0.005425  \n",
       "17        0.015687         0.000000  \n",
       "18        0.016876         0.007151  \n",
       "19        0.015611         0.005044  \n",
       "20        0.016080         0.005680  \n",
       "21        0.015660         0.006125  \n",
       "22        0.016637         0.004049  \n",
       "23        0.019492         0.003639  \n",
       "24        0.018109         0.003030  \n",
       "25        0.013614         0.009092  \n",
       "26        0.013429         0.000000  \n",
       "27        0.022480         0.008558  \n",
       "28        0.014206         0.007195  \n",
       "29        0.020396         0.006311  \n",
       "\n",
       "[30 rows x 36 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all results of Grid Search\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "#cv_results.to_csv('./tuning/'+model_name+'-'+start_date+'.csv')\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.9,\n",
       "       gamma=0.58, learning_rate=0.01, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=7, missing=None, n_estimators=250, nthread=4,\n",
       "       objective='multi:softprob', reg_alpha=0.54, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=15, silent=True, subsample=0.6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a new classifier using the best parameters found by the grid search\n",
    "clf = XGBClassifier(\n",
    "    nthread=4, \n",
    "    seed=15,\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_child_weight=best_params['min_child_weight'],\n",
    "    gamma=best_params['gamma'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree'],\n",
    "    objective=best_params['objective'],\n",
    "    scale_pos_weight=best_params['scale_pos_weight'],\n",
    "    reg_alpha=best_params['reg_alpha']\n",
    ")\n",
    "clf.fit(X_train, y_train, eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict target values\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_probs = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0064458718423066"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cross-entropy score\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          A       0.60      0.21      0.32       210\n",
      "          D       0.25      0.01      0.01       160\n",
      "          H       0.49      0.94      0.64       317\n",
      "\n",
      "avg / total       0.47      0.50      0.40       687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute precision, recall, F-measure and support\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=['A', 'D', 'H']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49927219796215427"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Season test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply on current season that is not use for train and test set\n",
    "X_pred_current_season = clf.predict(X_current_season)\n",
    "X_prob_current_season = clf.predict_proba(X_current_season)\n",
    "\n",
    "df_current_season['probs_A'] = X_prob_current_season[:,0]\n",
    "df_current_season['probs_D'] = X_prob_current_season[:,1]\n",
    "df_current_season['probs_H'] = X_prob_current_season[:,2]\n",
    "df_current_season['probs'] = df_current_season[['probs_A','probs_D','probs_H']].max(axis=1)\n",
    "\n",
    "df_current_season['pred'] = le.inverse_transform(X_pred_current_season)\n",
    "\n",
    "df_current_season['GOOD'] = False\n",
    "df_current_season.loc[df_current_season.INFO_FTR == df_current_season.pred, 'GOOD'] = True\n",
    "\n",
    "df_current_season['WIN'] = -1\n",
    "df_current_season.loc[df_current_season.INFO_FTR == df_current_season.pred, 'WIN'] = df_current_season['INFO_WIN']-1\n",
    "\n",
    "df_current_season['WIN_P'] = -1\n",
    "df_current_season.loc[df_current_season.INFO_FTR == df_current_season.pred, 'WIN_P'] = df_current_season['INFO_WIN_P']-1\n",
    "\n",
    "df_current_season['INFO_ODD'] = 0\n",
    "df_current_season.loc[df_current_season.pred == 'A', 'INFO_ODD_BET'] = df_current_season[odd_A]\n",
    "df_current_season.loc[df_current_season.pred == 'D', 'INFO_ODD_BET'] = df_current_season[odd_D]\n",
    "df_current_season.loc[df_current_season.pred == 'H', 'INFO_ODD_BET'] = df_current_season[odd_H]\n",
    "\n",
    "#TODO\n",
    "# test prob_less_bet <= 0.2 et si ceux que je predisais H je les mettais A en fait !\n",
    "\n",
    "df_current_season['prob_less_bet'] = 0\n",
    "df_current_season.loc[df_current_season.pred == 'A', 'prob_less_bet'] = df_current_season['probs'] - df_current_season[odd_A].apply(lambda x: 1/x)\n",
    "df_current_season.loc[df_current_season.pred == 'D', 'prob_less_bet'] = df_current_season['probs'] - df_current_season[odd_D].apply(lambda x: 1/x)\n",
    "df_current_season.loc[df_current_season.pred == 'H', 'prob_less_bet'] = df_current_season['probs'] - df_current_season[odd_H].apply(lambda x: 1/x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          A       0.62      0.26      0.37        80\n",
      "          D       0.50      0.02      0.03        62\n",
      "          H       0.53      0.94      0.68       138\n",
      "\n",
      "avg / total       0.55      0.54      0.45       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score for this current season\n",
    "print(classification_report(y_current_season, X_pred_current_season, target_names=['A', 'D', 'H']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97667854257992337"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cross-entropy score for season\n",
    "log_loss(y_current_season, X_prob_current_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54285714285714282"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute accuracy score for season\n",
    "accuracy_score(y_current_season, X_pred_current_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019999999999999905"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What I win/lost on each match\n",
    "df_current_season.WIN.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
