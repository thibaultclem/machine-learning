{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost FTR Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the library\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "import matplotlib.pyplot as plt # plotting library\n",
    "import seaborn as sns # visualization library based on matplotlib\n",
    "from IPython.display import display # Manage multiple output per cell\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features = [\"A_MEANS_FIVE_AC\",\"A_MEANS_FIVE_AF\",\"A_MEANS_FIVE_AR\",\"A_MEANS_FIVE_AS\",\"A_MEANS_FIVE_AST\",\"A_MEANS_FIVE_AY\",\"A_MEANS_FIVE_FTAG\",\"A_MEANS_FIVE_FTHG\",\"A_MEANS_FIVE_FTR_A\",\"A_MEANS_FIVE_FTR_D\",\"A_MEANS_FIVE_FTR_H\",\"A_MEANS_FIVE_HC\",\"A_MEANS_FIVE_HF\",\"A_MEANS_FIVE_HR\",\"A_MEANS_FIVE_HS\",\"A_MEANS_FIVE_HST\",\"A_MEANS_FIVE_HTAG\",\"A_MEANS_FIVE_HTHG\",\"A_MEANS_FIVE_HTR_A\",\"A_MEANS_FIVE_HTR_D\",\"A_MEANS_FIVE_HTR_H\",\"A_MEANS_FIVE_HY\",\"H_MEANS_FIVE_AC\",\"H_MEANS_FIVE_AF\",\"H_MEANS_FIVE_AR\",\"H_MEANS_FIVE_AS\",\"H_MEANS_FIVE_AST\",\"H_MEANS_FIVE_AY\",\"H_MEANS_FIVE_FTAG\",\"H_MEANS_FIVE_FTHG\",\"H_MEANS_FIVE_FTR_A\",\"H_MEANS_FIVE_FTR_D\",\"H_MEANS_FIVE_FTR_H\",\"H_MEANS_FIVE_HC\",\"H_MEANS_FIVE_HF\",\"H_MEANS_FIVE_HR\",\"H_MEANS_FIVE_HS\",\"H_MEANS_FIVE_HST\",\"H_MEANS_FIVE_HTAG\",\"H_MEANS_FIVE_HTHG\",\"H_MEANS_FIVE_HTR_A\",\"H_MEANS_FIVE_HTR_D\",\"H_MEANS_FIVE_HTR_H\",\"H_MEANS_FIVE_HY\",\"A_MEANS_THREE_AC\",\"A_MEANS_THREE_AF\",\"A_MEANS_THREE_AR\",\"A_MEANS_THREE_AS\",\"A_MEANS_THREE_AST\",\"A_MEANS_THREE_AY\",\"A_MEANS_THREE_FTAG\",\"A_MEANS_THREE_FTHG\",\"A_MEANS_THREE_FTR_A\",\"A_MEANS_THREE_FTR_D\",\"A_MEANS_THREE_FTR_H\",\"A_MEANS_THREE_HC\",\"A_MEANS_THREE_HF\",\"A_MEANS_THREE_HR\",\"A_MEANS_THREE_HS\",\"A_MEANS_THREE_HST\",\"A_MEANS_THREE_HTAG\",\"A_MEANS_THREE_HTHG\",\"A_MEANS_THREE_HTR_A\",\"A_MEANS_THREE_HTR_D\",\"A_MEANS_THREE_HTR_H\",\"A_MEANS_THREE_HY\",\"H_MEANS_THREE_AC\",\"H_MEANS_THREE_AF\",\"H_MEANS_THREE_AR\",\"H_MEANS_THREE_AS\",\"H_MEANS_THREE_AST\",\"H_MEANS_THREE_AY\",\"H_MEANS_THREE_FTAG\",\"H_MEANS_THREE_FTHG\",\"H_MEANS_THREE_FTR_A\",\"H_MEANS_THREE_FTR_D\",\"H_MEANS_THREE_FTR_H\",\"H_MEANS_THREE_HC\",\"H_MEANS_THREE_HF\",\"H_MEANS_THREE_HR\",\"H_MEANS_THREE_HS\",\"H_MEANS_THREE_HST\",\"H_MEANS_THREE_HTAG\",\"H_MEANS_THREE_HTHG\",\"H_MEANS_THREE_HTR_A\",\"H_MEANS_THREE_HTR_D\",\"H_MEANS_THREE_HTR_H\",\"H_MEANS_THREE_HY\",\"A_STD_FIVE_AC\",\"A_STD_FIVE_AF\",\"A_STD_FIVE_AR\",\"A_STD_FIVE_AS\",\"A_STD_FIVE_AST\",\"A_STD_FIVE_AY\",\"A_STD_FIVE_FTAG\",\"A_STD_FIVE_FTHG\",\"A_STD_FIVE_FTR_A\",\"A_STD_FIVE_FTR_D\",\"A_STD_FIVE_FTR_H\",\"A_STD_FIVE_HC\",\"A_STD_FIVE_HF\",\"A_STD_FIVE_HR\",\"A_STD_FIVE_HS\",\"A_STD_FIVE_HST\",\"A_STD_FIVE_HTAG\",\"A_STD_FIVE_HTHG\",\"A_STD_FIVE_HTR_A\",\"A_STD_FIVE_HTR_D\",\"A_STD_FIVE_HTR_H\",\"A_STD_FIVE_HY\",\"H_STD_FIVE_AC\",\"H_STD_FIVE_AF\",\"H_STD_FIVE_AR\",\"H_STD_FIVE_AS\",\"H_STD_FIVE_AST\",\"H_STD_FIVE_AY\",\"H_STD_FIVE_FTAG\",\"H_STD_FIVE_FTHG\",\"H_STD_FIVE_FTR_A\",\"H_STD_FIVE_FTR_D\",\"H_STD_FIVE_FTR_H\",\"H_STD_FIVE_HC\",\"H_STD_FIVE_HF\",\"H_STD_FIVE_HR\",\"H_STD_FIVE_HS\",\"H_STD_FIVE_HST\",\"H_STD_FIVE_HTAG\",\"H_STD_FIVE_HTHG\",\"H_STD_FIVE_HTR_A\",\"H_STD_FIVE_HTR_D\",\"H_STD_FIVE_HTR_H\",\"H_STD_FIVE_HY\",\"A_STD_THREE_AC\",\"A_STD_THREE_AF\",\"A_STD_THREE_AR\",\"A_STD_THREE_AS\",\"A_STD_THREE_AST\",\"A_STD_THREE_AY\",\"A_STD_THREE_FTAG\",\"A_STD_THREE_FTHG\",\"A_STD_THREE_FTR_A\",\"A_STD_THREE_FTR_D\",\"A_STD_THREE_FTR_H\",\"A_STD_THREE_HC\",\"A_STD_THREE_HF\",\"A_STD_THREE_HR\",\"A_STD_THREE_HS\",\"A_STD_THREE_HST\",\"A_STD_THREE_HTAG\",\"A_STD_THREE_HTHG\",\"A_STD_THREE_HTR_A\",\"A_STD_THREE_HTR_D\",\"A_STD_THREE_HTR_H\",\"A_STD_THREE_HY\",\"H_STD_THREE_AC\",\"H_STD_THREE_AF\",\"H_STD_THREE_AR\",\"H_STD_THREE_AS\",\"H_STD_THREE_AST\",\"H_STD_THREE_AY\",\"H_STD_THREE_FTAG\",\"H_STD_THREE_FTHG\",\"H_STD_THREE_FTR_A\",\"H_STD_THREE_FTR_D\",\"H_STD_THREE_FTR_H\",\"H_STD_THREE_HC\",\"H_STD_THREE_HF\",\"H_STD_THREE_HR\",\"H_STD_THREE_HS\",\"H_STD_THREE_HST\",\"H_STD_THREE_HTAG\",\"H_STD_THREE_HTHG\",\"H_STD_THREE_HTR_A\",\"H_STD_THREE_HTR_D\",\"H_STD_THREE_HTR_H\",\"H_STD_THREE_HY\",\"INFO_Div\"]\n",
    "best_features_60 = ['A_MEANS_FIVE_AC', 'A_MEANS_FIVE_AS', 'A_MEANS_FIVE_AST',\n",
    "       'A_MEANS_FIVE_AY', 'A_MEANS_FIVE_FTAG', 'A_MEANS_FIVE_FTHG',\n",
    "       'A_MEANS_FIVE_FTR_A', 'A_MEANS_FIVE_FTR_H', 'A_MEANS_FIVE_HC',\n",
    "       'A_MEANS_FIVE_HF', 'A_MEANS_FIVE_HS', 'A_MEANS_FIVE_HST',\n",
    "       'A_MEANS_FIVE_HTHG', 'A_MEANS_FIVE_HTR_A', 'A_MEANS_FIVE_HY',\n",
    "       'H_MEANS_FIVE_AC', 'H_MEANS_FIVE_AS', 'H_MEANS_FIVE_AST',\n",
    "       'H_MEANS_FIVE_AY', 'H_MEANS_FIVE_FTAG', 'H_MEANS_FIVE_FTHG',\n",
    "       'H_MEANS_FIVE_FTR_A', 'H_MEANS_FIVE_FTR_H', 'H_MEANS_FIVE_HC',\n",
    "       'H_MEANS_FIVE_HF', 'H_MEANS_FIVE_HS', 'H_MEANS_FIVE_HST',\n",
    "       'H_MEANS_FIVE_HTAG', 'H_MEANS_FIVE_HTR_A', 'H_MEANS_FIVE_HTR_H',\n",
    "       'A_MEANS_THREE_AC', 'A_MEANS_THREE_AS', 'A_MEANS_THREE_FTHG',\n",
    "       'A_MEANS_THREE_FTR_A', 'A_MEANS_THREE_HC', 'A_MEANS_THREE_HF',\n",
    "       'A_MEANS_THREE_HS', 'A_MEANS_THREE_HST', 'H_MEANS_THREE_AC',\n",
    "       'H_MEANS_THREE_AS', 'H_MEANS_THREE_AST', 'H_MEANS_THREE_FTHG',\n",
    "       'H_MEANS_THREE_HC', 'H_MEANS_THREE_HST', 'H_MEANS_THREE_HTR_H',\n",
    "       'A_STD_FIVE_AF', 'A_STD_FIVE_AS', 'A_STD_FIVE_AST', 'A_STD_FIVE_HC',\n",
    "       'A_STD_FIVE_HF', 'A_STD_FIVE_HS', 'H_STD_FIVE_AF', 'H_STD_FIVE_AS',\n",
    "       'H_STD_FIVE_AST', 'H_STD_FIVE_HC', 'H_STD_FIVE_HF',\n",
    "       'H_STD_FIVE_HST', 'H_STD_FIVE_HTHG', 'H_STD_THREE_AS',\n",
    "       'H_STD_THREE_HST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'XGboost-best_features'\n",
    "target = 'INFO_FTR_D'\n",
    "odd = 'INFO_BbAvD'\n",
    "bet_on = 'D'\n",
    "start_date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DB Sqlite connection\n",
    "import sqlite3\n",
    "db = \"/Users/thibaultclement/Project/ligue1-predict/src/notebook/data/db/soccer_predict.sqlite\"\n",
    "conn = sqlite3.connect(db)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25275, 190)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all prematch data\n",
    "df = pd.read_sql_query(\"SELECT * FROM pre_matchs ORDER BY INFO_Date ASC;\", conn)\n",
    "df = (df[df.columns.drop(['index'])])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18027, 190)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all game between June (include) and October (include)\n",
    "df['INFO_Date'] = pd.to_datetime(df['INFO_Date'])\n",
    "df['INFO_Date'].dt.month\n",
    "df = df[(df['INFO_Date'].dt.month < 6) | (df['INFO_Date'].dt.month > 10)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18027, 190)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a particular league\n",
    "#df = df[(df['INFO_Div'] == 'D1')]\n",
    "#df = df[(df['INFO_Div'] == 'F1')]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7090353358850945"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the average odd\n",
    "df[odd].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18027, 190)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing column where odd is too high or too low\n",
    "# df = df.drop(df[df['INFO_BbAvH'] < 2].index)\n",
    "# df = df.drop(df[df['INFO_BbAvA'] < 2].index)\n",
    "# df = df.drop(df[df['INFO_BbAvH'] > 10].index)\n",
    "# df = df.drop(df[df['INFO_BbAvA'] > 10].index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a INFO_WIN column containing the gain. If bet success it's equal to odd -1, else -1 (loose your bet)\n",
    "df['INFO_WIN'] = df[odd]-1\n",
    "df.loc[df.INFO_FTR != bet_on, 'INFO_WIN'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "H    45.515061\n",
       "A    28.856715\n",
       "D    25.628224\n",
       "Name: INFO_FTR, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Statistic about winners\n",
    "display(plt.show(), 100. * df.INFO_FTR.value_counts() / len(df.INFO_FTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0966472513452044"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much did you win/lost per match if bet on all\n",
    "df.INFO_WIN.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep season 2016/2017 for further test and don't use it for traning\n",
    "import datetime\n",
    "date_start_current_season = datetime.date(2016, 8, 1)\n",
    "df_current_season = df[(df['INFO_Date'] > date_start_current_season)]\n",
    "df = df[(df['INFO_Date'] < date_start_current_season)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2222, 191)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of matches in current season\n",
    "df_current_season.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "features_list = all_features\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X = pd.get_dummies(df[features_list])\n",
    "y = pd.get_dummies(df)[target].astype('bool_')\n",
    "X_current_season = pd.get_dummies(df_current_season[features_list])\n",
    "y_current_season = pd.get_dummies(df_current_season)[target].astype('bool_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc_X = StandardScaler().fit(X)\n",
    "#X = sc_X.transform(X)\n",
    "#X_current_season = sc_X.transform(X_current_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impute of missing values (NaN) with the mean\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp = imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "X_current_season = imp.transform(X_current_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import model\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "classifier = XGBClassifier(nthread=4, seed=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find the best n_estimator\n",
    "# import xgboost as xgb\n",
    "# xgb1 = XGBClassifier(\n",
    "#  learning_rate =0.1,\n",
    "#  n_estimators=5000,\n",
    "#  max_depth=5,\n",
    "#  min_child_weight=1,\n",
    "#  gamma=0,\n",
    "#  subsample=0.8,\n",
    "#  colsample_bytree=0.8,\n",
    "#  objective= 'binary:logistic',\n",
    "#  nthread=4,\n",
    "#  scale_pos_weight=3,\n",
    "#  seed=27)\n",
    "# xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "# cvresult = xgb.cv(xgb1.get_xgb_params(), xgtrain, num_boost_round=5000, nfold=8,\n",
    "#             metrics='logloss', early_stopping_rounds=50, verbose_eval=True)\n",
    "# cvresult.shape[0]\n",
    "# Result = 242 for 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 12 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed: 13.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "826.8058869838715"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Grid Search to find the best hyper-parameters for our Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.classification import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "parameters = [{\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [352],\n",
    "    'max_depth': [5],\n",
    "    'min_child_weight': [1],\n",
    "    'gamma': [0],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'objective': ['binary:logistic'],\n",
    "    'scale_pos_weight': [3],\n",
    "    'reg_alpha':[0]\n",
    "}]\n",
    "\n",
    "parameters = [{\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [352],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'objective': ['binary:logistic'],\n",
    "    'scale_pos_weight': [3],\n",
    "    'reg_alpha':[0]\n",
    "}]\n",
    "# 'max_depth': 3,'min_child_weight': 5\n",
    "# F1:\n",
    "{'colsample_bytree': 0.8,\n",
    " 'gamma': 0,\n",
    " 'learning_rate': 0.1,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 5,\n",
    " 'n_estimators': 352,\n",
    " 'objective': 'binary:logistic',\n",
    " 'reg_alpha': 0,\n",
    " 'scale_pos_weight': 3,\n",
    " 'subsample': 0.8}\n",
    "grid_search = GridSearchCV(estimator=classifier,\n",
    "                           param_grid=parameters,\n",
    "                           #scoring=make_scorer(log_loss, greater_is_better=False),\n",
    "                           scoring='f1',\n",
    "                           cv=8,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33086615755708698"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract best score calculated with the GridSearchCV\n",
    "best_score = grid_search.best_score_\n",
    "display(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 5,\n",
       " 'n_estimators': 352,\n",
       " 'objective': 'binary:logistic',\n",
       " 'reg_alpha': 0,\n",
       " 'scale_pos_weight': 3,\n",
       " 'subsample': 0.8}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract best hyper-parameter calculated with the GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>...</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.272736</td>\n",
       "      <td>0.030797</td>\n",
       "      <td>0.325044</td>\n",
       "      <td>0.707262</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>0.708712</td>\n",
       "      <td>0.317530</td>\n",
       "      <td>0.701515</td>\n",
       "      <td>0.338530</td>\n",
       "      <td>0.701508</td>\n",
       "      <td>0.481483</td>\n",
       "      <td>0.019320</td>\n",
       "      <td>0.024889</td>\n",
       "      <td>0.005064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.785835</td>\n",
       "      <td>0.029168</td>\n",
       "      <td>0.327457</td>\n",
       "      <td>0.702880</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335159</td>\n",
       "      <td>0.697547</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.712777</td>\n",
       "      <td>0.339912</td>\n",
       "      <td>0.695323</td>\n",
       "      <td>0.090873</td>\n",
       "      <td>0.009904</td>\n",
       "      <td>0.019331</td>\n",
       "      <td>0.005128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.679128</td>\n",
       "      <td>0.029792</td>\n",
       "      <td>0.330866</td>\n",
       "      <td>0.700310</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344385</td>\n",
       "      <td>0.692741</td>\n",
       "      <td>0.315556</td>\n",
       "      <td>0.699483</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.692576</td>\n",
       "      <td>0.060549</td>\n",
       "      <td>0.018440</td>\n",
       "      <td>0.013112</td>\n",
       "      <td>0.008111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.513668</td>\n",
       "      <td>0.045109</td>\n",
       "      <td>0.237151</td>\n",
       "      <td>0.975771</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282246</td>\n",
       "      <td>0.979845</td>\n",
       "      <td>0.207752</td>\n",
       "      <td>0.976609</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.971714</td>\n",
       "      <td>0.293212</td>\n",
       "      <td>0.023780</td>\n",
       "      <td>0.025347</td>\n",
       "      <td>0.002209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.326967</td>\n",
       "      <td>0.035963</td>\n",
       "      <td>0.229206</td>\n",
       "      <td>0.970082</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.966137</td>\n",
       "      <td>0.200308</td>\n",
       "      <td>0.971549</td>\n",
       "      <td>0.214953</td>\n",
       "      <td>0.970267</td>\n",
       "      <td>0.102807</td>\n",
       "      <td>0.010065</td>\n",
       "      <td>0.025744</td>\n",
       "      <td>0.002036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27.260659</td>\n",
       "      <td>0.041862</td>\n",
       "      <td>0.257793</td>\n",
       "      <td>0.965107</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275148</td>\n",
       "      <td>0.967643</td>\n",
       "      <td>0.235123</td>\n",
       "      <td>0.959498</td>\n",
       "      <td>0.240240</td>\n",
       "      <td>0.958072</td>\n",
       "      <td>0.119781</td>\n",
       "      <td>0.013223</td>\n",
       "      <td>0.018973</td>\n",
       "      <td>0.004119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39.331556</td>\n",
       "      <td>0.068114</td>\n",
       "      <td>0.132057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162272</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.131417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.140421</td>\n",
       "      <td>0.018403</td>\n",
       "      <td>0.027540</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>38.765583</td>\n",
       "      <td>0.070465</td>\n",
       "      <td>0.158248</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.112450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.117904</td>\n",
       "      <td>0.023550</td>\n",
       "      <td>0.029840</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>38.536656</td>\n",
       "      <td>0.076223</td>\n",
       "      <td>0.163824</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189753</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.160305</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.999802</td>\n",
       "      <td>0.116385</td>\n",
       "      <td>0.022051</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>52.491525</td>\n",
       "      <td>0.104097</td>\n",
       "      <td>0.074111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063725</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212035</td>\n",
       "      <td>0.022396</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>51.192838</td>\n",
       "      <td>0.095136</td>\n",
       "      <td>0.098239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105960</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077449</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.075893</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.177252</td>\n",
       "      <td>0.028912</td>\n",
       "      <td>0.015645</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>51.042603</td>\n",
       "      <td>0.076411</td>\n",
       "      <td>0.118018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100656</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.110874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.223704</td>\n",
       "      <td>0.025849</td>\n",
       "      <td>0.016781</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       17.272736         0.030797         0.325044          0.707262   \n",
       "1       16.785835         0.029168         0.327457          0.702880   \n",
       "2       16.679128         0.029792         0.330866          0.700310   \n",
       "3       27.513668         0.045109         0.237151          0.975771   \n",
       "4       27.326967         0.035963         0.229206          0.970082   \n",
       "5       27.260659         0.041862         0.257793          0.965107   \n",
       "6       39.331556         0.068114         0.132057          1.000000   \n",
       "7       38.765583         0.070465         0.158248          1.000000   \n",
       "8       38.536656         0.076223         0.163824          0.999975   \n",
       "9       52.491525         0.104097         0.074111          1.000000   \n",
       "10      51.192838         0.095136         0.098239          1.000000   \n",
       "11      51.042603         0.076411         0.118018          1.000000   \n",
       "\n",
       "   param_colsample_bytree param_gamma param_learning_rate param_max_depth  \\\n",
       "0                     0.8           0                 0.1               3   \n",
       "1                     0.8           0                 0.1               3   \n",
       "2                     0.8           0                 0.1               3   \n",
       "3                     0.8           0                 0.1               5   \n",
       "4                     0.8           0                 0.1               5   \n",
       "5                     0.8           0                 0.1               5   \n",
       "6                     0.8           0                 0.1               7   \n",
       "7                     0.8           0                 0.1               7   \n",
       "8                     0.8           0                 0.1               7   \n",
       "9                     0.8           0                 0.1               9   \n",
       "10                    0.8           0                 0.1               9   \n",
       "11                    0.8           0                 0.1               9   \n",
       "\n",
       "   param_min_child_weight param_n_estimators       ...         \\\n",
       "0                       1                352       ...          \n",
       "1                       3                352       ...          \n",
       "2                       5                352       ...          \n",
       "3                       1                352       ...          \n",
       "4                       3                352       ...          \n",
       "5                       5                352       ...          \n",
       "6                       1                352       ...          \n",
       "7                       3                352       ...          \n",
       "8                       5                352       ...          \n",
       "9                       1                352       ...          \n",
       "10                      3                352       ...          \n",
       "11                      5                352       ...          \n",
       "\n",
       "   split5_test_score split5_train_score split6_test_score split6_train_score  \\\n",
       "0           0.345200           0.708712          0.317530           0.701515   \n",
       "1           0.335159           0.697547          0.324324           0.712777   \n",
       "2           0.344385           0.692741          0.315556           0.699483   \n",
       "3           0.282246           0.979845          0.207752           0.976609   \n",
       "4           0.258824           0.966137          0.200308           0.971549   \n",
       "5           0.275148           0.967643          0.235123           0.959498   \n",
       "6           0.162272           1.000000          0.146341           1.000000   \n",
       "7           0.198853           1.000000          0.112450           1.000000   \n",
       "8           0.189753           1.000000          0.160305           1.000000   \n",
       "9           0.099057           1.000000          0.062350           1.000000   \n",
       "10          0.105960           1.000000          0.077449           1.000000   \n",
       "11          0.115385           1.000000          0.100656           1.000000   \n",
       "\n",
       "   split7_test_score  split7_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.338530            0.701508      0.481483        0.019320   \n",
       "1           0.339912            0.695323      0.090873        0.009904   \n",
       "2           0.330435            0.692576      0.060549        0.018440   \n",
       "3           0.211180            0.971714      0.293212        0.023780   \n",
       "4           0.214953            0.970267      0.102807        0.010065   \n",
       "5           0.240240            0.958072      0.119781        0.013223   \n",
       "6           0.131417            1.000000      0.140421        0.018403   \n",
       "7           0.144578            1.000000      0.117904        0.023550   \n",
       "8           0.136000            0.999802      0.116385        0.022051   \n",
       "9           0.063725            1.000000      0.212035        0.022396   \n",
       "10          0.075893            1.000000      0.177252        0.028912   \n",
       "11          0.110874            1.000000      0.223704        0.025849   \n",
       "\n",
       "    std_test_score  std_train_score  \n",
       "0         0.024889         0.005064  \n",
       "1         0.019331         0.005128  \n",
       "2         0.013112         0.008111  \n",
       "3         0.025347         0.002209  \n",
       "4         0.025744         0.002036  \n",
       "5         0.018973         0.004119  \n",
       "6         0.027540         0.000000  \n",
       "7         0.029840         0.000000  \n",
       "8         0.016932         0.000065  \n",
       "9         0.013825         0.000000  \n",
       "10        0.015645         0.000000  \n",
       "11        0.016781         0.000000  \n",
       "\n",
       "[12 rows x 36 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all results of Grid Search\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "#cv_results.to_csv('./tuning/'+model_name+'-'+target+'_'+start_date+'.csv')\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=5, missing=None, n_estimators=352, nthread=4,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=3, seed=15, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a new classifier using the best parameters found by the grid search\n",
    "clf = XGBClassifier(\n",
    "    nthread=4, \n",
    "    seed=15,\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_child_weight=best_params['min_child_weight'],\n",
    "    gamma=best_params['gamma'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree'],\n",
    "    objective=best_params['objective'],\n",
    "    scale_pos_weight=best_params['scale_pos_weight']#,\n",
    "    #reg_alpha=best_params['reg_alpha']\n",
    ")\n",
    "clf.fit(X_train, y_train, eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict target values\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_probs = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.66724947845457"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cross-entropy score\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2700049091801669, 0.45304777594728174, 0.33835742848354355, None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute precision, recall, F-measure and support\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5157812575881533"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>2041</td>\n",
       "      <td>1487</td>\n",
       "      <td>3528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>664</td>\n",
       "      <td>550</td>\n",
       "      <td>1214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>2705</td>\n",
       "      <td>2037</td>\n",
       "      <td>4742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True   All\n",
       "Actual                      \n",
       "False       2041  1487  3528\n",
       "True         664   550  1214\n",
       "All         2705  2037  4742"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the confusion Matrix\n",
    "df_confusion = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAAFlCAYAAAAZGcpRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkDwrrv8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115ee9090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test.ravel(), y_probs[:, 1].ravel())\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('ROC')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL DATASET TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply on all the dataset\n",
    "X_pred = clf.predict(X)\n",
    "df['pred'] = X_pred\n",
    "df_bet_all_seasons = df.drop(df[df.pred == 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6911, 192)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many bet I did\n",
    "df_bet_all_seasons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "D    45.391405\n",
       "H    31.486037\n",
       "A    23.122558\n",
       "Name: INFO_FTR, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What will be the real result of games I bet on\n",
    "display(plt.show(), 100. * df_bet_all_seasons.INFO_FTR.value_counts() / len(df_bet_all_seasons.INFO_FTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.360731197024556"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cross-entropy score\n",
    "log_loss(y, X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.45391405006511359, 0.76437621832358671, 0.5695869269178393, None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score for all dataset\n",
    "precision_recall_fscore_support(y, X_pred, average='binary') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>7927</td>\n",
       "      <td>3774</td>\n",
       "      <td>11701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>967</td>\n",
       "      <td>3137</td>\n",
       "      <td>4104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>8894</td>\n",
       "      <td>6911</td>\n",
       "      <td>15805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True    All\n",
       "Actual                       \n",
       "False       7927  3774  11701\n",
       "True         967  3137   4104\n",
       "All         8894  6911  15805"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the confusion Matrix\n",
    "df_confusion = pd.crosstab(y, X_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5732687020691645"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bet_all_seasons.INFO_WIN.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEASON 2016/2017 TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply on current season that is not use for train and test set\n",
    "X_pred_current_season = clf.predict(X_current_season)\n",
    "X_prob_current_season = clf.predict_proba(X_current_season)\n",
    "df_current_season['pred'] = X_pred_current_season\n",
    "df_current_season['prob'] = X_prob_current_season[:,1:]\n",
    "df_current_season['prob_less_bet'] = df_current_season['prob'] - df_current_season[odd].apply(lambda x: 1/x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove games I didn't bet on\n",
    "df_bet_current_season = df_current_season\n",
    "df_bet_current_season = df_bet_current_season.drop(df_bet_current_season[df_bet_current_season.pred == 0].index)\n",
    "df_bet_current_season = df_bet_current_season[df_bet_current_season.prob > 0.6]\n",
    "#df_bet_current_season = df_bet_current_season[df_bet_current_season.prob_less_bet > 0.2]\n",
    "#df_bet_current_season = df_bet_current_season[df_bet_current_season[odd] > 2]\n",
    "#df_bet_current_season = df_bet_current_season[df_bet_current_season[odd] < 5]\n",
    "#df_bet_current_season = df_bet_current_season[df_bet_current_season['INFO_BbAvH'] > 1.5]\n",
    "#df_bet_current_season = df_bet_current_season[df_bet_current_season['INFO_BbAvA'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bet_current_season[odd].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many bet I did\n",
    "df_bet_current_season.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many matches was play\n",
    "X_pred_current_season.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What will be the real result of games I bet on\n",
    "display(plt.show(), 100. * df_bet_current_season.INFO_FTR.value_counts() / len(df_bet_current_season.INFO_FTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cross-entropy score\n",
    "log_loss(y_current_season, X_pred_current_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score for this current season\n",
    "precision_recall_fscore_support(y_current_season, X_pred_current_season, average='binary') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_current_season, X_pred_current_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the confusion Matrix\n",
    "df_confusion = pd.crosstab(y_current_season, X_pred_current_season, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What I win/lost on each match\n",
    "df_bet_current_season.INFO_WIN.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_MEANS_FIVE_AC</th>\n",
       "      <th>A_MEANS_FIVE_AF</th>\n",
       "      <th>A_MEANS_FIVE_AR</th>\n",
       "      <th>A_MEANS_FIVE_AS</th>\n",
       "      <th>A_MEANS_FIVE_AST</th>\n",
       "      <th>A_MEANS_FIVE_AY</th>\n",
       "      <th>A_MEANS_FIVE_FTAG</th>\n",
       "      <th>A_MEANS_FIVE_FTHG</th>\n",
       "      <th>A_MEANS_FIVE_FTR_A</th>\n",
       "      <th>A_MEANS_FIVE_FTR_D</th>\n",
       "      <th>...</th>\n",
       "      <th>INFO_FTR</th>\n",
       "      <th>INFO_HTR</th>\n",
       "      <th>INFO_HomeTeam</th>\n",
       "      <th>INFO_PSA</th>\n",
       "      <th>INFO_PSD</th>\n",
       "      <th>INFO_PSH</th>\n",
       "      <th>INFO_WIN</th>\n",
       "      <th>pred</th>\n",
       "      <th>prob</th>\n",
       "      <th>prob_less_bet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23053</th>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>A</td>\n",
       "      <td>Bristol Rvs</td>\n",
       "      <td>4.13</td>\n",
       "      <td>3.74</td>\n",
       "      <td>1.94</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.548300</td>\n",
       "      <td>0.262586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23054</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Coventry</td>\n",
       "      <td>4.99</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1.77</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.504511</td>\n",
       "      <td>0.225960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23058</th>\n",
       "      <td>3.4</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Malaga</td>\n",
       "      <td>5.83</td>\n",
       "      <td>3.79</td>\n",
       "      <td>1.71</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.572990</td>\n",
       "      <td>0.298265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23061</th>\n",
       "      <td>3.4</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Ein Frankfurt</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.70</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.575108</td>\n",
       "      <td>0.257647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23062</th>\n",
       "      <td>4.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.37</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.591229</td>\n",
       "      <td>0.288199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23064</th>\n",
       "      <td>2.6</td>\n",
       "      <td>16.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Ingolstadt</td>\n",
       "      <td>3.81</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.674445</td>\n",
       "      <td>0.369567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23068</th>\n",
       "      <td>4.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>6.47</td>\n",
       "      <td>4.37</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.562900</td>\n",
       "      <td>0.325933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23070</th>\n",
       "      <td>4.4</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>4.13</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.52</td>\n",
       "      <td>True</td>\n",
       "      <td>0.666288</td>\n",
       "      <td>0.382197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23071</th>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>6.33</td>\n",
       "      <td>4.05</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.508226</td>\n",
       "      <td>0.244373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23073</th>\n",
       "      <td>4.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Burton</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.55</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2.42</td>\n",
       "      <td>True</td>\n",
       "      <td>0.557636</td>\n",
       "      <td>0.265238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23074</th>\n",
       "      <td>5.4</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Huddersfield</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.22</td>\n",
       "      <td>True</td>\n",
       "      <td>0.630757</td>\n",
       "      <td>0.320198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23079</th>\n",
       "      <td>4.2</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Sheffield Weds</td>\n",
       "      <td>5.99</td>\n",
       "      <td>3.52</td>\n",
       "      <td>1.74</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.680310</td>\n",
       "      <td>0.389612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23081</th>\n",
       "      <td>4.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Wolves</td>\n",
       "      <td>2.94</td>\n",
       "      <td>3.23</td>\n",
       "      <td>2.66</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.521840</td>\n",
       "      <td>0.202352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23083</th>\n",
       "      <td>4.6</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>5.01</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.578446</td>\n",
       "      <td>0.293546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23086</th>\n",
       "      <td>4.2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Monaco</td>\n",
       "      <td>8.09</td>\n",
       "      <td>4.27</td>\n",
       "      <td>1.50</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.529639</td>\n",
       "      <td>0.286330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23088</th>\n",
       "      <td>5.2</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Napoli</td>\n",
       "      <td>5.50</td>\n",
       "      <td>3.98</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.87</td>\n",
       "      <td>True</td>\n",
       "      <td>0.545884</td>\n",
       "      <td>0.287486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23090</th>\n",
       "      <td>2.6</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Granada</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.16</td>\n",
       "      <td>2.74</td>\n",
       "      <td>2.11</td>\n",
       "      <td>True</td>\n",
       "      <td>0.570672</td>\n",
       "      <td>0.249129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23091</th>\n",
       "      <td>6.2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>9.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Las Palmas</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.71</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.578656</td>\n",
       "      <td>0.296966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23092</th>\n",
       "      <td>2.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Osasuna</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.32</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.535004</td>\n",
       "      <td>0.219546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23095</th>\n",
       "      <td>3.2</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Dundee</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.62</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.597121</td>\n",
       "      <td>0.299502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23097</th>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Kilmarnock</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.26</td>\n",
       "      <td>True</td>\n",
       "      <td>0.525983</td>\n",
       "      <td>0.219234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23098</th>\n",
       "      <td>5.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>RB Leipzig</td>\n",
       "      <td>6.21</td>\n",
       "      <td>4.24</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.591672</td>\n",
       "      <td>0.342916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23100</th>\n",
       "      <td>7.2</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>H</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>4.35</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.53</td>\n",
       "      <td>True</td>\n",
       "      <td>0.544363</td>\n",
       "      <td>0.261077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23102</th>\n",
       "      <td>4.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Leicester</td>\n",
       "      <td>5.49</td>\n",
       "      <td>3.63</td>\n",
       "      <td>1.78</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.666304</td>\n",
       "      <td>0.384614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23110</th>\n",
       "      <td>6.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>4.78</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.45</td>\n",
       "      <td>True</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.247939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23113</th>\n",
       "      <td>5.2</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7.2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Pescara</td>\n",
       "      <td>3.37</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.37</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.556343</td>\n",
       "      <td>0.243843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23115</th>\n",
       "      <td>4.4</td>\n",
       "      <td>17.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Sassuolo</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.24</td>\n",
       "      <td>2.51</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.582298</td>\n",
       "      <td>0.267832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23116</th>\n",
       "      <td>5.2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Celta</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.68</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.562060</td>\n",
       "      <td>0.272205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23117</th>\n",
       "      <td>4.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Espanol</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.34</td>\n",
       "      <td>2.92</td>\n",
       "      <td>2.21</td>\n",
       "      <td>True</td>\n",
       "      <td>0.501734</td>\n",
       "      <td>0.190207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23122</th>\n",
       "      <td>4.6</td>\n",
       "      <td>15.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>11.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Bradford</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.19</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.509945</td>\n",
       "      <td>0.203196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25202</th>\n",
       "      <td>4.8</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.27</td>\n",
       "      <td>True</td>\n",
       "      <td>0.592095</td>\n",
       "      <td>0.286284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25205</th>\n",
       "      <td>4.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Dundee</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.48</td>\n",
       "      <td>2.55</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.569171</td>\n",
       "      <td>0.267056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25207</th>\n",
       "      <td>3.4</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>9.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>St Johnstone</td>\n",
       "      <td>2.89</td>\n",
       "      <td>3.52</td>\n",
       "      <td>2.53</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.624430</td>\n",
       "      <td>0.322316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25209</th>\n",
       "      <td>4.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11.4</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Partick</td>\n",
       "      <td>1.34</td>\n",
       "      <td>5.43</td>\n",
       "      <td>10.55</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.529413</td>\n",
       "      <td>0.335615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25214</th>\n",
       "      <td>5.2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>FC Koln</td>\n",
       "      <td>5.47</td>\n",
       "      <td>4.24</td>\n",
       "      <td>1.66</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.511227</td>\n",
       "      <td>0.267918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25215</th>\n",
       "      <td>4.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>2.72</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.556309</td>\n",
       "      <td>0.256008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25216</th>\n",
       "      <td>5.6</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>3.96</td>\n",
       "      <td>3.66</td>\n",
       "      <td>2.02</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.532114</td>\n",
       "      <td>0.243930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25222</th>\n",
       "      <td>4.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Lille</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.562920</td>\n",
       "      <td>0.264412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25223</th>\n",
       "      <td>5.2</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>Lorient</td>\n",
       "      <td>2.79</td>\n",
       "      <td>3.79</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.56</td>\n",
       "      <td>True</td>\n",
       "      <td>0.515524</td>\n",
       "      <td>0.234625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25226</th>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Nancy</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.66</td>\n",
       "      <td>2.11</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.697916</td>\n",
       "      <td>0.407218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25229</th>\n",
       "      <td>5.4</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Toulouse</td>\n",
       "      <td>2.93</td>\n",
       "      <td>3.43</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.34</td>\n",
       "      <td>True</td>\n",
       "      <td>0.521027</td>\n",
       "      <td>0.221625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25231</th>\n",
       "      <td>6.2</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Napoli</td>\n",
       "      <td>10.77</td>\n",
       "      <td>6.39</td>\n",
       "      <td>1.30</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.625794</td>\n",
       "      <td>0.461591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25233</th>\n",
       "      <td>3.6</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>9.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Leganes</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.57</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.36</td>\n",
       "      <td>True</td>\n",
       "      <td>0.589684</td>\n",
       "      <td>0.292065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25235</th>\n",
       "      <td>3.8</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Sp Gijon</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.65</td>\n",
       "      <td>2.54</td>\n",
       "      <td>2.44</td>\n",
       "      <td>True</td>\n",
       "      <td>0.523785</td>\n",
       "      <td>0.233087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25236</th>\n",
       "      <td>3.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Hamilton</td>\n",
       "      <td>5.14</td>\n",
       "      <td>3.83</td>\n",
       "      <td>1.76</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.509174</td>\n",
       "      <td>0.232930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25237</th>\n",
       "      <td>4.4</td>\n",
       "      <td>14.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Inverness C</td>\n",
       "      <td>5.28</td>\n",
       "      <td>4.26</td>\n",
       "      <td>1.66</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.684574</td>\n",
       "      <td>0.429472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25238</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Kilmarnock</td>\n",
       "      <td>2.61</td>\n",
       "      <td>3.49</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.672689</td>\n",
       "      <td>0.369659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25240</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Burnley</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.38</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.544086</td>\n",
       "      <td>0.249969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25242</th>\n",
       "      <td>7.2</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Hull</td>\n",
       "      <td>1.55</td>\n",
       "      <td>4.61</td>\n",
       "      <td>6.36</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.545583</td>\n",
       "      <td>0.318311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25244</th>\n",
       "      <td>2.4</td>\n",
       "      <td>16.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>27.82</td>\n",
       "      <td>9.31</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.507034</td>\n",
       "      <td>0.390890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25246</th>\n",
       "      <td>4.2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>4.73</td>\n",
       "      <td>3.95</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.520616</td>\n",
       "      <td>0.256764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25250</th>\n",
       "      <td>4.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.627357</td>\n",
       "      <td>0.329738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25254</th>\n",
       "      <td>2.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Sassuolo</td>\n",
       "      <td>4.69</td>\n",
       "      <td>3.89</td>\n",
       "      <td>1.82</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.511337</td>\n",
       "      <td>0.248870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25259</th>\n",
       "      <td>5.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Malaga</td>\n",
       "      <td>1.24</td>\n",
       "      <td>7.48</td>\n",
       "      <td>12.70</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.512226</td>\n",
       "      <td>0.366027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25263</th>\n",
       "      <td>5.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>St Johnstone</td>\n",
       "      <td>2.24</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.40</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.673154</td>\n",
       "      <td>0.372853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25266</th>\n",
       "      <td>4.8</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Bologna</td>\n",
       "      <td>1.78</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.69</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.566725</td>\n",
       "      <td>0.307658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25268</th>\n",
       "      <td>6.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Crotone</td>\n",
       "      <td>3.18</td>\n",
       "      <td>4.06</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.598771</td>\n",
       "      <td>0.344964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25271</th>\n",
       "      <td>5.2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Palermo</td>\n",
       "      <td>1.57</td>\n",
       "      <td>4.46</td>\n",
       "      <td>6.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.577490</td>\n",
       "      <td>0.341641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25273</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Sampdoria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.508348</td>\n",
       "      <td>0.380307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25274</th>\n",
       "      <td>4.8</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Torino</td>\n",
       "      <td>5.04</td>\n",
       "      <td>4.51</td>\n",
       "      <td>1.65</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.546495</td>\n",
       "      <td>0.321776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>883 rows Ã— 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       A_MEANS_FIVE_AC  A_MEANS_FIVE_AF  A_MEANS_FIVE_AR  A_MEANS_FIVE_AS  \\\n",
       "23053              4.0             12.0              0.0             10.0   \n",
       "23054              7.0              9.2              0.0             11.2   \n",
       "23058              3.4             13.4              0.0              6.2   \n",
       "23061              3.4             11.4              0.0              9.2   \n",
       "23062              4.8             14.6              0.2             12.6   \n",
       "23064              2.6             16.6              0.0              9.6   \n",
       "23068              4.0             11.2              0.0             13.8   \n",
       "23070              4.4             10.2              0.0             10.0   \n",
       "23071              7.0             14.0              0.0             12.0   \n",
       "23073              4.0             11.2              0.0             15.4   \n",
       "23074              5.4             14.0              0.0             13.8   \n",
       "23079              4.2             13.8              0.0              8.8   \n",
       "23081              4.4             11.2              0.0             10.6   \n",
       "23083              4.6             12.4              0.2             12.6   \n",
       "23086              4.2             15.0              0.0              7.0   \n",
       "23088              5.2             13.8              0.0             14.2   \n",
       "23090              2.6             14.4              0.4             10.8   \n",
       "23091              6.2             13.0              0.2              9.8   \n",
       "23092              2.0             14.6              0.0              6.2   \n",
       "23095              3.2             10.2              0.0              8.2   \n",
       "23097              5.0             13.0              0.0             13.2   \n",
       "23098              5.8             14.6              0.2             10.8   \n",
       "23100              7.2             13.2              0.0             17.6   \n",
       "23102              4.8             13.0              0.0             11.4   \n",
       "23110              6.6             12.2              0.0             10.2   \n",
       "23113              5.2             11.8              0.2              7.2   \n",
       "23115              4.4             17.6              0.4             10.4   \n",
       "23116              5.2             16.0              0.0             10.0   \n",
       "23117              4.0             13.4              0.2             10.0   \n",
       "23122              4.6             15.2              0.4             11.6   \n",
       "...                ...              ...              ...              ...   \n",
       "25202              4.8             13.8              0.2             10.2   \n",
       "25205              4.2             10.6              0.2             11.0   \n",
       "25207              3.4             14.6              0.6              9.6   \n",
       "25209              4.4             12.0              0.2             11.4   \n",
       "25214              5.2             16.0              0.2             11.0   \n",
       "25215              4.0             14.6              0.0             13.6   \n",
       "25216              5.6             12.8              0.0             13.4   \n",
       "25222              4.2             14.8              0.0             11.0   \n",
       "25223              5.2             12.4              0.2             12.6   \n",
       "25226              4.0             12.0              0.2             10.0   \n",
       "25229              5.4             13.4              0.2             11.2   \n",
       "25231              6.2             14.4              0.4             16.2   \n",
       "25233              3.6             14.4              0.2              9.2   \n",
       "25235              3.8             15.4              0.0             12.4   \n",
       "25236              3.0             14.2              0.2              5.6   \n",
       "25237              4.4             14.2              0.0              7.6   \n",
       "25238              5.0             11.6              0.0              8.0   \n",
       "25240              3.0              8.4              0.2             10.6   \n",
       "25242              7.2              9.2              0.0             18.4   \n",
       "25244              2.4             16.6              0.2              7.0   \n",
       "25246              4.2             13.4              0.0             10.6   \n",
       "25250              4.0             11.4              0.4             12.6   \n",
       "25254              2.4             11.0              0.0             11.2   \n",
       "25259              5.2             10.0              0.0             17.4   \n",
       "25263              5.8             15.0              0.0             12.0   \n",
       "25266              4.8              9.2              0.0             11.8   \n",
       "25268              6.0             13.2              0.0             15.0   \n",
       "25271              5.2             14.0              0.0             11.6   \n",
       "25273              5.0             11.4              0.0             14.8   \n",
       "25274              4.8             12.2              0.0              9.0   \n",
       "\n",
       "       A_MEANS_FIVE_AST  A_MEANS_FIVE_AY  A_MEANS_FIVE_FTAG  \\\n",
       "23053               4.6              2.2                1.2   \n",
       "23054               2.8              1.4                0.4   \n",
       "23058               1.6              2.2                0.4   \n",
       "23061               4.6              1.4                1.4   \n",
       "23062               5.0              1.6                1.0   \n",
       "23064               3.4              2.6                1.0   \n",
       "23068               5.6              1.8                1.4   \n",
       "23070               3.6              2.0                1.0   \n",
       "23071               3.0              3.0                0.8   \n",
       "23073               5.2              2.2                1.8   \n",
       "23074               4.2              2.0                0.6   \n",
       "23079               3.0              2.2                0.4   \n",
       "23081               4.0              2.0                0.8   \n",
       "23083               3.8              2.0                0.6   \n",
       "23086               2.4              2.6                0.6   \n",
       "23088               5.0              2.8                2.0   \n",
       "23090               0.8              3.0                0.2   \n",
       "23091               4.8              3.6                1.6   \n",
       "23092               2.0              3.8                1.0   \n",
       "23095               4.4              1.6                1.0   \n",
       "23097               4.4              3.2                1.4   \n",
       "23098               4.4              2.0                1.2   \n",
       "23100               5.8              1.8                1.6   \n",
       "23102               3.2              3.0                0.8   \n",
       "23110               3.4              2.6                1.0   \n",
       "23113               2.4              2.8                0.0   \n",
       "23115               3.4              2.6                1.0   \n",
       "23116               3.0              3.2                1.2   \n",
       "23117               3.6              2.4                1.2   \n",
       "23122               5.0              2.4                1.2   \n",
       "...                 ...              ...                ...   \n",
       "25202               4.4              1.6                1.2   \n",
       "25205               4.4              1.4                0.8   \n",
       "25207               2.6              2.2                0.2   \n",
       "25209               5.8              2.2                3.4   \n",
       "25214               4.2              2.0                0.8   \n",
       "25215               4.0              3.0                1.4   \n",
       "25216               5.4              1.8                0.8   \n",
       "25222               4.6              1.8                1.6   \n",
       "25223               3.4              2.0                1.0   \n",
       "25226               4.0              1.0                0.6   \n",
       "25229               5.0              2.0                0.8   \n",
       "25231               5.0              2.0                1.0   \n",
       "25233               3.0              2.4                1.4   \n",
       "25235               4.0              2.8                0.8   \n",
       "25236               2.4              1.8                1.0   \n",
       "25237               2.6              3.6                0.8   \n",
       "25238               2.4              2.0                1.0   \n",
       "25240               4.0              1.2                1.0   \n",
       "25242               7.2              1.4                2.4   \n",
       "25244               2.0              2.0                0.4   \n",
       "25246               2.4              2.2                0.4   \n",
       "25250               3.8              1.8                2.0   \n",
       "25254               4.4              0.8                1.4   \n",
       "25259               8.2              1.0                4.2   \n",
       "25263               5.0              2.2                1.4   \n",
       "25266               5.2              1.2                1.4   \n",
       "25268               5.2              3.4                1.8   \n",
       "25271               4.4              2.6                1.2   \n",
       "25273               5.8              2.2                2.8   \n",
       "25274               3.4              2.2                1.4   \n",
       "\n",
       "       A_MEANS_FIVE_FTHG  A_MEANS_FIVE_FTR_A  A_MEANS_FIVE_FTR_D  \\\n",
       "23053                1.2                 0.4                 0.0   \n",
       "23054                1.2                 0.0                 0.2   \n",
       "23058                1.8                 0.0                 0.4   \n",
       "23061                1.2                 0.2                 0.6   \n",
       "23062                1.0                 0.4                 0.2   \n",
       "23064                1.2                 0.2                 0.4   \n",
       "23068                1.0                 0.4                 0.2   \n",
       "23070                1.4                 0.2                 0.4   \n",
       "23071                1.2                 0.2                 0.2   \n",
       "23073                1.0                 0.6                 0.0   \n",
       "23074                1.2                 0.2                 0.4   \n",
       "23079                1.2                 0.2                 0.2   \n",
       "23081                0.8                 0.2                 0.4   \n",
       "23083                1.8                 0.0                 0.0   \n",
       "23086                0.8                 0.2                 0.4   \n",
       "23088                1.6                 0.4                 0.4   \n",
       "23090                1.8                 0.0                 0.4   \n",
       "23091                1.8                 0.2                 0.4   \n",
       "23092                1.8                 0.2                 0.2   \n",
       "23095                1.4                 0.2                 0.4   \n",
       "23097                2.2                 0.0                 0.6   \n",
       "23098                1.4                 0.4                 0.2   \n",
       "23100                0.6                 0.4                 0.6   \n",
       "23102                1.0                 0.2                 0.4   \n",
       "23110                1.6                 0.4                 0.0   \n",
       "23113                1.2                 0.0                 0.4   \n",
       "23115                1.2                 0.4                 0.2   \n",
       "23116                1.2                 0.4                 0.2   \n",
       "23117                1.4                 0.4                 0.0   \n",
       "23122                1.8                 0.4                 0.2   \n",
       "...                  ...                 ...                 ...   \n",
       "25202                0.8                 0.4                 0.2   \n",
       "25205                2.4                 0.0                 0.2   \n",
       "25207                1.4                 0.0                 0.2   \n",
       "25209                1.0                 0.8                 0.2   \n",
       "25214                1.4                 0.0                 0.4   \n",
       "25215                1.6                 0.4                 0.2   \n",
       "25216                1.0                 0.2                 0.2   \n",
       "25222                2.0                 0.4                 0.2   \n",
       "25223                1.2                 0.2                 0.4   \n",
       "25226                1.2                 0.4                 0.2   \n",
       "25229                2.2                 0.0                 0.4   \n",
       "25231                1.2                 0.2                 0.6   \n",
       "25233                1.4                 0.4                 0.2   \n",
       "25235                2.4                 0.2                 0.0   \n",
       "25236                1.4                 0.4                 0.0   \n",
       "25237                0.8                 0.4                 0.2   \n",
       "25238                1.0                 0.2                 0.6   \n",
       "25240                2.0                 0.0                 0.4   \n",
       "25242                0.6                 0.8                 0.0   \n",
       "25244                2.6                 0.0                 0.2   \n",
       "25246                1.4                 0.0                 0.4   \n",
       "25250                1.8                 0.4                 0.4   \n",
       "25254                1.6                 0.4                 0.0   \n",
       "25259                1.4                 1.0                 0.0   \n",
       "25263                0.8                 0.4                 0.4   \n",
       "25266                1.2                 0.4                 0.4   \n",
       "25268                1.4                 0.4                 0.4   \n",
       "25271                2.2                 0.4                 0.0   \n",
       "25273                0.8                 0.8                 0.2   \n",
       "25274                1.2                 0.4                 0.4   \n",
       "\n",
       "           ...        INFO_FTR  INFO_HTR   INFO_HomeTeam  INFO_PSA  INFO_PSD  \\\n",
       "23053      ...               H         A     Bristol Rvs      4.13      3.74   \n",
       "23054      ...               H         D        Coventry      4.99      3.85   \n",
       "23058      ...               H         D          Malaga      5.83      3.79   \n",
       "23061      ...               H         H   Ein Frankfurt      2.96      3.20   \n",
       "23062      ...               A         A        Freiburg      2.84      3.37   \n",
       "23064      ...               A         D      Ingolstadt      3.81      3.45   \n",
       "23068      ...               H         H         Chelsea      6.47      4.37   \n",
       "23070      ...               D         D        West Ham      4.13      3.59   \n",
       "23071      ...               H         D     Aston Villa      6.33      4.05   \n",
       "23073      ...               D         D          Burton      3.43      3.55   \n",
       "23074      ...               D         D    Huddersfield      3.73      3.26   \n",
       "23079      ...               A         D  Sheffield Weds      5.99      3.52   \n",
       "23081      ...               A         A          Wolves      2.94      3.23   \n",
       "23083      ...               H         H        Bordeaux      5.01      3.65   \n",
       "23086      ...               H         H          Monaco      8.09      4.27   \n",
       "23088      ...               D         D          Napoli      5.50      3.98   \n",
       "23090      ...               D         D         Granada      2.95      3.16   \n",
       "23091      ...               H         D      Las Palmas      4.04      3.71   \n",
       "23092      ...               A         D         Osasuna      3.54      3.25   \n",
       "23095      ...               H         H          Dundee      3.23      3.62   \n",
       "23097      ...               D         D      Kilmarnock      2.69      3.45   \n",
       "23098      ...               H         H      RB Leipzig      6.21      4.24   \n",
       "23100      ...               D         H         Arsenal      4.35      3.59   \n",
       "23102      ...               A         D       Leicester      5.49      3.63   \n",
       "23110      ...               D         D           Genoa      4.78      3.45   \n",
       "23113      ...               A         A         Pescara      3.37      3.33   \n",
       "23115      ...               A         A        Sassuolo      3.20      3.24   \n",
       "23116      ...               H         D           Celta      3.23      3.68   \n",
       "23117      ...               D         D         Espanol      2.65      3.34   \n",
       "23122      ...               H         H        Bradford      3.53      3.50   \n",
       "...        ...             ...       ...             ...       ...       ...   \n",
       "25202      ...               D         D     Southampton      2.80      3.33   \n",
       "25205      ...               A         A          Dundee      2.88      3.48   \n",
       "25207      ...               H         H    St Johnstone      2.89      3.52   \n",
       "25209      ...               A         A         Partick      1.34      5.43   \n",
       "25214      ...               H         H         FC Koln      5.47      4.24   \n",
       "25215      ...               H         D         Hamburg      2.72      3.50   \n",
       "25216      ...               A         A          Hertha      3.96      3.66   \n",
       "25222      ...               H         H           Lille      2.98      3.56   \n",
       "25223      ...               D         A         Lorient      2.79      3.79   \n",
       "25226      ...               H         H           Nancy      3.59      3.66   \n",
       "25229      ...               D         D        Toulouse      2.93      3.43   \n",
       "25231      ...               H         H          Napoli     10.77      6.39   \n",
       "25233      ...               D         D         Leganes      3.79      3.57   \n",
       "25235      ...               D         D        Sp Gijon      2.84      3.65   \n",
       "25236      ...               H         H        Hamilton      5.14      3.83   \n",
       "25237      ...               H         D     Inverness C      5.28      4.26   \n",
       "25238      ...               A         D      Kilmarnock      2.61      3.49   \n",
       "25240      ...               A         D         Burnley      3.26      3.40   \n",
       "25242      ...               A         A            Hull      1.55      4.61   \n",
       "25244      ...               H         H       Liverpool     27.82      9.31   \n",
       "25246      ...               A         D     Southampton      4.73      3.95   \n",
       "25250      ...               H         H           Genoa      4.00      3.40   \n",
       "25254      ...               H         H        Sassuolo      4.69      3.89   \n",
       "25259      ...               A         A          Malaga      1.24      7.48   \n",
       "25263      ...               A         A    St Johnstone      2.24      3.53   \n",
       "25266      ...               A         D         Bologna      1.78      3.99   \n",
       "25268      ...               H         H         Crotone      3.18      4.06   \n",
       "25271      ...               H         D         Palermo      1.57      4.46   \n",
       "25273      ...               A         A       Sampdoria       NaN       NaN   \n",
       "25274      ...               H         H          Torino      5.04      4.51   \n",
       "\n",
       "       INFO_PSH  INFO_WIN  pred      prob  prob_less_bet  \n",
       "23053      1.94     -1.00  True  0.548300       0.262586  \n",
       "23054      1.77     -1.00  True  0.504511       0.225960  \n",
       "23058      1.71     -1.00  True  0.572990       0.298265  \n",
       "23061      2.70     -1.00  True  0.575108       0.257647  \n",
       "23062      2.69     -1.00  True  0.591229       0.288199  \n",
       "23064      2.14     -1.00  True  0.674445       0.369567  \n",
       "23068      1.57     -1.00  True  0.562900       0.325933  \n",
       "23070      2.00      2.52  True  0.666288       0.382197  \n",
       "23071      1.61     -1.00  True  0.508226       0.244373  \n",
       "23073      2.21      2.42  True  0.557636       0.265238  \n",
       "23074      2.22      2.22  True  0.630757       0.320198  \n",
       "23079      1.74     -1.00  True  0.680310       0.389612  \n",
       "23081      2.66     -1.00  True  0.521840       0.202352  \n",
       "23083      1.81     -1.00  True  0.578446       0.293546  \n",
       "23086      1.50     -1.00  True  0.529639       0.286330  \n",
       "23088      1.70      2.87  True  0.545884       0.287486  \n",
       "23090      2.74      2.11  True  0.570672       0.249129  \n",
       "23091      1.99     -1.00  True  0.578656       0.296966  \n",
       "23092      2.32     -1.00  True  0.535004       0.219546  \n",
       "23095      2.28     -1.00  True  0.597121       0.299502  \n",
       "23097      2.75      2.26  True  0.525983       0.219234  \n",
       "23098      1.61     -1.00  True  0.591672       0.342916  \n",
       "23100      1.95      2.53  True  0.544363       0.261077  \n",
       "23102      1.78     -1.00  True  0.666304       0.384614  \n",
       "23110      1.92      2.45  True  0.537794       0.247939  \n",
       "23113      2.37     -1.00  True  0.556343       0.243843  \n",
       "23115      2.51     -1.00  True  0.582298       0.267832  \n",
       "23116      2.28     -1.00  True  0.562060       0.272205  \n",
       "23117      2.92      2.21  True  0.501734       0.190207  \n",
       "23122      2.19     -1.00  True  0.509945       0.203196  \n",
       "...         ...       ...   ...       ...            ...  \n",
       "25202      2.76      2.27  True  0.592095       0.286284  \n",
       "25205      2.55     -1.00  True  0.569171       0.267056  \n",
       "25207      2.53     -1.00  True  0.624430       0.322316  \n",
       "25209     10.55     -1.00  True  0.529413       0.335615  \n",
       "25214      1.66     -1.00  True  0.511227       0.267918  \n",
       "25215      2.72     -1.00  True  0.556309       0.256008  \n",
       "25216      2.02     -1.00  True  0.532114       0.243930  \n",
       "25222      2.45     -1.00  True  0.562920       0.264412  \n",
       "25223      2.49      2.56  True  0.515524       0.234625  \n",
       "25226      2.11     -1.00  True  0.697916       0.407218  \n",
       "25229      2.55      2.34  True  0.521027       0.221625  \n",
       "25231      1.30     -1.00  True  0.625794       0.461591  \n",
       "25233      2.08      2.36  True  0.589684       0.292065  \n",
       "25235      2.54      2.44  True  0.523785       0.233087  \n",
       "25236      1.76     -1.00  True  0.509174       0.232930  \n",
       "25237      1.66     -1.00  True  0.684574       0.429472  \n",
       "25238      2.82     -1.00  True  0.672689       0.369659  \n",
       "25240      2.38     -1.00  True  0.544086       0.249969  \n",
       "25242      6.36     -1.00  True  0.545583       0.318311  \n",
       "25244      1.14     -1.00  True  0.507034       0.390890  \n",
       "25246      1.80     -1.00  True  0.520616       0.256764  \n",
       "25250      2.08     -1.00  True  0.627357       0.329738  \n",
       "25254      1.82     -1.00  True  0.511337       0.248870  \n",
       "25259     12.70     -1.00  True  0.512226       0.366027  \n",
       "25263      3.40     -1.00  True  0.673154       0.372853  \n",
       "25266      4.69     -1.00  True  0.566725       0.307658  \n",
       "25268      2.15     -1.00  True  0.598771       0.344964  \n",
       "25271      6.00     -1.00  True  0.577490       0.341641  \n",
       "25273       NaN     -1.00  True  0.508348       0.380307  \n",
       "25274      1.65     -1.00  True  0.546495       0.321776  \n",
       "\n",
       "[883 rows x 194 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bet_current_season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot a leqrning curve\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='f1')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "plot_learning_curve(clf, 'Learning Curve', X, y, cv=4, n_jobs=-1).show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "prob_pos = y_probs[:,1]\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=20)\n",
    "\n",
    "ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",label=\"%s\" % ('Classifier', ))\n",
    "\n",
    "ax2.hist(prob_iso_pos, range=(0, 1), bins=10, label='Classifier',histtype=\"step\", lw=2)\n",
    "\n",
    "ax1.set_ylabel(\"Fraction of positives\")\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.set_title('Calibration plots  (reliability curve)')\n",
    "\n",
    "ax2.set_xlabel(\"Mean predicted value\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "iso = CalibratedClassifierCV(clf, cv=4, method='isotonic')\n",
    "iso.fit(X_train, y_train)\n",
    "y_iso_probs = iso.predict_proba(X_test)\n",
    "prob_iso_pos = y_iso_probs[:,1]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_iso_pos, n_bins=20)\n",
    "\n",
    "ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",label=\"%s\" % ('Classifier', ))\n",
    "\n",
    "ax2.hist(prob_pos, range=(0, 1), bins=10, label='Classifier',histtype=\"step\", lw=2)\n",
    "\n",
    "ax1.set_ylabel(\"Fraction of positives\")\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.set_title('Calibration plots  (reliability curve)')\n",
    "\n",
    "ax2.set_xlabel(\"Mean predicted value\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_iso_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
