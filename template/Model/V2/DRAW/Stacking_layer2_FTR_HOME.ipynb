{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "import matplotlib.pyplot as plt # plotting library\n",
    "from IPython.display import display # Manage multiple output per cell\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import sickit methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define target\n",
    "target = 'INFO_FTR_H'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all different features dataset\n",
    "all_feature_L2 = ['A_MEANS_FIVE_AC','A_MEANS_FIVE_AF','A_MEANS_FIVE_AR','A_MEANS_FIVE_AS','A_MEANS_FIVE_AST','A_MEANS_FIVE_AY','A_MEANS_FIVE_FTAG','A_MEANS_FIVE_FTHG','A_MEANS_FIVE_FTR_A','A_MEANS_FIVE_FTR_D','A_MEANS_FIVE_FTR_H','A_MEANS_FIVE_HC','A_MEANS_FIVE_HF','A_MEANS_FIVE_HR','A_MEANS_FIVE_HS','A_MEANS_FIVE_HST','A_MEANS_FIVE_HTAG','A_MEANS_FIVE_HTHG','A_MEANS_FIVE_HTR_A','A_MEANS_FIVE_HTR_D','A_MEANS_FIVE_HTR_H','A_MEANS_FIVE_HY','H_MEANS_FIVE_AC','H_MEANS_FIVE_AF','H_MEANS_FIVE_AR','H_MEANS_FIVE_AS','H_MEANS_FIVE_AST','H_MEANS_FIVE_AY','H_MEANS_FIVE_FTAG','H_MEANS_FIVE_FTHG','H_MEANS_FIVE_FTR_A','H_MEANS_FIVE_FTR_D','H_MEANS_FIVE_FTR_H','H_MEANS_FIVE_HC','H_MEANS_FIVE_HF','H_MEANS_FIVE_HR','H_MEANS_FIVE_HS','H_MEANS_FIVE_HST','H_MEANS_FIVE_HTAG','H_MEANS_FIVE_HTHG','H_MEANS_FIVE_HTR_A','H_MEANS_FIVE_HTR_D','H_MEANS_FIVE_HTR_H','H_MEANS_FIVE_HY','A_MEANS_THREE_AC','A_MEANS_THREE_AF','A_MEANS_THREE_AR','A_MEANS_THREE_AS','A_MEANS_THREE_AST','A_MEANS_THREE_AY','A_MEANS_THREE_FTAG','A_MEANS_THREE_FTHG','A_MEANS_THREE_FTR_A','A_MEANS_THREE_FTR_D','A_MEANS_THREE_FTR_H','A_MEANS_THREE_HC','A_MEANS_THREE_HF','A_MEANS_THREE_HR','A_MEANS_THREE_HS','A_MEANS_THREE_HST','A_MEANS_THREE_HTAG','A_MEANS_THREE_HTHG','A_MEANS_THREE_HTR_A','A_MEANS_THREE_HTR_D','A_MEANS_THREE_HTR_H','A_MEANS_THREE_HY','H_MEANS_THREE_AC','H_MEANS_THREE_AF','H_MEANS_THREE_AR','H_MEANS_THREE_AS','H_MEANS_THREE_AST','H_MEANS_THREE_AY','H_MEANS_THREE_FTAG','H_MEANS_THREE_FTHG','H_MEANS_THREE_FTR_A','H_MEANS_THREE_FTR_D','H_MEANS_THREE_FTR_H','H_MEANS_THREE_HC','H_MEANS_THREE_HF','H_MEANS_THREE_HR','H_MEANS_THREE_HS','H_MEANS_THREE_HST','H_MEANS_THREE_HTAG','H_MEANS_THREE_HTHG','H_MEANS_THREE_HTR_A','H_MEANS_THREE_HTR_D','H_MEANS_THREE_HTR_H','H_MEANS_THREE_HY','A_STD_FIVE_AC','A_STD_FIVE_AF','A_STD_FIVE_AR','A_STD_FIVE_AS','A_STD_FIVE_AST','A_STD_FIVE_AY','A_STD_FIVE_FTAG','A_STD_FIVE_FTHG','A_STD_FIVE_FTR_A','A_STD_FIVE_FTR_D','A_STD_FIVE_FTR_H','A_STD_FIVE_HC','A_STD_FIVE_HF','A_STD_FIVE_HR','A_STD_FIVE_HS','A_STD_FIVE_HST','A_STD_FIVE_HTAG','A_STD_FIVE_HTHG','A_STD_FIVE_HTR_A','A_STD_FIVE_HTR_D','A_STD_FIVE_HTR_H','A_STD_FIVE_HY','H_STD_FIVE_AC','H_STD_FIVE_AF','H_STD_FIVE_AR','H_STD_FIVE_AS','H_STD_FIVE_AST','H_STD_FIVE_AY','H_STD_FIVE_FTAG','H_STD_FIVE_FTHG','H_STD_FIVE_FTR_A','H_STD_FIVE_FTR_D','H_STD_FIVE_FTR_H','H_STD_FIVE_HC','H_STD_FIVE_HF','H_STD_FIVE_HR','H_STD_FIVE_HS','H_STD_FIVE_HST','H_STD_FIVE_HTAG','H_STD_FIVE_HTHG','H_STD_FIVE_HTR_A','H_STD_FIVE_HTR_D','H_STD_FIVE_HTR_H','H_STD_FIVE_HY','A_STD_THREE_AC','A_STD_THREE_AF','A_STD_THREE_AR','A_STD_THREE_AS','A_STD_THREE_AST','A_STD_THREE_AY','A_STD_THREE_FTAG','A_STD_THREE_FTHG','A_STD_THREE_FTR_A','A_STD_THREE_FTR_D','A_STD_THREE_FTR_H','A_STD_THREE_HC','A_STD_THREE_HF','A_STD_THREE_HR','A_STD_THREE_HS','A_STD_THREE_HST','A_STD_THREE_HTAG','A_STD_THREE_HTHG','A_STD_THREE_HTR_A','A_STD_THREE_HTR_D','A_STD_THREE_HTR_H','A_STD_THREE_HY','H_STD_THREE_AC','H_STD_THREE_AF','H_STD_THREE_AR','H_STD_THREE_AS','H_STD_THREE_AST','H_STD_THREE_AY','H_STD_THREE_FTAG','H_STD_THREE_FTHG','H_STD_THREE_FTR_A','H_STD_THREE_FTR_D','H_STD_THREE_FTR_H','H_STD_THREE_HC','H_STD_THREE_HF','H_STD_THREE_HR','H_STD_THREE_HS','H_STD_THREE_HST','H_STD_THREE_HTAG','H_STD_THREE_HTHG','H_STD_THREE_HTR_A','H_STD_THREE_HTR_D','H_STD_THREE_HTR_H','H_STD_THREE_HY','INFO_Div_D1','INFO_Div_E0','INFO_Div_E1','INFO_Div_E2','INFO_Div_F1','INFO_Div_I1','INFO_Div_SC0','INFO_Div_SP1','DNN_adam_all','DNN_adam_bf','DNN_sgd_all','DNN_sgd_bf','XGBoost_all','XGBoost_bf','SVM','RF_all','RF_bf','NB','MLP','LDA','KNN','ExraTree','ElasticNEt','Bagging_max_sample','Bagging_min_sample_all','Bagging_min_sample_bf','AdaBoost','GB','LR']\n",
    "best_features_L2_NB = ['A_MEANS_FIVE_FTAG', 'A_MEANS_FIVE_HF', 'A_MEANS_FIVE_HS','A_MEANS_FIVE_HTR_D', 'H_MEANS_FIVE_AF', 'H_MEANS_FIVE_AY','H_MEANS_FIVE_HF', 'A_MEANS_THREE_AF', 'A_MEANS_THREE_HF','H_MEANS_THREE_AF', 'A_STD_FIVE_HF', 'H_STD_FIVE_AS','H_STD_FIVE_AST', 'H_STD_FIVE_FTHG', 'H_STD_FIVE_HTAG','H_STD_FIVE_HY', 'DNN_adam_all', 'DNN_adam_bf', 'DNN_sgd_all','DNN_sgd_bf', 'XGBoost_all', 'XGBoost_bf', 'SVM', 'RF_all','RF_bf', 'NB', 'MLP', 'LDA', 'KNN', 'ExraTree', 'ElasticNEt','Bagging_max_sample', 'Bagging_min_sample_all','Bagging_min_sample_bf', 'AdaBoost', 'GB']\n",
    "best_features_L2_MLP = ['DNN_sgd_bf', 'RF_all', 'RF_bf', 'NB', 'MLP', 'LDA', 'KNN','Bagging_min_sample_bf']\n",
    "best_feature_L2_ET = ['A_MEANS_FIVE_FTAG', 'A_MEANS_FIVE_HS', 'H_MEANS_FIVE_AF','H_MEANS_FIVE_HF', 'A_MEANS_THREE_AF', 'A_MEANS_THREE_HF','H_MEANS_THREE_AF', 'A_STD_FIVE_HF', 'H_STD_FIVE_AS','H_STD_FIVE_AST', 'H_STD_FIVE_HY', 'DNN_adam_all', 'DNN_adam_bf','DNN_sgd_bf', 'XGBoost_all', 'XGBoost_bf', 'SVM', 'RF_all','RF_bf', 'NB', 'MLP', 'LDA', 'KNN', 'ElasticNEt','Bagging_max_sample', 'Bagging_min_sample_bf', 'AdaBoost', 'GB']\n",
    "best_features_L2_LR = ['A_MEANS_FIVE_AC','A_MEANS_FIVE_AF','A_MEANS_FIVE_AS','A_MEANS_FIVE_AST','A_MEANS_FIVE_AY','A_MEANS_FIVE_FTAG','A_MEANS_FIVE_FTR_H','A_MEANS_FIVE_HC','A_MEANS_FIVE_HF','A_MEANS_FIVE_HS','A_MEANS_FIVE_HTR_D','A_MEANS_FIVE_HTR_H','A_MEANS_FIVE_HY','H_MEANS_FIVE_AC','H_MEANS_FIVE_AF','H_MEANS_FIVE_AS','H_MEANS_FIVE_AST','H_MEANS_FIVE_AY','H_MEANS_FIVE_FTAG','H_MEANS_FIVE_FTHG','H_MEANS_FIVE_FTR_A','H_MEANS_FIVE_HC','H_MEANS_FIVE_HF','H_MEANS_FIVE_HR','H_MEANS_FIVE_HST','H_MEANS_FIVE_HTAG','H_MEANS_FIVE_HTR_H','H_MEANS_FIVE_HY','A_MEANS_THREE_AC','A_MEANS_THREE_AF','A_MEANS_THREE_AS','A_MEANS_THREE_FTHG','A_MEANS_THREE_HC','A_MEANS_THREE_HF','A_MEANS_THREE_HS','A_MEANS_THREE_HST','A_MEANS_THREE_HTR_D','A_MEANS_THREE_HY','H_MEANS_THREE_AC','H_MEANS_THREE_AF','H_MEANS_THREE_FTHG','H_MEANS_THREE_HF','H_MEANS_THREE_HS','H_MEANS_THREE_HST','H_MEANS_THREE_HY','A_STD_FIVE_AC','A_STD_FIVE_AF','A_STD_FIVE_AST','A_STD_FIVE_AY','A_STD_FIVE_FTAG','A_STD_FIVE_FTHG','A_STD_FIVE_FTR_D','A_STD_FIVE_FTR_H','A_STD_FIVE_HC','A_STD_FIVE_HF','A_STD_FIVE_HS','A_STD_FIVE_HST','A_STD_FIVE_HY','H_STD_FIVE_AC','H_STD_FIVE_AF','H_STD_FIVE_AS','H_STD_FIVE_AST','H_STD_FIVE_AY','H_STD_FIVE_FTAG','H_STD_FIVE_FTHG','H_STD_FIVE_HF','H_STD_FIVE_HS','H_STD_FIVE_HST','H_STD_FIVE_HTAG','H_STD_FIVE_HTHG','H_STD_FIVE_HY','A_STD_THREE_AF','A_STD_THREE_AS','A_STD_THREE_AST','A_STD_THREE_FTHG','A_STD_THREE_HC','A_STD_THREE_HF','A_STD_THREE_HS','A_STD_THREE_HST','H_STD_THREE_AF','H_STD_THREE_AS','H_STD_THREE_AST','H_STD_THREE_FTAG','H_STD_THREE_FTHG','H_STD_THREE_HF','H_STD_THREE_HS','H_STD_THREE_HST','H_STD_THREE_HTHG','H_STD_THREE_HY','INFO_Div_E1','DNN_adam_all','DNN_adam_bf','DNN_sgd_all','DNN_sgd_bf','XGBoost_all','XGBoost_bf','SVM','RF_all','RF_bf','NB','MLP','LDA','KNN','ExraTree','ElasticNEt','Bagging_max_sample','Bagging_min_sample_all','Bagging_min_sample_bf','AdaBoost','GB','LR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct layer 2\n",
    "layer_2 = [\n",
    "    ['NB', False, 'isotonic', GaussianNB(), best_features_L2_NB],\n",
    "    ['XGBoost_all', False, 'sigmoid', XGBClassifier(learning_rate =0.01,n_estimators=572,max_depth=3,min_child_weight=5,gamma=0,subsample=0.9,colsample_bytree=0.9,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,seed=15), all_feature_L2],\n",
    "    ['ExraTree', False, 'isotonic', ExtraTreesClassifier(random_state=0,n_jobs=-1,criterion='entropy',max_depth=30,max_features=None,min_samples_leaf=1,min_samples_split=16,n_estimators=350), best_feature_L2_ET],\n",
    "    ['MLP', False, 'no', MLPClassifier(random_state=0,activation='identity', alpha=3, hidden_layer_sizes=(10,), max_iter=200, solver= 'adam'), best_features_L2_MLP],\n",
    "    ['LR', False, 'no', LogisticRegression(random_state=0,n_jobs=-1,solver='liblinear',C=0.008,penalty='l1'), best_features_L2_LR]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "train_df = pd.DataFrame.from_csv('./report/layer2-train-INFO_FTR_H-2017-07-10-15-56.csv')\n",
    "test_df = pd.DataFrame.from_csv('./report/layer2-test-INFO_FTR_H-2017-07-10-15-56.csv')\n",
    "X_train_df =  train_df.drop(target, 1)\n",
    "X_test_df = test_df.drop(target, 1)\n",
    "y_train_df = train_df[target].astype('bool_')\n",
    "y_test_df= test_df[target].astype('bool_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "ntrain = X_train_df.shape[0]\n",
    "ntest = X_test_df.shape[0]\n",
    "NFOLDS = 4\n",
    "kf = KFold(ntrain, n_folds=NFOLDS, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, x_test):\n",
    "    oof_train = np.zeros((x_train.shape[0],))\n",
    "    oof_test = np.zeros((x_test.shape[0],))\n",
    "    oof_test_skf = np.empty((NFOLDS, x_test.shape[0]))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train_df[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        # fit the model\n",
    "        clf.fit(x_tr, y_tr)\n",
    "        # Predict the K-th fold and the test set\n",
    "        oof_train[test_index] = clf.predict_proba(x_te)[:,1]\n",
    "        oof_test_skf[i, :] = clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: NB\n",
      "Processing model: XGBoost_all\n",
      "Processing model: ExraTree\n",
      "Processing model: MLP\n",
      "Processing model: LR\n",
      "Finish after 291 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute out-of-fold predictions for layer 2\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# Keep prediction of test in array\n",
    "X_train_layer3 = np.zeros((X_train_df.shape[0], len(layer_2)))\n",
    "X_train_layer3 = pd.DataFrame(X_train_layer3, columns=[clf_name for clf_name, preprocessing, calibration, clf, features in layer_2])\n",
    "\n",
    "X_test_layer3 = np.zeros((X_test_df.shape[0], len(layer_2)))\n",
    "X_test_layer3 = pd.DataFrame(X_test_layer3, columns=[clf_name for clf_name, preprocessing, calibration, clf, features in layer_2])\n",
    "\n",
    "for clf_name, preprocessing, calibration, classifier, features in layer_2:\n",
    "    print \"Processing model:\",clf_name\n",
    "    # Check if we need to standardize X for this model\n",
    "    if preprocessing:\n",
    "        sc_X = StandardScaler().fit(X_train_df[features])\n",
    "        X_train_model = sc_X.transform(X_train_df[features])\n",
    "        X_test_model = sc_X.transform(X_test_df[features])\n",
    "    else:\n",
    "        X_train_model = X_train_df[features].as_matrix()\n",
    "        X_test_model = X_test_df[features].as_matrix()\n",
    "    # Check if we need to recalibrate the prediction\n",
    "    if calibration == 'sigmoid':\n",
    "        clf = CalibratedClassifierCV(classifier, cv=4, method='sigmoid')\n",
    "    elif calibration == 'isotonic':\n",
    "        clf = CalibratedClassifierCV(classifier, cv=4, method='isotonic')\n",
    "    elif calibration == 'no':\n",
    "        clf = classifier\n",
    "    # obtain out-of-fold predictions for this model\n",
    "    oof_train, oof_test = get_oof(clf, X_train_model, X_test_model)\n",
    "    X_train_layer3.loc[:, clf_name] = oof_train\n",
    "    X_test_layer3.loc[:, clf_name] = oof_test\n",
    "X_train_layer3.to_csv('./report/'+start.strftime(\"%Y-%m-%d-%H-%M\")+'-layer2-train-pred.csv')\n",
    "X_test_layer3.to_csv('./report/'+start.strftime(\"%Y-%m-%d-%H-%M\")+'-layer2-test-pred.csv')\n",
    "\n",
    "print(\"Finish after %d seconds\" % (datetime.datetime.now() - start).total_seconds())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concat prediction with initial features\n",
    "X_train_layer3_df = pd.concat([X_train_df, X_train_layer3, y_train_df], axis=1)\n",
    "X_test_layer3_df = pd.concat([X_test_df, X_test_layer3, y_test_df], axis=1)\n",
    "X_train_layer3_df.to_csv('./report/layer3-train-'+target+'-'+start.strftime(\"%Y-%m-%d-%H-%M\")+'.csv')\n",
    "X_test_layer3_df.to_csv('./report/layer3-test-'+target+'-'+start.strftime(\"%Y-%m-%d-%H-%M\")+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.653418416725\n",
      "0.549233151809\n",
      "0.585312172683\n",
      "0.517677775706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(GaussianNB(), X_train_layer3, y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(GaussianNB(), X_train_layer3, y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(GaussianNB(), X_train_layer3, y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(GaussianNB(), X_train_layer3, y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65328057223\n",
      "0.52954941174\n",
      "0.596861526648\n",
      "0.476413930685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(CalibratedClassifierCV(GaussianNB(), cv=8, method='isotonic'), X_train_layer3, y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(CalibratedClassifierCV(GaussianNB(), cv=8, method='isotonic'), X_train_layer3, y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(CalibratedClassifierCV(GaussianNB(), cv=8, method='isotonic'), X_train_layer3, y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(CalibratedClassifierCV(GaussianNB(), cv=8, method='isotonic'), X_train_layer3, y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.651937693156\n",
      "0.407118128443\n",
      "0.643071204041\n",
      "0.298108656465\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(SVC(), X_train_layer3, y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(SVC(), X_train_layer3, y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(SVC(), X_train_layer3, y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(SVC(), X_train_layer3, y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.654294887209\n",
      "0.518045887857\n",
      "0.604133858895\n",
      "0.453775062839\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(LogisticRegression(), X_train_layer3, y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(LogisticRegression(), X_train_layer3, y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(LogisticRegression(), X_train_layer3, y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(LogisticRegression(), X_train_layer3, y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652503077371\n",
      "0.193334210824\n",
      "0.736787502851\n",
      "0.111788955451\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(LogisticRegression(random_state=0,n_jobs=-1,solver='liblinear',C=0.008,penalty='l1'), X_train_layer3, y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(LogisticRegression(random_state=0,n_jobs=-1,solver='liblinear',C=0.008,penalty='l1'), X_train_layer3, y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(LogisticRegression(random_state=0,n_jobs=-1,solver='liblinear',C=0.008,penalty='l1'), X_train_layer3, y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(LogisticRegression(random_state=0,n_jobs=-1,solver='liblinear',C=0.008,penalty='l1'), X_train_layer3, y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.653256474931\n",
      "0.503531244165\n",
      "0.60731766898\n",
      "0.431318302428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(MLPClassifier(random_state=0,activation='identity', alpha=1, hidden_layer_sizes=(50,), max_iter=200, solver= 'adam'), X_train_layer3, y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(MLPClassifier(random_state=0,activation='identity', alpha=1, hidden_layer_sizes=(50,), max_iter=200, solver= 'adam'), X_train_layer3, y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(MLPClassifier(random_state=0,activation='identity', alpha=1, hidden_layer_sizes=(50,), max_iter=200, solver= 'adam'), X_train_layer3, y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(MLPClassifier(random_state=0,activation='identity', alpha=1, hidden_layer_sizes=(50,), max_iter=200, solver= 'adam'), X_train_layer3, y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65336467758\n",
      "0.511957338765\n",
      "0.603005278506\n",
      "0.449072077637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(MLPClassifier(random_state=0), X_train_layer3, y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(MLPClassifier(random_state=0), X_train_layer3, y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(MLPClassifier(random_state=0), X_train_layer3, y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(MLPClassifier(random_state=0), X_train_layer3, y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.653702983787\n",
      "0.515867766543\n",
      "0.606613097232\n",
      "0.456896627079\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3, y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3, y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3, y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3, y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.653335205099\n",
      "0.516535397684\n",
      "0.608402041969\n",
      "0.455159807772\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'ExraTree', 'LR']], y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'ExraTree', 'LR']], y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'ExraTree', 'LR']], y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'ExraTree', 'LR']], y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.653335205099\n",
      "0.516535397684\n",
      "0.608402041969\n",
      "0.455159807772\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'ExraTree', 'LR']], y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'ExraTree', 'LR']], y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'ExraTree', 'LR']], y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'ExraTree', 'LR']], y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.653029340108\n",
      "0.51713449198\n",
      "0.607562649256\n",
      "0.454813318027\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['MLP', 'ExraTree', 'XGBoost_all']], y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['MLP', 'ExraTree', 'XGBoost_all']], y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['MLP', 'ExraTree', 'XGBoost_all']], y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['MLP', 'ExraTree', 'XGBoost_all']], y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.651400296719\n",
      "0.515670751695\n",
      "0.601057068052\n",
      "0.455857157842\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['XGBoost_all', 'ExraTree', 'LR']], y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['XGBoost_all', 'ExraTree', 'LR']], y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['XGBoost_all', 'ExraTree', 'LR']], y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['XGBoost_all', 'ExraTree', 'LR']], y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652111710658\n",
      "0.513081893268\n",
      "0.606562513816\n",
      "0.450634316616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'XGBoost_all', 'LR']], y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'XGBoost_all', 'LR']], y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'XGBoost_all', 'LR']], y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'XGBoost_all', 'LR']], y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.651400296719\n",
      "0.515670751695\n",
      "0.601057068052\n",
      "0.455857157842\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['ExraTree', 'XGBoost_all', 'LR']], y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['ExraTree', 'XGBoost_all', 'LR']], y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['ExraTree', 'XGBoost_all', 'LR']], y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['ExraTree', 'XGBoost_all', 'LR']], y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'ExraTree', 'MLP', 'LR', 'XGBoost_all']], y_train_df, cv=8, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'ExraTree', 'MLP', 'LR', 'XGBoost_all']], y_train_df, cv=8, scoring='f1', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'ExraTree', 'MLP', 'LR', 'XGBoost_all']], y_train_df, cv=8, scoring='precision', n_jobs=-1).mean()\n",
    "print cross_val_score(SGDClassifier(random_state=0,penalty='elasticnet',loss='log',alpha=0.0005,l1_ratio=0.4), X_train_layer3[['NB', 'ExraTree', 'MLP', 'LR', 'XGBoost_all']], y_train_df, cv=8, scoring='recall', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = X_train_layer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df['join'] = (join_df['NB'] + join_df['MLP'] + join_df['LR']) / 3\n",
    "join_df['join_probs'] = 0\n",
    "join_df.loc[join_df['join'] >= 0.5, 'join_probs'] = 1\n",
    "join_df['nb_probs'] = 0\n",
    "join_df.loc[join_df['NB'] >= 0.5, 'nb_probs'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_train_df, join_df['nb_probs'], average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score, roc_auc_score\n",
    "for col_pred in join_df:\n",
    "    logloss = log_loss(y_train_df, join_df[col_pred].as_matrix())\n",
    "    auc = roc_auc_score(y_train_df, join_df[col_pred].as_matrix())\n",
    "    print(\"%5s -> log_loss: %.4f, auc: %.4f\" % (col_pred, logloss, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0-0-1 -> log_loss: 0.6567, precision: 0.5917, recall: 0.4409, f1: 0.5053, acc: 0.6071, auc: 0.6427\n",
      " 0-0.1-0.9 -> log_loss: 0.6562, precision: 0.5928, recall: 0.4388, f1: 0.5043, acc: 0.6074, auc: 0.6444\n",
      " 0-0.2-0.8 -> log_loss: 0.6558, precision: 0.5952, recall: 0.4358, f1: 0.5032, acc: 0.6083, auc: 0.6460\n",
      " 0-0.3-0.7 -> log_loss: 0.6555, precision: 0.5986, recall: 0.4320, f1: 0.5018, acc: 0.6096, auc: 0.6476\n",
      " 0-0.4-0.6 -> log_loss: 0.6553, precision: 0.6007, recall: 0.4301, f1: 0.5013, acc: 0.6105, auc: 0.6491\n",
      " 0-0.5-0.5 -> log_loss: 0.6551, precision: 0.6045, recall: 0.4277, f1: 0.5009, acc: 0.6121, auc: 0.6505\n",
      " 0-0.6-0.4 -> log_loss: 0.6551, precision: 0.6066, recall: 0.4235, f1: 0.4988, acc: 0.6126, auc: 0.6518\n",
      " 0-0.7-0.3 -> log_loss: 0.6551, precision: 0.6107, recall: 0.4202, f1: 0.4978, acc: 0.6142, auc: 0.6528\n",
      " 0-0.8-0.2 -> log_loss: 0.6551, precision: 0.6147, recall: 0.4172, f1: 0.4970, acc: 0.6157, auc: 0.6537\n",
      " 0-0.9-0.1 -> log_loss: 0.6553, precision: 0.6178, recall: 0.4129, f1: 0.4949, acc: 0.6165, auc: 0.6544\n",
      "     0-1-0 -> log_loss: 0.6555, precision: 0.6169, recall: 0.4090, f1: 0.4919, acc: 0.6154, auc: 0.6548\n",
      " 0.1-0-0.9 -> log_loss: 0.6555, precision: 0.5913, recall: 0.4419, f1: 0.5058, acc: 0.6070, auc: 0.6452\n",
      "0.1-0.1-0.8 -> log_loss: 0.6551, precision: 0.5947, recall: 0.4390, f1: 0.5051, acc: 0.6085, auc: 0.6467\n",
      "0.1-0.2-0.7 -> log_loss: 0.6547, precision: 0.5972, recall: 0.4360, f1: 0.5040, acc: 0.6094, auc: 0.6481\n",
      "0.1-0.3-0.6 -> log_loss: 0.6544, precision: 0.6018, recall: 0.4353, f1: 0.5052, acc: 0.6119, auc: 0.6495\n",
      "0.1-0.4-0.5 -> log_loss: 0.6542, precision: 0.6026, recall: 0.4310, f1: 0.5025, acc: 0.6117, auc: 0.6508\n",
      "0.1-0.5-0.4 -> log_loss: 0.6541, precision: 0.6049, recall: 0.4266, f1: 0.5004, acc: 0.6122, auc: 0.6519\n",
      "0.1-0.6-0.3 -> log_loss: 0.6541, precision: 0.6086, recall: 0.4245, f1: 0.5002, acc: 0.6138, auc: 0.6529\n",
      "0.1-0.7-0.2 -> log_loss: 0.6542, precision: 0.6135, recall: 0.4210, f1: 0.4994, acc: 0.6158, auc: 0.6537\n",
      "0.1-0.8-0.1 -> log_loss: 0.6543, precision: 0.6156, recall: 0.4174, f1: 0.4975, acc: 0.6162, auc: 0.6543\n",
      " 0.1-0.9-0 -> log_loss: 0.6545, precision: 0.6149, recall: 0.4118, f1: 0.4933, acc: 0.6149, auc: 0.6546\n",
      " 0.2-0-0.8 -> log_loss: 0.6545, precision: 0.5949, recall: 0.4442, f1: 0.5086, acc: 0.6094, auc: 0.6472\n",
      "0.2-0.1-0.7 -> log_loss: 0.6541, precision: 0.5974, recall: 0.4416, f1: 0.5078, acc: 0.6104, auc: 0.6486\n",
      "0.2-0.2-0.6 -> log_loss: 0.6537, precision: 0.6009, recall: 0.4386, f1: 0.5071, acc: 0.6119, auc: 0.6498\n",
      "0.2-0.3-0.5 -> log_loss: 0.6535, precision: 0.6015, recall: 0.4348, f1: 0.5048, acc: 0.6117, auc: 0.6510\n",
      "0.2-0.4-0.4 -> log_loss: 0.6534, precision: 0.6028, recall: 0.4303, f1: 0.5021, acc: 0.6117, auc: 0.6520\n",
      "0.2-0.5-0.3 -> log_loss: 0.6533, precision: 0.6086, recall: 0.4297, f1: 0.5038, acc: 0.6147, auc: 0.6529\n",
      "0.2-0.6-0.2 -> log_loss: 0.6533, precision: 0.6119, recall: 0.4250, f1: 0.5016, acc: 0.6156, auc: 0.6536\n",
      "0.2-0.7-0.1 -> log_loss: 0.6534, precision: 0.6133, recall: 0.4205, f1: 0.4989, acc: 0.6155, auc: 0.6541\n",
      " 0.2-0.8-0 -> log_loss: 0.6536, precision: 0.6140, recall: 0.4155, f1: 0.4956, acc: 0.6151, auc: 0.6544\n",
      " 0.3-0-0.7 -> log_loss: 0.6536, precision: 0.5981, recall: 0.4459, f1: 0.5109, acc: 0.6114, auc: 0.6488\n",
      "0.3-0.1-0.6 -> log_loss: 0.6532, precision: 0.5992, recall: 0.4412, f1: 0.5082, acc: 0.6113, auc: 0.6500\n",
      "0.3-0.2-0.5 -> log_loss: 0.6530, precision: 0.5996, recall: 0.4388, f1: 0.5067, acc: 0.6112, auc: 0.6511\n",
      "0.3-0.3-0.4 -> log_loss: 0.6528, precision: 0.6018, recall: 0.4350, f1: 0.5050, acc: 0.6118, auc: 0.6520\n",
      "0.3-0.4-0.3 -> log_loss: 0.6527, precision: 0.6065, recall: 0.4324, f1: 0.5048, acc: 0.6140, auc: 0.6528\n",
      "0.3-0.5-0.2 -> log_loss: 0.6527, precision: 0.6092, recall: 0.4283, f1: 0.5030, acc: 0.6148, auc: 0.6535\n",
      "0.3-0.6-0.1 -> log_loss: 0.6527, precision: 0.6117, recall: 0.4235, f1: 0.5005, acc: 0.6152, auc: 0.6539\n",
      " 0.3-0.7-0 -> log_loss: 0.6529, precision: 0.6138, recall: 0.4203, f1: 0.4990, acc: 0.6158, auc: 0.6541\n",
      " 0.4-0-0.6 -> log_loss: 0.6529, precision: 0.5965, recall: 0.4451, f1: 0.5098, acc: 0.6104, auc: 0.6502\n",
      "0.4-0.1-0.5 -> log_loss: 0.6526, precision: 0.5987, recall: 0.4425, f1: 0.5089, acc: 0.6113, auc: 0.6511\n",
      "0.4-0.2-0.4 -> log_loss: 0.6524, precision: 0.6025, recall: 0.4400, f1: 0.5086, acc: 0.6130, auc: 0.6520\n",
      "0.4-0.3-0.3 -> log_loss: 0.6522, precision: 0.6045, recall: 0.4357, f1: 0.5064, acc: 0.6134, auc: 0.6527\n",
      "0.4-0.4-0.2 -> log_loss: 0.6522, precision: 0.6078, recall: 0.4320, f1: 0.5050, acc: 0.6146, auc: 0.6533\n",
      "0.4-0.5-0.1 -> log_loss: 0.6522, precision: 0.6106, recall: 0.4270, f1: 0.5025, acc: 0.6152, auc: 0.6537\n",
      " 0.4-0.6-0 -> log_loss: 0.6523, precision: 0.6131, recall: 0.4229, f1: 0.5006, acc: 0.6159, auc: 0.6539\n",
      " 0.5-0-0.5 -> log_loss: 0.6524, precision: 0.5975, recall: 0.4466, f1: 0.5112, acc: 0.6112, auc: 0.6511\n",
      "0.5-0.1-0.4 -> log_loss: 0.6521, precision: 0.6006, recall: 0.4440, f1: 0.5106, acc: 0.6125, auc: 0.6519\n",
      "0.5-0.2-0.3 -> log_loss: 0.6519, precision: 0.6026, recall: 0.4397, f1: 0.5084, acc: 0.6130, auc: 0.6525\n",
      "0.5-0.3-0.2 -> log_loss: 0.6518, precision: 0.6064, recall: 0.4353, f1: 0.5068, acc: 0.6144, auc: 0.6531\n",
      "0.5-0.4-0.1 -> log_loss: 0.6518, precision: 0.6085, recall: 0.4301, f1: 0.5040, acc: 0.6147, auc: 0.6534\n",
      " 0.5-0.5-0 -> log_loss: 0.6519, precision: 0.6122, recall: 0.4264, f1: 0.5027, acc: 0.6160, auc: 0.6536\n",
      " 0.6-0-0.4 -> log_loss: 0.6521, precision: 0.5995, recall: 0.4485, f1: 0.5131, acc: 0.6126, auc: 0.6518\n",
      "0.6-0.1-0.3 -> log_loss: 0.6519, precision: 0.6008, recall: 0.4435, f1: 0.5103, acc: 0.6126, auc: 0.6524\n",
      "0.6-0.2-0.2 -> log_loss: 0.6517, precision: 0.6036, recall: 0.4388, f1: 0.5082, acc: 0.6134, auc: 0.6529\n",
      "0.6-0.3-0.1 -> log_loss: 0.6517, precision: 0.6075, recall: 0.4348, f1: 0.5069, acc: 0.6149, auc: 0.6532\n",
      " 0.6-0.4-0 -> log_loss: 0.6517, precision: 0.6101, recall: 0.4290, f1: 0.5038, acc: 0.6153, auc: 0.6534\n",
      " 0.7-0-0.3 -> log_loss: 0.6520, precision: 0.5983, recall: 0.4468, f1: 0.5116, acc: 0.6117, auc: 0.6523\n",
      "0.7-0.1-0.2 -> log_loss: 0.6518, precision: 0.6025, recall: 0.4437, f1: 0.5110, acc: 0.6136, auc: 0.6527\n",
      "0.7-0.2-0.1 -> log_loss: 0.6517, precision: 0.6065, recall: 0.4391, f1: 0.5094, acc: 0.6151, auc: 0.6529\n",
      " 0.7-0.3-0 -> log_loss: 0.6517, precision: 0.6064, recall: 0.4336, f1: 0.5056, acc: 0.6141, auc: 0.6531\n",
      " 0.8-0-0.2 -> log_loss: 0.6522, precision: 0.6015, recall: 0.4475, f1: 0.5132, acc: 0.6136, auc: 0.6524\n",
      "0.8-0.1-0.1 -> log_loss: 0.6520, precision: 0.6048, recall: 0.4440, f1: 0.5121, acc: 0.6149, auc: 0.6527\n",
      " 0.8-0.2-0 -> log_loss: 0.6520, precision: 0.6037, recall: 0.4405, f1: 0.5094, acc: 0.6137, auc: 0.6528\n",
      " 0.9-0-0.1 -> log_loss: 0.6528, precision: 0.6020, recall: 0.4520, f1: 0.5164, acc: 0.6146, auc: 0.6524\n",
      " 0.9-0.1-0 -> log_loss: 0.6526, precision: 0.6002, recall: 0.4531, f1: 0.5164, acc: 0.6137, auc: 0.6526\n",
      "     1-0-0 -> log_loss: 0.6631, precision: 0.5983, recall: 0.4614, f1: 0.5210, acc: 0.6139, auc: 0.6522\n"
     ]
    }
   ],
   "source": [
    "# Test different combination for layer 3\n",
    "model_percent = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "for m1_per in model_percent:\n",
    "    if m1_per < 1:\n",
    "        for m2_per in model_percent:\n",
    "            if (m1_per + m2_per) < 1:\n",
    "                m3_per = 1 - (m1_per + m2_per)\n",
    "                # Merge prediction in one column and check roc_auc\n",
    "                merge_probs = X_train_layer3['ExraTree']*m1_per + X_train_layer3['MLP']*m2_per + X_train_layer3['LR']*m3_per\n",
    "                merge_preds = np.where(merge_probs > 0.5, 1, 0)\n",
    "                logloss = log_loss(y_train_df, merge_probs)\n",
    "                precision = precision_score(y_train_df, merge_preds)\n",
    "                recall = recall_score(y_train_df, merge_preds)\n",
    "                f1 = f1_score(y_train_df, merge_preds)\n",
    "                acc = accuracy_score(y_train_df, merge_preds)\n",
    "                auc = roc_auc_score(y_train_df, merge_probs)\n",
    "                print(\"%10s -> log_loss: %.4f, precision: %.4f, recall: %.4f, f1: %.4f, acc: %.4f, auc: %.4f\" % (str(m1_per)+'-'+str(m2_per)+'-'+str(m3_per), logloss, precision, recall, f1, acc, auc))\n",
    "            # M3 not taken into account\n",
    "            elif (m1_per + m2_per) == 1:\n",
    "                # Merge prediction in one column and check roc_auc\n",
    "                merge_probs = X_train_layer3['ExraTree']*m1_per + X_train_layer3['MLP']*m2_per\n",
    "                merge_preds = np.where(merge_probs > 0.5, 1, 0)\n",
    "                logloss = log_loss(y_train_df, merge_probs)\n",
    "                precision = precision_score(y_train_df, merge_preds)\n",
    "                recall = recall_score(y_train_df, merge_preds)\n",
    "                f1 = f1_score(y_train_df, merge_preds)\n",
    "                acc = accuracy_score(y_train_df, merge_preds)\n",
    "                auc = roc_auc_score(y_train_df, merge_probs)\n",
    "                print(\"%10s -> log_loss: %.4f, precision: %.4f, recall: %.4f, f1: %.4f, acc: %.4f, auc: %.4f\" % (str(m1_per)+'-'+str(m2_per)+'-0', logloss, precision, recall, f1, acc, auc))\n",
    "    # Only M1\n",
    "    else:\n",
    "        # Merge prediction in one column and check roc_auc\n",
    "        merge_probs = X_train_layer3['ExraTree']*m1_per\n",
    "        merge_preds = np.where(merge_probs > 0.5, 1, 0)\n",
    "        logloss = log_loss(y_train_df, merge_probs)\n",
    "        precision = precision_score(y_train_df, merge_preds)\n",
    "        recall = recall_score(y_train_df, merge_preds)\n",
    "        f1 = f1_score(y_train_df, merge_preds)\n",
    "        acc = accuracy_score(y_train_df, merge_preds)\n",
    "        auc = roc_auc_score(y_train_df, merge_probs)\n",
    "        print(\"%10s -> log_loss: %.4f, precision: %.4f, recall: %.4f, f1: %.4f, acc: %.4f, auc: %.4f\" % (str(m1_per)+'-0-0', logloss, precision, recall, f1, acc, auc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0-0-1 -> log_loss: 0.6586, precision: 0.5866, recall: 0.4415, f1: 0.5038, acc: 0.6042, auc: 0.6364\n",
      " 0-0.1-0.9 -> log_loss: 0.6582, precision: 0.5895, recall: 0.4403, f1: 0.5041, acc: 0.6057, auc: 0.6375\n",
      " 0-0.2-0.8 -> log_loss: 0.6579, precision: 0.5902, recall: 0.4370, f1: 0.5022, acc: 0.6057, auc: 0.6386\n",
      " 0-0.3-0.7 -> log_loss: 0.6577, precision: 0.5887, recall: 0.4314, f1: 0.4979, acc: 0.6040, auc: 0.6395\n",
      " 0-0.4-0.6 -> log_loss: 0.6576, precision: 0.5952, recall: 0.4305, f1: 0.4996, acc: 0.6075, auc: 0.6404\n",
      " 0-0.5-0.5 -> log_loss: 0.6575, precision: 0.5999, recall: 0.4293, f1: 0.5005, acc: 0.6099, auc: 0.6411\n",
      " 0-0.6-0.4 -> log_loss: 0.6575, precision: 0.6032, recall: 0.4261, f1: 0.4994, acc: 0.6112, auc: 0.6419\n",
      " 0-0.7-0.3 -> log_loss: 0.6576, precision: 0.6063, recall: 0.4240, f1: 0.4990, acc: 0.6125, auc: 0.6425\n",
      " 0-0.8-0.2 -> log_loss: 0.6578, precision: 0.6082, recall: 0.4200, f1: 0.4969, acc: 0.6129, auc: 0.6428\n",
      " 0-0.9-0.1 -> log_loss: 0.6581, precision: 0.6053, recall: 0.4143, f1: 0.4919, acc: 0.6105, auc: 0.6430\n",
      "     0-1-0 -> log_loss: 0.6584, precision: 0.6037, recall: 0.4090, f1: 0.4877, acc: 0.6088, auc: 0.6430\n",
      " 0.1-0-0.9 -> log_loss: 0.6578, precision: 0.5883, recall: 0.4439, f1: 0.5060, acc: 0.6055, auc: 0.6378\n",
      "0.1-0.1-0.8 -> log_loss: 0.6575, precision: 0.5892, recall: 0.4399, f1: 0.5037, acc: 0.6055, auc: 0.6388\n",
      "0.1-0.2-0.7 -> log_loss: 0.6573, precision: 0.5921, recall: 0.4362, f1: 0.5023, acc: 0.6066, auc: 0.6397\n",
      "0.1-0.3-0.6 -> log_loss: 0.6571, precision: 0.5973, recall: 0.4374, f1: 0.5050, acc: 0.6097, auc: 0.6405\n",
      "0.1-0.4-0.5 -> log_loss: 0.6570, precision: 0.6017, recall: 0.4326, f1: 0.5033, acc: 0.6114, auc: 0.6413\n",
      "0.1-0.5-0.4 -> log_loss: 0.6570, precision: 0.6018, recall: 0.4273, f1: 0.4998, acc: 0.6106, auc: 0.6419\n",
      "0.1-0.6-0.3 -> log_loss: 0.6571, precision: 0.6037, recall: 0.4220, f1: 0.4968, acc: 0.6108, auc: 0.6424\n",
      "0.1-0.7-0.2 -> log_loss: 0.6573, precision: 0.6049, recall: 0.4192, f1: 0.4952, acc: 0.6110, auc: 0.6428\n",
      "0.1-0.8-0.1 -> log_loss: 0.6575, precision: 0.6033, recall: 0.4139, f1: 0.4910, acc: 0.6094, auc: 0.6430\n",
      " 0.1-0.9-0 -> log_loss: 0.6578, precision: 0.6007, recall: 0.4106, f1: 0.4878, acc: 0.6075, auc: 0.6429\n",
      " 0.2-0-0.8 -> log_loss: 0.6572, precision: 0.5916, recall: 0.4431, f1: 0.5067, acc: 0.6073, auc: 0.6389\n",
      "0.2-0.1-0.7 -> log_loss: 0.6569, precision: 0.5932, recall: 0.4383, f1: 0.5041, acc: 0.6075, auc: 0.6397\n",
      "0.2-0.2-0.6 -> log_loss: 0.6567, precision: 0.5981, recall: 0.4358, f1: 0.5042, acc: 0.6099, auc: 0.6405\n",
      "0.2-0.3-0.5 -> log_loss: 0.6567, precision: 0.6025, recall: 0.4334, f1: 0.5041, acc: 0.6119, auc: 0.6412\n",
      "0.2-0.4-0.4 -> log_loss: 0.6566, precision: 0.6016, recall: 0.4257, f1: 0.4986, acc: 0.6103, auc: 0.6417\n",
      "0.2-0.5-0.3 -> log_loss: 0.6567, precision: 0.6000, recall: 0.4228, f1: 0.4961, acc: 0.6090, auc: 0.6422\n",
      "0.2-0.6-0.2 -> log_loss: 0.6568, precision: 0.6020, recall: 0.4184, f1: 0.4936, acc: 0.6094, auc: 0.6425\n",
      "0.2-0.7-0.1 -> log_loss: 0.6570, precision: 0.6040, recall: 0.4188, f1: 0.4946, acc: 0.6105, auc: 0.6426\n",
      " 0.2-0.8-0 -> log_loss: 0.6573, precision: 0.6006, recall: 0.4135, f1: 0.4898, acc: 0.6079, auc: 0.6426\n",
      " 0.3-0-0.7 -> log_loss: 0.6567, precision: 0.5942, recall: 0.4407, f1: 0.5061, acc: 0.6084, auc: 0.6395\n",
      "0.3-0.1-0.6 -> log_loss: 0.6565, precision: 0.5974, recall: 0.4358, f1: 0.5040, acc: 0.6095, auc: 0.6403\n",
      "0.3-0.2-0.5 -> log_loss: 0.6564, precision: 0.6007, recall: 0.4350, f1: 0.5046, acc: 0.6112, auc: 0.6409\n",
      "0.3-0.3-0.4 -> log_loss: 0.6564, precision: 0.6023, recall: 0.4305, f1: 0.5021, acc: 0.6114, auc: 0.6414\n",
      "0.3-0.4-0.3 -> log_loss: 0.6564, precision: 0.5994, recall: 0.4261, f1: 0.4981, acc: 0.6092, auc: 0.6418\n",
      "0.3-0.5-0.2 -> log_loss: 0.6565, precision: 0.6021, recall: 0.4240, f1: 0.4976, acc: 0.6103, auc: 0.6421\n",
      "0.3-0.6-0.1 -> log_loss: 0.6567, precision: 0.6000, recall: 0.4204, f1: 0.4944, acc: 0.6086, auc: 0.6421\n",
      " 0.3-0.7-0 -> log_loss: 0.6570, precision: 0.5992, recall: 0.4184, f1: 0.4927, acc: 0.6079, auc: 0.6420\n",
      " 0.4-0-0.6 -> log_loss: 0.6564, precision: 0.5977, recall: 0.4399, f1: 0.5068, acc: 0.6103, auc: 0.6400\n",
      "0.4-0.1-0.5 -> log_loss: 0.6563, precision: 0.5999, recall: 0.4366, f1: 0.5054, acc: 0.6110, auc: 0.6405\n",
      "0.4-0.2-0.4 -> log_loss: 0.6562, precision: 0.5982, recall: 0.4318, f1: 0.5015, acc: 0.6094, auc: 0.6410\n",
      "0.4-0.3-0.3 -> log_loss: 0.6562, precision: 0.5983, recall: 0.4289, f1: 0.4996, acc: 0.6090, auc: 0.6414\n",
      "0.4-0.4-0.2 -> log_loss: 0.6563, precision: 0.6003, recall: 0.4277, f1: 0.4995, acc: 0.6099, auc: 0.6416\n",
      "0.4-0.5-0.1 -> log_loss: 0.6565, precision: 0.5967, recall: 0.4224, f1: 0.4946, acc: 0.6071, auc: 0.6415\n",
      " 0.4-0.6-0 -> log_loss: 0.6568, precision: 0.5974, recall: 0.4224, f1: 0.4949, acc: 0.6075, auc: 0.6414\n",
      " 0.5-0-0.5 -> log_loss: 0.6563, precision: 0.5953, recall: 0.4415, f1: 0.5070, acc: 0.6092, auc: 0.6401\n",
      "0.5-0.1-0.4 -> log_loss: 0.6562, precision: 0.5967, recall: 0.4362, f1: 0.5040, acc: 0.6092, auc: 0.6405\n",
      "0.5-0.2-0.3 -> log_loss: 0.6562, precision: 0.5971, recall: 0.4346, f1: 0.5031, acc: 0.6092, auc: 0.6408\n",
      "0.5-0.3-0.2 -> log_loss: 0.6563, precision: 0.5995, recall: 0.4318, f1: 0.5020, acc: 0.6101, auc: 0.6409\n",
      "0.5-0.4-0.1 -> log_loss: 0.6564, precision: 0.5976, recall: 0.4289, f1: 0.4994, acc: 0.6086, auc: 0.6408\n",
      " 0.5-0.5-0 -> log_loss: 0.6567, precision: 0.5949, recall: 0.4265, f1: 0.4968, acc: 0.6068, auc: 0.6406\n",
      " 0.6-0-0.4 -> log_loss: 0.6563, precision: 0.5926, recall: 0.4419, f1: 0.5063, acc: 0.6077, auc: 0.6399\n",
      "0.6-0.1-0.3 -> log_loss: 0.6563, precision: 0.5932, recall: 0.4383, f1: 0.5041, acc: 0.6075, auc: 0.6401\n",
      "0.6-0.2-0.2 -> log_loss: 0.6563, precision: 0.5949, recall: 0.4354, f1: 0.5028, acc: 0.6081, auc: 0.6401\n",
      "0.6-0.3-0.1 -> log_loss: 0.6565, precision: 0.5953, recall: 0.4338, f1: 0.5019, acc: 0.6081, auc: 0.6400\n",
      " 0.6-0.4-0 -> log_loss: 0.6567, precision: 0.5954, recall: 0.4310, f1: 0.5000, acc: 0.6077, auc: 0.6397\n",
      " 0.7-0-0.3 -> log_loss: 0.6565, precision: 0.5950, recall: 0.4452, f1: 0.5093, acc: 0.6095, auc: 0.6393\n",
      "0.7-0.1-0.2 -> log_loss: 0.6565, precision: 0.5918, recall: 0.4399, f1: 0.5047, acc: 0.6070, auc: 0.6393\n",
      "0.7-0.2-0.1 -> log_loss: 0.6567, precision: 0.5910, recall: 0.4354, f1: 0.5014, acc: 0.6058, auc: 0.6391\n",
      " 0.7-0.3-0 -> log_loss: 0.6569, precision: 0.5910, recall: 0.4326, f1: 0.4995, acc: 0.6055, auc: 0.6389\n",
      " 0.8-0-0.2 -> log_loss: 0.6569, precision: 0.5903, recall: 0.4435, f1: 0.5065, acc: 0.6066, auc: 0.6384\n",
      "0.8-0.1-0.1 -> log_loss: 0.6570, precision: 0.5899, recall: 0.4399, f1: 0.5040, acc: 0.6058, auc: 0.6382\n",
      " 0.8-0.2-0 -> log_loss: 0.6572, precision: 0.5873, recall: 0.4358, f1: 0.5003, acc: 0.6038, auc: 0.6380\n",
      " 0.9-0-0.1 -> log_loss: 0.6575, precision: 0.5870, recall: 0.4427, f1: 0.5047, acc: 0.6045, auc: 0.6373\n",
      " 0.9-0.1-0 -> log_loss: 0.6576, precision: 0.5885, recall: 0.4415, f1: 0.5045, acc: 0.6053, auc: 0.6370\n",
      "     1-0-0 -> log_loss: 0.6582, precision: 0.5877, recall: 0.4452, f1: 0.5066, acc: 0.6053, auc: 0.6361\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score, roc_auc_score\n",
    "# Test different combination for layer 3\n",
    "model_percent = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "for m1_per in model_percent:\n",
    "    if m1_per < 1:\n",
    "        for m2_per in model_percent:\n",
    "            if (m1_per + m2_per) < 1:\n",
    "                m3_per = 1 - (m1_per + m2_per)\n",
    "                # Merge prediction in one column and check roc_auc\n",
    "                merge_probs = X_test_layer3['ExraTree']*m1_per + X_test_layer3['MLP']*m2_per + X_test_layer3['LR']*m3_per\n",
    "                merge_preds = np.where(merge_probs > 0.5, 1, 0)\n",
    "                logloss = log_loss(y_test_df, merge_probs)\n",
    "                precision = precision_score(y_test_df, merge_preds)\n",
    "                recall = recall_score(y_test_df, merge_preds)\n",
    "                f1 = f1_score(y_test_df, merge_preds)\n",
    "                acc = accuracy_score(y_test_df, merge_preds)\n",
    "                auc = roc_auc_score(y_test_df, merge_probs)\n",
    "                print(\"%10s -> log_loss: %.4f, precision: %.4f, recall: %.4f, f1: %.4f, acc: %.4f, auc: %.4f\" % (str(m1_per)+'-'+str(m2_per)+'-'+str(m3_per), logloss, precision, recall, f1, acc, auc))\n",
    "            # M3 not taken into account\n",
    "            elif (m1_per + m2_per) == 1:\n",
    "                # Merge prediction in one column and check roc_auc\n",
    "                merge_probs = X_test_layer3['ExraTree']*m1_per + X_test_layer3['MLP']*m2_per\n",
    "                merge_preds = np.where(merge_probs > 0.5, 1, 0)\n",
    "                logloss = log_loss(y_test_df, merge_probs)\n",
    "                precision = precision_score(y_test_df, merge_preds)\n",
    "                recall = recall_score(y_test_df, merge_preds)\n",
    "                f1 = f1_score(y_test_df, merge_preds)\n",
    "                acc = accuracy_score(y_test_df, merge_preds)\n",
    "                auc = roc_auc_score(y_test_df, merge_probs)\n",
    "                print(\"%10s -> log_loss: %.4f, precision: %.4f, recall: %.4f, f1: %.4f, acc: %.4f, auc: %.4f\" % (str(m1_per)+'-'+str(m2_per)+'-0', logloss, precision, recall, f1, acc, auc))\n",
    "    # Only M1\n",
    "    else:\n",
    "        # Merge prediction in one column and check roc_auc\n",
    "        merge_probs = X_test_layer3['ExraTree']*m1_per\n",
    "        merge_preds = np.where(merge_probs > 0.5, 1, 0)\n",
    "        logloss = log_loss(y_test_df, merge_probs)\n",
    "        precision = precision_score(y_test_df, merge_preds)\n",
    "        recall = recall_score(y_test_df, merge_preds)\n",
    "        f1 = f1_score(y_test_df, merge_preds)\n",
    "        acc = accuracy_score(y_test_df, merge_preds)\n",
    "        auc = roc_auc_score(y_test_df, merge_probs)\n",
    "        print(\"%10s -> log_loss: %.4f, precision: %.4f, recall: %.4f, f1: %.4f, acc: %.4f, auc: %.4f\" % (str(m1_per)+'-0-0', logloss, precision, recall, f1, acc, auc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.3-0.3-0.4 -> log_loss: 0.6555, precision: 0.6051, recall: 0.4431, f1: 0.5116, acc: 0.6149, auc: 0.6008\n",
    "# 0.4-0.6-0 -> log_loss: 0.6523, precision: 0.6131, recall: 0.4229, f1: 0.5006, acc: 0.6159, auc: 0.6539\n",
    "\n",
    "y_probs = X_test_layer3['NB']*0.3 + X_test_layer3['MLP']*0.3 + X_test_layer3['LR']*0.4\n",
    "y_preds = np.where(merge_probs > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ET, MLP, LR\n",
    "#0.2-0.3-0.5 -> log_loss: 0.6539, precision: 0.6044, recall: 0.4369, f1: 0.5072, acc: 0.6136, auc: 0.6512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test_df, y_probs.ravel())\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('ROC')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.4f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2EAAAN3CAYAAAC7g1K5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niTAAAICQCAMAAAiJMAAAgJAIAwAACP0BKnAQnpc3lHwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f34ad90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute and display the correlation matrix of predictions\n",
    "import seaborn as sns # visualization library based on matplotlib\n",
    "f, ax = plt.subplots(figsize=(16,16))\n",
    "sns.heatmap(X_test_layer3.corr(), square=True, annot=True)\n",
    "plt.title(\"Prediction correlation matrix\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df.to_csv('./TEST_LAYER3_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All 5\n",
    "0.653702983787\n",
    "0.515867766543\n",
    "0.606613097232\n",
    "0.456896627079"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
