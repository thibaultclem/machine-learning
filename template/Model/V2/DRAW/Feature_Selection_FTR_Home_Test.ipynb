{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection FTR HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the library\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "import matplotlib.pyplot as plt # plotting library\n",
    "import seaborn as sns # visualization library based on matplotlib\n",
    "from IPython.display import display # Manage multiple output per cell\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features = [\"A_MEANS_FIVE_AC\",\"A_MEANS_FIVE_AF\",\"A_MEANS_FIVE_AR\",\"A_MEANS_FIVE_AS\",\"A_MEANS_FIVE_AST\",\"A_MEANS_FIVE_AY\",\"A_MEANS_FIVE_FTAG\",\"A_MEANS_FIVE_FTHG\",\"A_MEANS_FIVE_FTR_A\",\"A_MEANS_FIVE_FTR_D\",\"A_MEANS_FIVE_FTR_H\",\"A_MEANS_FIVE_HC\",\"A_MEANS_FIVE_HF\",\"A_MEANS_FIVE_HR\",\"A_MEANS_FIVE_HS\",\"A_MEANS_FIVE_HST\",\"A_MEANS_FIVE_HTAG\",\"A_MEANS_FIVE_HTHG\",\"A_MEANS_FIVE_HTR_A\",\"A_MEANS_FIVE_HTR_D\",\"A_MEANS_FIVE_HTR_H\",\"A_MEANS_FIVE_HY\",\"H_MEANS_FIVE_AC\",\"H_MEANS_FIVE_AF\",\"H_MEANS_FIVE_AR\",\"H_MEANS_FIVE_AS\",\"H_MEANS_FIVE_AST\",\"H_MEANS_FIVE_AY\",\"H_MEANS_FIVE_FTAG\",\"H_MEANS_FIVE_FTHG\",\"H_MEANS_FIVE_FTR_A\",\"H_MEANS_FIVE_FTR_D\",\"H_MEANS_FIVE_FTR_H\",\"H_MEANS_FIVE_HC\",\"H_MEANS_FIVE_HF\",\"H_MEANS_FIVE_HR\",\"H_MEANS_FIVE_HS\",\"H_MEANS_FIVE_HST\",\"H_MEANS_FIVE_HTAG\",\"H_MEANS_FIVE_HTHG\",\"H_MEANS_FIVE_HTR_A\",\"H_MEANS_FIVE_HTR_D\",\"H_MEANS_FIVE_HTR_H\",\"H_MEANS_FIVE_HY\",\"A_MEANS_THREE_AC\",\"A_MEANS_THREE_AF\",\"A_MEANS_THREE_AR\",\"A_MEANS_THREE_AS\",\"A_MEANS_THREE_AST\",\"A_MEANS_THREE_AY\",\"A_MEANS_THREE_FTAG\",\"A_MEANS_THREE_FTHG\",\"A_MEANS_THREE_FTR_A\",\"A_MEANS_THREE_FTR_D\",\"A_MEANS_THREE_FTR_H\",\"A_MEANS_THREE_HC\",\"A_MEANS_THREE_HF\",\"A_MEANS_THREE_HR\",\"A_MEANS_THREE_HS\",\"A_MEANS_THREE_HST\",\"A_MEANS_THREE_HTAG\",\"A_MEANS_THREE_HTHG\",\"A_MEANS_THREE_HTR_A\",\"A_MEANS_THREE_HTR_D\",\"A_MEANS_THREE_HTR_H\",\"A_MEANS_THREE_HY\",\"H_MEANS_THREE_AC\",\"H_MEANS_THREE_AF\",\"H_MEANS_THREE_AR\",\"H_MEANS_THREE_AS\",\"H_MEANS_THREE_AST\",\"H_MEANS_THREE_AY\",\"H_MEANS_THREE_FTAG\",\"H_MEANS_THREE_FTHG\",\"H_MEANS_THREE_FTR_A\",\"H_MEANS_THREE_FTR_D\",\"H_MEANS_THREE_FTR_H\",\"H_MEANS_THREE_HC\",\"H_MEANS_THREE_HF\",\"H_MEANS_THREE_HR\",\"H_MEANS_THREE_HS\",\"H_MEANS_THREE_HST\",\"H_MEANS_THREE_HTAG\",\"H_MEANS_THREE_HTHG\",\"H_MEANS_THREE_HTR_A\",\"H_MEANS_THREE_HTR_D\",\"H_MEANS_THREE_HTR_H\",\"H_MEANS_THREE_HY\",\"A_STD_FIVE_AC\",\"A_STD_FIVE_AF\",\"A_STD_FIVE_AR\",\"A_STD_FIVE_AS\",\"A_STD_FIVE_AST\",\"A_STD_FIVE_AY\",\"A_STD_FIVE_FTAG\",\"A_STD_FIVE_FTHG\",\"A_STD_FIVE_FTR_A\",\"A_STD_FIVE_FTR_D\",\"A_STD_FIVE_FTR_H\",\"A_STD_FIVE_HC\",\"A_STD_FIVE_HF\",\"A_STD_FIVE_HR\",\"A_STD_FIVE_HS\",\"A_STD_FIVE_HST\",\"A_STD_FIVE_HTAG\",\"A_STD_FIVE_HTHG\",\"A_STD_FIVE_HTR_A\",\"A_STD_FIVE_HTR_D\",\"A_STD_FIVE_HTR_H\",\"A_STD_FIVE_HY\",\"H_STD_FIVE_AC\",\"H_STD_FIVE_AF\",\"H_STD_FIVE_AR\",\"H_STD_FIVE_AS\",\"H_STD_FIVE_AST\",\"H_STD_FIVE_AY\",\"H_STD_FIVE_FTAG\",\"H_STD_FIVE_FTHG\",\"H_STD_FIVE_FTR_A\",\"H_STD_FIVE_FTR_D\",\"H_STD_FIVE_FTR_H\",\"H_STD_FIVE_HC\",\"H_STD_FIVE_HF\",\"H_STD_FIVE_HR\",\"H_STD_FIVE_HS\",\"H_STD_FIVE_HST\",\"H_STD_FIVE_HTAG\",\"H_STD_FIVE_HTHG\",\"H_STD_FIVE_HTR_A\",\"H_STD_FIVE_HTR_D\",\"H_STD_FIVE_HTR_H\",\"H_STD_FIVE_HY\",\"A_STD_THREE_AC\",\"A_STD_THREE_AF\",\"A_STD_THREE_AR\",\"A_STD_THREE_AS\",\"A_STD_THREE_AST\",\"A_STD_THREE_AY\",\"A_STD_THREE_FTAG\",\"A_STD_THREE_FTHG\",\"A_STD_THREE_FTR_A\",\"A_STD_THREE_FTR_D\",\"A_STD_THREE_FTR_H\",\"A_STD_THREE_HC\",\"A_STD_THREE_HF\",\"A_STD_THREE_HR\",\"A_STD_THREE_HS\",\"A_STD_THREE_HST\",\"A_STD_THREE_HTAG\",\"A_STD_THREE_HTHG\",\"A_STD_THREE_HTR_A\",\"A_STD_THREE_HTR_D\",\"A_STD_THREE_HTR_H\",\"A_STD_THREE_HY\",\"H_STD_THREE_AC\",\"H_STD_THREE_AF\",\"H_STD_THREE_AR\",\"H_STD_THREE_AS\",\"H_STD_THREE_AST\",\"H_STD_THREE_AY\",\"H_STD_THREE_FTAG\",\"H_STD_THREE_FTHG\",\"H_STD_THREE_FTR_A\",\"H_STD_THREE_FTR_D\",\"H_STD_THREE_FTR_H\",\"H_STD_THREE_HC\",\"H_STD_THREE_HF\",\"H_STD_THREE_HR\",\"H_STD_THREE_HS\",\"H_STD_THREE_HST\",\"H_STD_THREE_HTAG\",\"H_STD_THREE_HTHG\",\"H_STD_THREE_HTR_A\",\"H_STD_THREE_HTR_D\",\"H_STD_THREE_HTR_H\",\"H_STD_THREE_HY\",\"INFO_Div\"]\n",
    "target = 'INFO_FTR_H'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DB Sqlite connection\n",
    "import sqlite3\n",
    "db = \"/Users/thibaultclement/Project/ligue1-predict/src/notebook/data/db/soccer_predict.sqlite\"\n",
    "conn = sqlite3.connect(db)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25275, 190)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all prematch data\n",
    "df = pd.read_sql_query(\"SELECT * FROM pre_matchs ORDER BY INFO_Date ASC;\", conn)\n",
    "df = (df[df.columns.drop(['index'])])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18027, 190)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all game between June (include) and October (include)\n",
    "df['INFO_Date'] = pd.to_datetime(df['INFO_Date'])\n",
    "df['INFO_Date'].dt.month\n",
    "df = df[(df['INFO_Date'].dt.month < 6) | (df['INFO_Date'].dt.month > 10)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "date_start_current_season = datetime.date(2016, 8, 1)\n",
    "df_current_season = df[(df['INFO_Date'] > date_start_current_season)]\n",
    "df = df[(df['INFO_Date'] < date_start_current_season)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "features_list = all_features\n",
    "X = pd.get_dummies(df[features_list])\n",
    "y = pd.get_dummies(df)[target].astype('bool_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impute of missing values (NaN) with the mean\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp = imp.fit(X)\n",
    "X = imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Use tuned XGBoost to get the feature importance\n",
    "from numpy import sort\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(\n",
    "    learning_rate =0.01,\n",
    "    n_estimators=572,\n",
    "    max_depth=3,\n",
    "    min_child_weight=5,\n",
    "    gamma=0,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=15\n",
    ")\n",
    "model.fit(X, y, eval_metric='logloss')\n",
    "thresholds = np.unique(sort(model.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "# Need only for no tree classifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#sc_X = StandardScaler().fit(X)\n",
    "#X = sc_X.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresh=0.00000, n=184, LogLoss mean: 0.63092, LogLoss std: 0.01\n",
      "Thresh=0.00026, n=151, LogLoss mean: 0.63333, LogLoss std: 0.01\n",
      "Thresh=0.00051, n=140, LogLoss mean: 0.63594, LogLoss std: 0.01\n",
      "Thresh=0.00077, n=127, LogLoss mean: 0.63776, LogLoss std: 0.01\n",
      "Thresh=0.00103, n=120, LogLoss mean: 0.63866, LogLoss std: 0.01\n",
      "Thresh=0.00128, n=115, LogLoss mean: 0.63877, LogLoss std: 0.01\n",
      "Thresh=0.00154, n=109, LogLoss mean: 0.63929, LogLoss std: 0.01\n",
      "Thresh=0.00180, n=106, LogLoss mean: 0.63987, LogLoss std: 0.01\n",
      "Thresh=0.00205, n=102, LogLoss mean: 0.64014, LogLoss std: 0.01\n",
      "Thresh=0.00231, n=97, LogLoss mean: 0.64049, LogLoss std: 0.01\n",
      "Thresh=0.00257, n=89, LogLoss mean: 0.64166, LogLoss std: 0.01\n",
      "Thresh=0.00282, n=85, LogLoss mean: 0.64183, LogLoss std: 0.01\n",
      "Thresh=0.00308, n=83, LogLoss mean: 0.64195, LogLoss std: 0.01\n",
      "Thresh=0.00334, n=79, LogLoss mean: 0.64227, LogLoss std: 0.01\n",
      "Thresh=0.00359, n=73, LogLoss mean: 0.64363, LogLoss std: 0.02\n",
      "Thresh=0.00385, n=69, LogLoss mean: 0.64403, LogLoss std: 0.02\n",
      "Thresh=0.00411, n=64, LogLoss mean: 0.64420, LogLoss std: 0.02\n",
      "Thresh=0.00436, n=61, LogLoss mean: 0.64453, LogLoss std: 0.02\n",
      "Thresh=0.00462, n=60, LogLoss mean: 0.64448, LogLoss std: 0.02\n",
      "Thresh=0.00488, n=58, LogLoss mean: 0.64453, LogLoss std: 0.02\n",
      "Thresh=0.00513, n=56, LogLoss mean: 0.64480, LogLoss std: 0.02\n",
      "Thresh=0.00539, n=53, LogLoss mean: 0.64587, LogLoss std: 0.02\n",
      "Thresh=0.00565, n=52, LogLoss mean: 0.64603, LogLoss std: 0.02\n",
      "Thresh=0.00591, n=49, LogLoss mean: 0.64611, LogLoss std: 0.02\n",
      "Thresh=0.00616, n=48, LogLoss mean: 0.64688, LogLoss std: 0.02\n",
      "Thresh=0.00642, n=46, LogLoss mean: 0.64669, LogLoss std: 0.01\n",
      "Thresh=0.00668, n=44, LogLoss mean: 0.64745, LogLoss std: 0.01\n",
      "Thresh=0.00693, n=43, LogLoss mean: 0.64747, LogLoss std: 0.01\n",
      "Thresh=0.00719, n=40, LogLoss mean: 0.64811, LogLoss std: 0.01\n",
      "Thresh=0.00745, n=38, LogLoss mean: 0.64884, LogLoss std: 0.01\n",
      "Thresh=0.00770, n=37, LogLoss mean: 0.64883, LogLoss std: 0.01\n",
      "Thresh=0.00796, n=36, LogLoss mean: 0.64895, LogLoss std: 0.01\n",
      "Thresh=0.00847, n=34, LogLoss mean: 0.64910, LogLoss std: 0.01\n",
      "Thresh=0.00873, n=33, LogLoss mean: 0.64914, LogLoss std: 0.01\n",
      "Thresh=0.00899, n=32, LogLoss mean: 0.64903, LogLoss std: 0.01\n",
      "Thresh=0.00924, n=30, LogLoss mean: 0.64939, LogLoss std: 0.01\n",
      "Thresh=0.00950, n=29, LogLoss mean: 0.64936, LogLoss std: 0.02\n",
      "Thresh=0.00976, n=28, LogLoss mean: 0.64962, LogLoss std: 0.02\n",
      "Thresh=0.01001, n=27, LogLoss mean: 0.64954, LogLoss std: 0.02\n",
      "Thresh=0.01027, n=26, LogLoss mean: 0.64925, LogLoss std: 0.02\n",
      "Thresh=0.01053, n=25, LogLoss mean: 0.64995, LogLoss std: 0.02\n",
      "Thresh=0.01130, n=24, LogLoss mean: 0.65025, LogLoss std: 0.02\n",
      "Thresh=0.01181, n=23, LogLoss mean: 0.65070, LogLoss std: 0.02\n",
      "Thresh=0.01207, n=22, LogLoss mean: 0.65000, LogLoss std: 0.02\n",
      "Thresh=0.01232, n=21, LogLoss mean: 0.64950, LogLoss std: 0.01\n",
      "Thresh=0.01258, n=20, LogLoss mean: 0.64925, LogLoss std: 0.02\n",
      "Thresh=0.01386, n=19, LogLoss mean: 0.64890, LogLoss std: 0.02\n",
      "Thresh=0.01540, n=18, LogLoss mean: 0.64894, LogLoss std: 0.02\n",
      "Thresh=0.01592, n=17, LogLoss mean: 0.64891, LogLoss std: 0.02\n",
      "Thresh=0.01617, n=16, LogLoss mean: 0.64972, LogLoss std: 0.02\n",
      "Thresh=0.01849, n=15, LogLoss mean: 0.64754, LogLoss std: 0.02\n",
      "Thresh=0.01926, n=14, LogLoss mean: 0.64669, LogLoss std: 0.02\n",
      "Thresh=0.02080, n=13, LogLoss mean: 0.64669, LogLoss std: 0.02\n",
      "Thresh=0.02208, n=12, LogLoss mean: 0.64485, LogLoss std: 0.02\n",
      "Thresh=0.02259, n=11, LogLoss mean: 0.64380, LogLoss std: 0.01\n",
      "Thresh=0.02413, n=10, LogLoss mean: 0.64266, LogLoss std: 0.02\n",
      "Thresh=0.02439, n=9, LogLoss mean: 0.64045, LogLoss std: 0.02\n",
      "Thresh=0.02721, n=8, LogLoss mean: 0.63363, LogLoss std: 0.02\n",
      "Thresh=0.02824, n=7, LogLoss mean: 0.63503, LogLoss std: 0.01\n",
      "Thresh=0.03338, n=6, LogLoss mean: 0.63187, LogLoss std: 0.02\n",
      "Thresh=0.03979, n=4, LogLoss mean: 0.62732, LogLoss std: 0.01\n",
      "Thresh=0.04159, n=3, LogLoss mean: 0.60515, LogLoss std: 0.01\n",
      "Thresh=0.04724, n=2, LogLoss mean: 0.57906, LogLoss std: 0.01\n",
      "Thresh=0.05392, n=1, LogLoss mean: 0.55170, LogLoss std: 0.02\n"
     ]
    }
   ],
   "source": [
    "# Fit model using each importance as a threshold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "############## CONF\n",
    "bet_on = 'H'\n",
    "start_date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "\n",
    "############## MODEL\n",
    "\n",
    "# NAIVES BAYES\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model_name = 'GaussianNB'\n",
    "selection_model = GaussianNB()\n",
    "# threshold=0.00924\n",
    "\n",
    "# SVM\n",
    "#from sklearn.svm import SVC\n",
    "#model_name = 'SVM'\n",
    "#selection_model = SVC(random_state=0,C=0.02,kernel='linear',gamma=1,probability=True)\n",
    "# threshold=0.00899\n",
    "\n",
    "# ElasticNEt\n",
    "#from sklearn.linear_model.stochastic_gradient import SGDClassifier\n",
    "#model_name = 'ElasticNEt'\n",
    "#selection_model = SGDClassifier(random_state=0,alpha=0.0024, l1_ratio=0.64, loss='log', penalty='elasticnet')\n",
    "# threshold=0.01053\n",
    "\n",
    "# KNeighbors\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#model_name = 'KNeighbors'\n",
    "#selection_model = KNeighborsClassifier(n_jobs=-1,n_neighbors=200, p=2, weights='uniform')\n",
    "# threshold=0.01207\n",
    "\n",
    "# MLP\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#model_name = 'MLP'\n",
    "#selection_model = MLPClassifier(random_state=0,activation='identity', alpha=3, hidden_layer_sizes=(10,), max_iter=200, solver= 'adam')\n",
    "# threshold=0.00924\n",
    "\n",
    "# LinearDiscriminantAnalysis\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "#model_name = 'LinearDiscriminantAnalysis'\n",
    "#selection_model = LinearDiscriminantAnalysis()\n",
    "# threshold=0.02413\n",
    "\n",
    "# LogisticRegression\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#model_name = 'LogisticRegression'\n",
    "#selection_model = LogisticRegression(random_state=0,n_jobs=-1,solver='liblinear',C=0.008,penalty='l1')\n",
    "# \n",
    "\n",
    "# ENSEMBLE TREES\n",
    "\n",
    "# AdaBoost\n",
    "#from sklearn.ensemble import AdaBoostClassifier\n",
    "#model_name = 'AdaBoost'\n",
    "#selection_model = AdaBoostClassifier(random_state=0,n_estimators=30)\n",
    "# threshold=0.00796\n",
    "\n",
    "# BaggingClassifier\n",
    "#from sklearn.ensemble import BaggingClassifier\n",
    "#model_name = 'BaggingClassifier'\n",
    "#selection_model = BaggingClassifier(random_state=0, n_jobs=-1,max_features=0.7, max_samples=1.0, n_estimators=100)\n",
    "# Thresh=0.00000\n",
    "\n",
    "# RandomForestClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#model_name = 'RandomForestClassifier'\n",
    "#selection_model = RandomForestClassifier(random_state=0, n_jobs=-1,criterion='entropy', max_depth=None, max_features='sqrt', min_samples_leaf=1, min_samples_split=2, n_estimators=360)\n",
    "# Thresh=0.00000\n",
    "\n",
    "# ExtraTreesClassifier\n",
    "#from sklearn.ensemble import ExtraTreesClassifier\n",
    "#model_name = 'ExtraTreesClassifier'\n",
    "#selection_model = ExtraTreesClassifier(random_state=0,n_jobs=-1,criterion='entropy',max_depth=30,max_features=None,min_samples_leaf=1,min_samples_split=16,n_estimators=350)\n",
    "# threshold=0.00513\n",
    "\n",
    "# BaggingClassifier\n",
    "#from sklearn.ensemble import BaggingClassifier\n",
    "#model_name = 'BaggingClassifierSmallMaxSample'\n",
    "#selection_model = BaggingClassifier(random_state=0, n_jobs=-1,max_features=0.5, max_samples=0.1, n_estimators=400)\n",
    "# threshold=0.01617 & threshold=0.00000\n",
    "\n",
    "# GradientBoostingClassifier\n",
    "#from sklearn.ensemble import GradientBoostingClassifier\n",
    "#model_name = 'GradientBoostingClassifier'\n",
    "#selection_model = GradientBoostingClassifier(random_state=0,criterion='friedman_mse',loss='deviance',max_depth=20,max_features='log2',min_samples_leaf=3,min_samples_split=10,n_estimators=150)\n",
    "# \n",
    "\n",
    "############### FEATURE SELECTION\n",
    "\n",
    "df_fs = pd.DataFrame(columns=('Thresh', 'n', 'll_mean', 'll_std'))\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(X_train)\n",
    "    # eval model\n",
    "    logloss = cross_val_score(selection_model, select_X_train, y_train, cv=8, scoring='roc_auc', n_jobs=-1)\n",
    "    ll_mean = logloss.mean()\n",
    "    ll_std = logloss.std()\n",
    "    df2 = pd.DataFrame([[thresh, select_X_train.shape[1], ll_mean, ll_std]], columns=('Thresh', 'n', 'll_mean', 'll_std'))\n",
    "    df_fs = df_fs.append(df2)\n",
    "    print(\"Thresh=%.5f, n=%d, LogLoss mean: %.5f, LogLoss std: %.2f\" % (thresh, select_X_train.shape[1], ll_mean, ll_std))\n",
    "df_fs.to_csv('./feature_selection/'+model_name+'-'+bet_on+'_'+start_date+'.csv')\n",
    "\n",
    "############### FEATURE SELECTION DEEP LEARNING\n",
    "\n",
    "#from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# model_name = 'KerasClassifierSGD'\n",
    "\n",
    "# df_fs = pd.DataFrame(columns=('Thresh', 'n', 'll_mean', 'll_std'))\n",
    "# for thresh in thresholds:\n",
    "#     # select features using threshold\n",
    "#     selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "#     select_X_train = selection.transform(X_train)\n",
    "#     # eval model\n",
    "#     def create_model(optimizer='adam'):\n",
    "#         # create model\n",
    "#         model = Sequential()\n",
    "#         model.add(Dense(select_X_train.shape[1], input_dim=select_X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "#         model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "#         # Compile model\n",
    "#         model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "#         return model\n",
    "#     # selection_model = KerasClassifier(build_fn=create_model,optimizer='Adam',batch_size=1500,epochs=3)\n",
    "#     selection_model = KerasClassifier(build_fn=create_model,optimizer='SGD',batch_size=80,epochs=5)\n",
    "#     logloss = cross_val_score(selection_model, select_X_train, y_train, cv=8, scoring=make_scorer(log_loss, greater_is_better=False), n_jobs=-1)\n",
    "#     ll_mean = logloss.mean()\n",
    "#     ll_std = logloss.std()\n",
    "#     df2 = pd.DataFrame([[thresh, select_X_train.shape[1], ll_mean, ll_std]], columns=('Thresh', 'n', 'll_mean', 'll_std'))\n",
    "#     df_fs = df_fs.append(df2)\n",
    "#     print(\"Thresh=%.5f, n=%d, LogLoss mean: %.2f, LogLoss std: %.2f\" % (thresh, select_X_train.shape[1], ll_mean, ll_std))\n",
    "# df_fs.to_csv('./feature_selection/'+model_name+'-'+bet_on+'_'+start_date+'.csv')\n",
    "# Adam: 38\n",
    "# SGD: 56\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = SelectFromModel(model, threshold=0.01181, prefit=True)\n",
    "select_X_test = selection.transform(X_test)\n",
    "cross_val_score(selection_model, select_X_test, y_test, cv=8, scoring=make_scorer(log_loss, greater_is_better=False)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'A_MEANS_FIVE_AC', u'A_MEANS_FIVE_AS', u'A_MEANS_FIVE_AST',\n",
       "       u'A_MEANS_FIVE_FTAG', u'A_MEANS_FIVE_HC', u'A_MEANS_FIVE_HS',\n",
       "       u'A_MEANS_FIVE_HST', u'A_MEANS_FIVE_HTR_A', u'H_MEANS_FIVE_AC',\n",
       "       u'H_MEANS_FIVE_AS', u'H_MEANS_FIVE_AST', u'H_MEANS_FIVE_AY',\n",
       "       u'H_MEANS_FIVE_FTAG', u'H_MEANS_FIVE_FTHG', u'H_MEANS_FIVE_FTR_A',\n",
       "       u'H_MEANS_FIVE_HC', u'H_MEANS_FIVE_HS', u'H_MEANS_FIVE_HST',\n",
       "       u'A_MEANS_THREE_AS', u'A_MEANS_THREE_FTHG', u'A_MEANS_THREE_HS',\n",
       "       u'A_STD_FIVE_HF', u'H_STD_FIVE_HST'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "selection = SelectFromModel(model, threshold=0.01181, prefit=True)\n",
    "feature_idx = selection.get_support()\n",
    "feature_name = pd.get_dummies(df[features_list]).columns[feature_idx]\n",
    "feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = pd.get_dummies(df[['A_MEANS_FIVE_AC', 'A_MEANS_FIVE_AS', 'A_MEANS_FIVE_AST',\n",
    "       'A_MEANS_FIVE_FTAG', 'A_MEANS_FIVE_HC', 'A_MEANS_FIVE_HS',\n",
    "       'A_MEANS_FIVE_HST', 'A_MEANS_FIVE_HTR_A', 'H_MEANS_FIVE_AC',\n",
    "       'H_MEANS_FIVE_AS', 'H_MEANS_FIVE_AST', 'H_MEANS_FIVE_AY',\n",
    "       'H_MEANS_FIVE_FTAG', 'H_MEANS_FIVE_FTHG', 'H_MEANS_FIVE_FTR_A',\n",
    "       'H_MEANS_FIVE_HC', 'H_MEANS_FIVE_HS', 'H_MEANS_FIVE_HST',\n",
    "       'A_MEANS_THREE_AS', 'A_MEANS_THREE_FTHG', 'A_MEANS_THREE_HS',\n",
    "       'A_STD_FIVE_HF', 'H_STD_FIVE_HST']])\n",
    "cross_val_score(selection_model, X_2, y, cv=8, scoring=make_scorer(log_loss, greater_is_better=False)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in feature_name:\n",
    "    print \"'\"+str(col)+\"',\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
