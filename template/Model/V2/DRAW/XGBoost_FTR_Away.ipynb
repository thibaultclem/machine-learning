{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost FTR Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the library\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "import matplotlib.pyplot as plt # plotting library\n",
    "import seaborn as sns # visualization library based on matplotlib\n",
    "from IPython.display import display # Manage multiple output per cell\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features = [\"A_MEANS_FIVE_AC\",\"A_MEANS_FIVE_AF\",\"A_MEANS_FIVE_AR\",\"A_MEANS_FIVE_AS\",\"A_MEANS_FIVE_AST\",\"A_MEANS_FIVE_AY\",\"A_MEANS_FIVE_FTAG\",\"A_MEANS_FIVE_FTHG\",\"A_MEANS_FIVE_FTR_A\",\"A_MEANS_FIVE_FTR_D\",\"A_MEANS_FIVE_FTR_H\",\"A_MEANS_FIVE_HC\",\"A_MEANS_FIVE_HF\",\"A_MEANS_FIVE_HR\",\"A_MEANS_FIVE_HS\",\"A_MEANS_FIVE_HST\",\"A_MEANS_FIVE_HTAG\",\"A_MEANS_FIVE_HTHG\",\"A_MEANS_FIVE_HTR_A\",\"A_MEANS_FIVE_HTR_D\",\"A_MEANS_FIVE_HTR_H\",\"A_MEANS_FIVE_HY\",\"H_MEANS_FIVE_AC\",\"H_MEANS_FIVE_AF\",\"H_MEANS_FIVE_AR\",\"H_MEANS_FIVE_AS\",\"H_MEANS_FIVE_AST\",\"H_MEANS_FIVE_AY\",\"H_MEANS_FIVE_FTAG\",\"H_MEANS_FIVE_FTHG\",\"H_MEANS_FIVE_FTR_A\",\"H_MEANS_FIVE_FTR_D\",\"H_MEANS_FIVE_FTR_H\",\"H_MEANS_FIVE_HC\",\"H_MEANS_FIVE_HF\",\"H_MEANS_FIVE_HR\",\"H_MEANS_FIVE_HS\",\"H_MEANS_FIVE_HST\",\"H_MEANS_FIVE_HTAG\",\"H_MEANS_FIVE_HTHG\",\"H_MEANS_FIVE_HTR_A\",\"H_MEANS_FIVE_HTR_D\",\"H_MEANS_FIVE_HTR_H\",\"H_MEANS_FIVE_HY\",\"A_MEANS_THREE_AC\",\"A_MEANS_THREE_AF\",\"A_MEANS_THREE_AR\",\"A_MEANS_THREE_AS\",\"A_MEANS_THREE_AST\",\"A_MEANS_THREE_AY\",\"A_MEANS_THREE_FTAG\",\"A_MEANS_THREE_FTHG\",\"A_MEANS_THREE_FTR_A\",\"A_MEANS_THREE_FTR_D\",\"A_MEANS_THREE_FTR_H\",\"A_MEANS_THREE_HC\",\"A_MEANS_THREE_HF\",\"A_MEANS_THREE_HR\",\"A_MEANS_THREE_HS\",\"A_MEANS_THREE_HST\",\"A_MEANS_THREE_HTAG\",\"A_MEANS_THREE_HTHG\",\"A_MEANS_THREE_HTR_A\",\"A_MEANS_THREE_HTR_D\",\"A_MEANS_THREE_HTR_H\",\"A_MEANS_THREE_HY\",\"H_MEANS_THREE_AC\",\"H_MEANS_THREE_AF\",\"H_MEANS_THREE_AR\",\"H_MEANS_THREE_AS\",\"H_MEANS_THREE_AST\",\"H_MEANS_THREE_AY\",\"H_MEANS_THREE_FTAG\",\"H_MEANS_THREE_FTHG\",\"H_MEANS_THREE_FTR_A\",\"H_MEANS_THREE_FTR_D\",\"H_MEANS_THREE_FTR_H\",\"H_MEANS_THREE_HC\",\"H_MEANS_THREE_HF\",\"H_MEANS_THREE_HR\",\"H_MEANS_THREE_HS\",\"H_MEANS_THREE_HST\",\"H_MEANS_THREE_HTAG\",\"H_MEANS_THREE_HTHG\",\"H_MEANS_THREE_HTR_A\",\"H_MEANS_THREE_HTR_D\",\"H_MEANS_THREE_HTR_H\",\"H_MEANS_THREE_HY\",\"A_STD_FIVE_AC\",\"A_STD_FIVE_AF\",\"A_STD_FIVE_AR\",\"A_STD_FIVE_AS\",\"A_STD_FIVE_AST\",\"A_STD_FIVE_AY\",\"A_STD_FIVE_FTAG\",\"A_STD_FIVE_FTHG\",\"A_STD_FIVE_FTR_A\",\"A_STD_FIVE_FTR_D\",\"A_STD_FIVE_FTR_H\",\"A_STD_FIVE_HC\",\"A_STD_FIVE_HF\",\"A_STD_FIVE_HR\",\"A_STD_FIVE_HS\",\"A_STD_FIVE_HST\",\"A_STD_FIVE_HTAG\",\"A_STD_FIVE_HTHG\",\"A_STD_FIVE_HTR_A\",\"A_STD_FIVE_HTR_D\",\"A_STD_FIVE_HTR_H\",\"A_STD_FIVE_HY\",\"H_STD_FIVE_AC\",\"H_STD_FIVE_AF\",\"H_STD_FIVE_AR\",\"H_STD_FIVE_AS\",\"H_STD_FIVE_AST\",\"H_STD_FIVE_AY\",\"H_STD_FIVE_FTAG\",\"H_STD_FIVE_FTHG\",\"H_STD_FIVE_FTR_A\",\"H_STD_FIVE_FTR_D\",\"H_STD_FIVE_FTR_H\",\"H_STD_FIVE_HC\",\"H_STD_FIVE_HF\",\"H_STD_FIVE_HR\",\"H_STD_FIVE_HS\",\"H_STD_FIVE_HST\",\"H_STD_FIVE_HTAG\",\"H_STD_FIVE_HTHG\",\"H_STD_FIVE_HTR_A\",\"H_STD_FIVE_HTR_D\",\"H_STD_FIVE_HTR_H\",\"H_STD_FIVE_HY\",\"A_STD_THREE_AC\",\"A_STD_THREE_AF\",\"A_STD_THREE_AR\",\"A_STD_THREE_AS\",\"A_STD_THREE_AST\",\"A_STD_THREE_AY\",\"A_STD_THREE_FTAG\",\"A_STD_THREE_FTHG\",\"A_STD_THREE_FTR_A\",\"A_STD_THREE_FTR_D\",\"A_STD_THREE_FTR_H\",\"A_STD_THREE_HC\",\"A_STD_THREE_HF\",\"A_STD_THREE_HR\",\"A_STD_THREE_HS\",\"A_STD_THREE_HST\",\"A_STD_THREE_HTAG\",\"A_STD_THREE_HTHG\",\"A_STD_THREE_HTR_A\",\"A_STD_THREE_HTR_D\",\"A_STD_THREE_HTR_H\",\"A_STD_THREE_HY\",\"H_STD_THREE_AC\",\"H_STD_THREE_AF\",\"H_STD_THREE_AR\",\"H_STD_THREE_AS\",\"H_STD_THREE_AST\",\"H_STD_THREE_AY\",\"H_STD_THREE_FTAG\",\"H_STD_THREE_FTHG\",\"H_STD_THREE_FTR_A\",\"H_STD_THREE_FTR_D\",\"H_STD_THREE_FTR_H\",\"H_STD_THREE_HC\",\"H_STD_THREE_HF\",\"H_STD_THREE_HR\",\"H_STD_THREE_HS\",\"H_STD_THREE_HST\",\"H_STD_THREE_HTAG\",\"H_STD_THREE_HTHG\",\"H_STD_THREE_HTR_A\",\"H_STD_THREE_HTR_D\",\"H_STD_THREE_HTR_H\",\"H_STD_THREE_HY\",\"INFO_Div\"]\n",
    "best_features_60 = ['A_MEANS_FIVE_AC', 'A_MEANS_FIVE_AS', 'A_MEANS_FIVE_AST',\n",
    "       'A_MEANS_FIVE_AY', 'A_MEANS_FIVE_FTAG', 'A_MEANS_FIVE_FTHG',\n",
    "       'A_MEANS_FIVE_FTR_A', 'A_MEANS_FIVE_FTR_H', 'A_MEANS_FIVE_HC',\n",
    "       'A_MEANS_FIVE_HF', 'A_MEANS_FIVE_HS', 'A_MEANS_FIVE_HST',\n",
    "       'A_MEANS_FIVE_HTHG', 'A_MEANS_FIVE_HTR_A', 'A_MEANS_FIVE_HY',\n",
    "       'H_MEANS_FIVE_AC', 'H_MEANS_FIVE_AS', 'H_MEANS_FIVE_AST',\n",
    "       'H_MEANS_FIVE_AY', 'H_MEANS_FIVE_FTAG', 'H_MEANS_FIVE_FTHG',\n",
    "       'H_MEANS_FIVE_FTR_A', 'H_MEANS_FIVE_FTR_H', 'H_MEANS_FIVE_HC',\n",
    "       'H_MEANS_FIVE_HF', 'H_MEANS_FIVE_HS', 'H_MEANS_FIVE_HST',\n",
    "       'H_MEANS_FIVE_HTAG', 'H_MEANS_FIVE_HTR_A', 'H_MEANS_FIVE_HTR_H',\n",
    "       'A_MEANS_THREE_AC', 'A_MEANS_THREE_AS', 'A_MEANS_THREE_FTHG',\n",
    "       'A_MEANS_THREE_FTR_A', 'A_MEANS_THREE_HC', 'A_MEANS_THREE_HF',\n",
    "       'A_MEANS_THREE_HS', 'A_MEANS_THREE_HST', 'H_MEANS_THREE_AC',\n",
    "       'H_MEANS_THREE_AS', 'H_MEANS_THREE_AST', 'H_MEANS_THREE_FTHG',\n",
    "       'H_MEANS_THREE_HC', 'H_MEANS_THREE_HST', 'H_MEANS_THREE_HTR_H',\n",
    "       'A_STD_FIVE_AF', 'A_STD_FIVE_AS', 'A_STD_FIVE_AST', 'A_STD_FIVE_HC',\n",
    "       'A_STD_FIVE_HF', 'A_STD_FIVE_HS', 'H_STD_FIVE_AF', 'H_STD_FIVE_AS',\n",
    "       'H_STD_FIVE_AST', 'H_STD_FIVE_HC', 'H_STD_FIVE_HF',\n",
    "       'H_STD_FIVE_HST', 'H_STD_FIVE_HTHG', 'H_STD_THREE_AS',\n",
    "       'H_STD_THREE_HST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'XGboost-best_features'\n",
    "target = 'INFO_FTR_A'\n",
    "odd = 'INFO_BbAvA'\n",
    "bet_on = 'A'\n",
    "start_date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DB Sqlite connection\n",
    "import sqlite3\n",
    "db = \"/Users/thibaultclement/Project/ligue1-predict/src/notebook/data/db/soccer_predict.sqlite\"\n",
    "conn = sqlite3.connect(db)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25275, 190)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all prematch data\n",
    "df = pd.read_sql_query(\"SELECT * FROM pre_matchs ORDER BY INFO_Date ASC;\", conn)\n",
    "df = (df[df.columns.drop(['index'])])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18027, 190)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all game between June (include) and October (include)\n",
    "df['INFO_Date'] = pd.to_datetime(df['INFO_Date'])\n",
    "df['INFO_Date'].dt.month\n",
    "df = df[(df['INFO_Date'].dt.month < 6) | (df['INFO_Date'].dt.month > 10)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18027, 190)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a particular league\n",
    "#df = df[(df['INFO_Div'] == 'D1')]\n",
    "#df = df[(df['INFO_Div'] == 'F1')]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.289410328951023"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the average odd\n",
    "df[odd].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18027, 190)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing column where odd is too high or too low\n",
    "# df = df.drop(df[df['INFO_BbAvH'] < 2].index)\n",
    "# df = df.drop(df[df['INFO_BbAvA'] < 2].index)\n",
    "# df = df.drop(df[df['INFO_BbAvH'] > 10].index)\n",
    "# df = df.drop(df[df['INFO_BbAvA'] > 10].index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a INFO_WIN column containing the gain. If bet success it's equal to odd -1, else -1 (loose your bet)\n",
    "df['INFO_WIN'] = df[odd]-1\n",
    "df.loc[df.INFO_FTR != bet_on, 'INFO_WIN'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "H    45.515061\n",
       "A    28.856715\n",
       "D    25.628224\n",
       "Name: INFO_FTR, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Statistic about winners\n",
    "display(plt.show(), 100. * df.INFO_FTR.value_counts() / len(df.INFO_FTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11119542907860423"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much did you win/lost per match if bet on all\n",
    "df.INFO_WIN.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep season 2016/2017 for further test and don't use it for traning\n",
    "import datetime\n",
    "date_start_current_season = datetime.date(2016, 8, 1)\n",
    "df_current_season = df[(df['INFO_Date'] > date_start_current_season)]\n",
    "df = df[(df['INFO_Date'] < date_start_current_season)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2222, 191)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of matches in current season\n",
    "df_current_season.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "features_list = all_features\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X = pd.get_dummies(df[features_list])\n",
    "y = pd.get_dummies(df)[target].astype('bool_')\n",
    "X_current_season = pd.get_dummies(df_current_season[features_list])\n",
    "y_current_season = pd.get_dummies(df_current_season)[target].astype('bool_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc_X = StandardScaler().fit(X)\n",
    "#X = sc_X.transform(X)\n",
    "#X_current_season = sc_X.transform(X_current_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impute of missing values (NaN) with the mean\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp = imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "X_current_season = imp.transform(X_current_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import model\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "classifier = XGBClassifier(nthread=4, seed=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find the best n_estimator\n",
    "# import xgboost as xgb\n",
    "# xgb1 = XGBClassifier(\n",
    "#  learning_rate =0.1,\n",
    "#  n_estimators=5000,\n",
    "#  max_depth=5,\n",
    "#  min_child_weight=1,\n",
    "#  gamma=0,\n",
    "#  subsample=0.8,\n",
    "#  colsample_bytree=0.8,\n",
    "#  objective= 'binary:logistic',\n",
    "#  nthread=4,\n",
    "#  scale_pos_weight=3,\n",
    "#  seed=27)\n",
    "# xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "# cvresult = xgb.cv(xgb1.get_xgb_params(), xgtrain, num_boost_round=5000, nfold=8,\n",
    "#             metrics='logloss', early_stopping_rounds=50, verbose_eval=True)\n",
    "# cvresult.shape[0]\n",
    "# Result = 307 for 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "141.12423706054688"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Grid Search to find the best hyper-parameters for our Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.classification import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "parameters = [{\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [352],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'objective': ['binary:logistic'],\n",
    "    'scale_pos_weight': [1]\n",
    "}]\n",
    "parameters = [{\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [307],\n",
    "    'max_depth': [5],\n",
    "    'min_child_weight': [1],\n",
    "    'gamma': [0],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'objective': ['binary:logistic'],\n",
    "    'scale_pos_weight': [2.5],\n",
    "    'reg_alpha':[0]\n",
    "}]\n",
    "grid_search = GridSearchCV(estimator=classifier,\n",
    "                           param_grid=parameters,\n",
    "                           scoring=make_scorer(log_loss, greater_is_better=False),\n",
    "                           #scoring='roc_auc',\n",
    "                           cv=8,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.097912916859091"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract best score calculated with the GridSearchCV\n",
    "best_score = grid_search.best_score_\n",
    "display(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 1,\n",
       " 'n_estimators': 307,\n",
       " 'objective': 'binary:logistic',\n",
       " 'reg_alpha': 0,\n",
       " 'scale_pos_weight': 2.5,\n",
       " 'subsample': 0.8}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract best hyper-parameter calculated with the GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>...</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.415466</td>\n",
       "      <td>0.110448</td>\n",
       "      <td>-12.097913</td>\n",
       "      <td>-0.690872</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.371119</td>\n",
       "      <td>-0.702849</td>\n",
       "      <td>-12.071209</td>\n",
       "      <td>-0.745663</td>\n",
       "      <td>-12.221161</td>\n",
       "      <td>-0.602952</td>\n",
       "      <td>0.200956</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.266586</td>\n",
       "      <td>0.070608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0      50.415466         0.110448       -12.097913         -0.690872   \n",
       "\n",
       "  param_colsample_bytree param_gamma param_learning_rate param_max_depth  \\\n",
       "0                    0.8           0                 0.1               5   \n",
       "\n",
       "  param_min_child_weight param_n_estimators       ...         \\\n",
       "0                      1                307       ...          \n",
       "\n",
       "  split5_test_score split5_train_score split6_test_score split6_train_score  \\\n",
       "0        -12.371119          -0.702849        -12.071209          -0.745663   \n",
       "\n",
       "  split7_test_score  split7_train_score  std_fit_time  std_score_time  \\\n",
       "0        -12.221161           -0.602952      0.200956        0.024414   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.266586         0.070608  \n",
       "\n",
       "[1 rows x 36 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all results of Grid Search\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "#cv_results.to_csv('./tuning/'+model_name+'-'+target+'_'+start_date+'.csv')\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=307, nthread=4,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=2.5, seed=15, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a new classifier using the best parameters found by the grid search\n",
    "clf = XGBClassifier(\n",
    "    nthread=4, \n",
    "    seed=15,\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_child_weight=best_params['min_child_weight'],\n",
    "    gamma=best_params['gamma'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree'],\n",
    "    objective=best_params['objective'],\n",
    "    scale_pos_weight=best_params['scale_pos_weight'],\n",
    "    reg_alpha=best_params['reg_alpha']\n",
    ")\n",
    "clf.fit(X_train, y_train, eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict target values\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_probs = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.85052941411281"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cross-entropy score\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.41446872645064053, 0.39285714285714285, 0.40337367070040336, None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute precision, recall, F-measure and support\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58018081559374202"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>2565</td>\n",
       "      <td>777</td>\n",
       "      <td>3342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>850</td>\n",
       "      <td>550</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>3415</td>\n",
       "      <td>1327</td>\n",
       "      <td>4742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True   All\n",
       "Actual                      \n",
       "False       2565   777  3342\n",
       "True         850   550  1400\n",
       "All         3415  1327  4742"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the confusion Matrix\n",
    "df_confusion = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAAFlCAYAAAAZGcpRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112c23f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test.ravel(), y_probs[:, 1].ravel())\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('ROC')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL DATASET TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply on all the dataset\n",
    "X_pred = clf.predict(X)\n",
    "df['pred'] = X_pred\n",
    "df_bet_all_seasons = df.drop(df[df.pred == 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4738, 192)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many bet I did\n",
    "df_bet_all_seasons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "A    77.669903\n",
       "H    12.473618\n",
       "D     9.856480\n",
       "Name: INFO_FTR, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What will be the real result of games I bet on\n",
    "display(plt.show(), 100. * df_bet_all_seasons.INFO_FTR.value_counts() / len(df_bet_all_seasons.INFO_FTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2461049357413865"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cross-entropy score\n",
    "log_loss(y, X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.77669902912621358, 0.80613362541073386, 0.79114264215844354, None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score for all dataset\n",
    "precision_recall_fscore_support(y, X_pred, average='binary') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>10182</td>\n",
       "      <td>1058</td>\n",
       "      <td>11240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>885</td>\n",
       "      <td>3680</td>\n",
       "      <td>4565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>11067</td>\n",
       "      <td>4738</td>\n",
       "      <td>15805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True    All\n",
       "Actual                       \n",
       "False      10182  1058  11240\n",
       "True         885  3680   4565\n",
       "All        11067  4738  15805"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the confusion Matrix\n",
    "df_confusion = pd.crosstab(y, X_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3153313634444916"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bet_all_seasons.INFO_WIN.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEASON 2016/2017 TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply on current season that is not use for train and test set\n",
    "X_pred_current_season = clf.predict(X_current_season)\n",
    "X_prob_current_season = clf.predict_proba(X_current_season)\n",
    "df_current_season['pred'] = X_pred_current_season\n",
    "df_current_season['prob'] = X_prob_current_season[:,1:]\n",
    "df_current_season['prob_less_bet'] = df_current_season['prob'] - df_current_season[odd].apply(lambda x: 1/x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove games I didn't bet on\n",
    "df_bet_current_season = df_current_season\n",
    "df_bet_current_season = df_bet_current_season.drop(df_bet_current_season[df_bet_current_season.pred == 0].index)\n",
    "#df_bet_current_season = df_bet_current_season[df_bet_current_season.prob > 0.6]\n",
    "df_bet_current_season = df_bet_current_season[df_bet_current_season.prob_less_bet > 0.2]\n",
    "#df_bet_current_season = df_bet_current_season[df_bet_current_season[odd] > 2]\n",
    "df_bet_current_season = df_bet_current_season[df_bet_current_season[odd] < 4]\n",
    "#df_bet_current_season = df_bet_current_season[df_bet_current_season['INFO_BbAvH'] > 1.5]\n",
    "#df_bet_current_season = df_bet_current_season[df_bet_current_season['INFO_BbAvA'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8865578635014844"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bet_current_season[odd].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 194)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many bet I did\n",
    "df_bet_current_season.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2222,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many matches was play\n",
    "X_pred_current_season.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "A    43.964232\n",
       "H    31.147541\n",
       "D    24.888227\n",
       "Name: INFO_FTR, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What will be the real result of games I bet on\n",
    "display(plt.show(), 100. * df_bet_current_season.INFO_FTR.value_counts() / len(df_bet_current_season.INFO_FTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.124485601291081"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cross-entropy score\n",
    "log_loss(y_current_season, X_pred_current_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.40604467805519051, 0.48508634222919939, 0.44206008583690987, None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score for this current season\n",
    "precision_recall_fscore_support(y_current_season, X_pred_current_season, average='binary') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5999564203259562"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_current_season, X_pred_current_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>1133</td>\n",
       "      <td>452</td>\n",
       "      <td>1585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>328</td>\n",
       "      <td>309</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1461</td>\n",
       "      <td>761</td>\n",
       "      <td>2222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True   All\n",
       "Actual                      \n",
       "False       1133   452  1585\n",
       "True         328   309   637\n",
       "All         1461   761  2222"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the confusion Matrix\n",
    "df_confusion = pd.crosstab(y_current_season, X_pred_current_season, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.058241430700447114"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What I win/lost on each match\n",
    "df_bet_current_season.INFO_WIN.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_MEANS_FIVE_AC</th>\n",
       "      <th>A_MEANS_FIVE_AF</th>\n",
       "      <th>A_MEANS_FIVE_AR</th>\n",
       "      <th>A_MEANS_FIVE_AS</th>\n",
       "      <th>A_MEANS_FIVE_AST</th>\n",
       "      <th>A_MEANS_FIVE_AY</th>\n",
       "      <th>A_MEANS_FIVE_FTAG</th>\n",
       "      <th>A_MEANS_FIVE_FTHG</th>\n",
       "      <th>A_MEANS_FIVE_FTR_A</th>\n",
       "      <th>A_MEANS_FIVE_FTR_D</th>\n",
       "      <th>...</th>\n",
       "      <th>INFO_FTR</th>\n",
       "      <th>INFO_HTR</th>\n",
       "      <th>INFO_HomeTeam</th>\n",
       "      <th>INFO_PSA</th>\n",
       "      <th>INFO_PSD</th>\n",
       "      <th>INFO_PSH</th>\n",
       "      <th>INFO_WIN</th>\n",
       "      <th>pred</th>\n",
       "      <th>prob</th>\n",
       "      <th>prob_less_bet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23054</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Coventry</td>\n",
       "      <td>4.99</td>\n",
       "      <td>3.85</td>\n",
       "      <td>1.77</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.505923</td>\n",
       "      <td>0.275509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23059</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Partick</td>\n",
       "      <td>1.87</td>\n",
       "      <td>3.81</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.82</td>\n",
       "      <td>True</td>\n",
       "      <td>0.711606</td>\n",
       "      <td>0.162155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23063</th>\n",
       "      <td>6.4</td>\n",
       "      <td>10.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>1.48</td>\n",
       "      <td>4.82</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>True</td>\n",
       "      <td>0.733252</td>\n",
       "      <td>0.048321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23067</th>\n",
       "      <td>6.4</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Burnley</td>\n",
       "      <td>2.32</td>\n",
       "      <td>3.37</td>\n",
       "      <td>3.42</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.613450</td>\n",
       "      <td>0.180549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23070</th>\n",
       "      <td>4.4</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>4.13</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.652072</td>\n",
       "      <td>0.393674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23073</th>\n",
       "      <td>4.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Burton</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.55</td>\n",
       "      <td>2.21</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.616898</td>\n",
       "      <td>0.310150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23081</th>\n",
       "      <td>4.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Wolves</td>\n",
       "      <td>2.94</td>\n",
       "      <td>3.23</td>\n",
       "      <td>2.66</td>\n",
       "      <td>1.84</td>\n",
       "      <td>True</td>\n",
       "      <td>0.591446</td>\n",
       "      <td>0.239333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23083</th>\n",
       "      <td>4.6</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>5.01</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.540715</td>\n",
       "      <td>0.326123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23084</th>\n",
       "      <td>2.4</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>H</td>\n",
       "      <td>Dijon</td>\n",
       "      <td>3.11</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.56</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.657867</td>\n",
       "      <td>0.325641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23087</th>\n",
       "      <td>2.4</td>\n",
       "      <td>17.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>Nantes</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.13</td>\n",
       "      <td>2.41</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.527696</td>\n",
       "      <td>0.232711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23092</th>\n",
       "      <td>2.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Osasuna</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2.32</td>\n",
       "      <td>2.42</td>\n",
       "      <td>True</td>\n",
       "      <td>0.592126</td>\n",
       "      <td>0.299729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23097</th>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Kilmarnock</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.75</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.628887</td>\n",
       "      <td>0.239782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23100</th>\n",
       "      <td>7.2</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>H</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>4.35</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.95</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.739646</td>\n",
       "      <td>0.496337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23101</th>\n",
       "      <td>2.4</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>A</td>\n",
       "      <td>Hull</td>\n",
       "      <td>1.70</td>\n",
       "      <td>3.88</td>\n",
       "      <td>5.73</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.695174</td>\n",
       "      <td>0.106939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23102</th>\n",
       "      <td>4.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Leicester</td>\n",
       "      <td>5.49</td>\n",
       "      <td>3.63</td>\n",
       "      <td>1.78</td>\n",
       "      <td>4.11</td>\n",
       "      <td>True</td>\n",
       "      <td>0.586321</td>\n",
       "      <td>0.390626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23104</th>\n",
       "      <td>4.6</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>1.76</td>\n",
       "      <td>4.01</td>\n",
       "      <td>4.94</td>\n",
       "      <td>0.74</td>\n",
       "      <td>True</td>\n",
       "      <td>0.628122</td>\n",
       "      <td>0.053409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23108</th>\n",
       "      <td>6.2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Chievo</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4.25</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.45</td>\n",
       "      <td>True</td>\n",
       "      <td>0.874600</td>\n",
       "      <td>0.184945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23112</th>\n",
       "      <td>3.6</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Palermo</td>\n",
       "      <td>1.69</td>\n",
       "      <td>3.92</td>\n",
       "      <td>5.80</td>\n",
       "      <td>0.66</td>\n",
       "      <td>True</td>\n",
       "      <td>0.779108</td>\n",
       "      <td>0.176698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Ross County</td>\n",
       "      <td>1.68</td>\n",
       "      <td>4.21</td>\n",
       "      <td>5.24</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.602302</td>\n",
       "      <td>-0.003758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23123</th>\n",
       "      <td>4.2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>8.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Bury</td>\n",
       "      <td>3.28</td>\n",
       "      <td>3.46</td>\n",
       "      <td>2.32</td>\n",
       "      <td>2.10</td>\n",
       "      <td>True</td>\n",
       "      <td>0.541157</td>\n",
       "      <td>0.218577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23124</th>\n",
       "      <td>6.6</td>\n",
       "      <td>10.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Coventry</td>\n",
       "      <td>2.41</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1.35</td>\n",
       "      <td>True</td>\n",
       "      <td>0.689296</td>\n",
       "      <td>0.263764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23126</th>\n",
       "      <td>6.6</td>\n",
       "      <td>13.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.6</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Millwall</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.61</td>\n",
       "      <td>2.26</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.507258</td>\n",
       "      <td>0.191801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23128</th>\n",
       "      <td>6.2</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Oldham</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.36</td>\n",
       "      <td>2.96</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.647099</td>\n",
       "      <td>0.245492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23129</th>\n",
       "      <td>3.6</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Port Vale</td>\n",
       "      <td>2.67</td>\n",
       "      <td>3.36</td>\n",
       "      <td>2.84</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.661268</td>\n",
       "      <td>0.262862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23130</th>\n",
       "      <td>7.0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Shrewsbury</td>\n",
       "      <td>2.03</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.76</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.893711</td>\n",
       "      <td>0.396199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23131</th>\n",
       "      <td>6.6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Swindon</td>\n",
       "      <td>2.23</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.45</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.621383</td>\n",
       "      <td>0.158420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23132</th>\n",
       "      <td>8.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.6</td>\n",
       "      <td>7.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>H</td>\n",
       "      <td>Chesterfield</td>\n",
       "      <td>1.68</td>\n",
       "      <td>4.02</td>\n",
       "      <td>5.56</td>\n",
       "      <td>0.65</td>\n",
       "      <td>True</td>\n",
       "      <td>0.811473</td>\n",
       "      <td>0.205412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23134</th>\n",
       "      <td>5.6</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>H</td>\n",
       "      <td>Leverkusen</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.43</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.13</td>\n",
       "      <td>True</td>\n",
       "      <td>0.716957</td>\n",
       "      <td>0.397469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23136</th>\n",
       "      <td>5.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>13.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Lille</td>\n",
       "      <td>2.53</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.24</td>\n",
       "      <td>1.46</td>\n",
       "      <td>True</td>\n",
       "      <td>0.584478</td>\n",
       "      <td>0.177974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23138</th>\n",
       "      <td>3.8</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>9.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Betis</td>\n",
       "      <td>3.21</td>\n",
       "      <td>3.48</td>\n",
       "      <td>2.37</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.559658</td>\n",
       "      <td>0.232860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25187</th>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>H</td>\n",
       "      <td>Sampdoria</td>\n",
       "      <td>4.82</td>\n",
       "      <td>3.82</td>\n",
       "      <td>1.78</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.731437</td>\n",
       "      <td>0.507222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25193</th>\n",
       "      <td>5.8</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>16.8</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Las Palmas</td>\n",
       "      <td>1.11</td>\n",
       "      <td>12.80</td>\n",
       "      <td>23.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>True</td>\n",
       "      <td>0.551804</td>\n",
       "      <td>-0.357287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25198</th>\n",
       "      <td>6.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>28.49</td>\n",
       "      <td>13.62</td>\n",
       "      <td>1.10</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.617039</td>\n",
       "      <td>0.579260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25200</th>\n",
       "      <td>3.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Motherwell</td>\n",
       "      <td>4.14</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1.93</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.537731</td>\n",
       "      <td>0.283924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25202</th>\n",
       "      <td>4.8</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.76</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.522154</td>\n",
       "      <td>0.158517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25204</th>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Celta</td>\n",
       "      <td>1.23</td>\n",
       "      <td>7.79</td>\n",
       "      <td>12.87</td>\n",
       "      <td>0.22</td>\n",
       "      <td>True</td>\n",
       "      <td>0.870814</td>\n",
       "      <td>0.051142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25205</th>\n",
       "      <td>4.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Dundee</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.48</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.80</td>\n",
       "      <td>True</td>\n",
       "      <td>0.643317</td>\n",
       "      <td>0.286174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25208</th>\n",
       "      <td>7.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Leicester</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3.78</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.85</td>\n",
       "      <td>True</td>\n",
       "      <td>0.513569</td>\n",
       "      <td>-0.026971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25209</th>\n",
       "      <td>4.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11.4</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Partick</td>\n",
       "      <td>1.34</td>\n",
       "      <td>5.43</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.30</td>\n",
       "      <td>True</td>\n",
       "      <td>0.539571</td>\n",
       "      <td>-0.229660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25213</th>\n",
       "      <td>5.4</td>\n",
       "      <td>17.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>Ein Frankfurt</td>\n",
       "      <td>1.97</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3.86</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.837197</td>\n",
       "      <td>0.319062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25214</th>\n",
       "      <td>5.2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>FC Koln</td>\n",
       "      <td>5.47</td>\n",
       "      <td>4.24</td>\n",
       "      <td>1.66</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.591860</td>\n",
       "      <td>0.404243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25220</th>\n",
       "      <td>5.6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Angers</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.11</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.616097</td>\n",
       "      <td>0.322842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25222</th>\n",
       "      <td>4.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Lille</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.514667</td>\n",
       "      <td>0.171024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25223</th>\n",
       "      <td>5.2</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>Lorient</td>\n",
       "      <td>2.79</td>\n",
       "      <td>3.79</td>\n",
       "      <td>2.49</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.593530</td>\n",
       "      <td>0.224526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25228</th>\n",
       "      <td>5.6</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Rennes</td>\n",
       "      <td>2.09</td>\n",
       "      <td>3.80</td>\n",
       "      <td>3.53</td>\n",
       "      <td>1.06</td>\n",
       "      <td>True</td>\n",
       "      <td>0.593234</td>\n",
       "      <td>0.107798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25230</th>\n",
       "      <td>4.6</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Chievo</td>\n",
       "      <td>1.25</td>\n",
       "      <td>6.95</td>\n",
       "      <td>13.08</td>\n",
       "      <td>0.23</td>\n",
       "      <td>True</td>\n",
       "      <td>0.888811</td>\n",
       "      <td>0.075803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25237</th>\n",
       "      <td>4.4</td>\n",
       "      <td>14.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Inverness C</td>\n",
       "      <td>5.28</td>\n",
       "      <td>4.26</td>\n",
       "      <td>1.66</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.527988</td>\n",
       "      <td>0.319654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25240</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Burnley</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.02</td>\n",
       "      <td>True</td>\n",
       "      <td>0.555823</td>\n",
       "      <td>0.224697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25242</th>\n",
       "      <td>7.2</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Hull</td>\n",
       "      <td>1.55</td>\n",
       "      <td>4.61</td>\n",
       "      <td>6.36</td>\n",
       "      <td>0.53</td>\n",
       "      <td>True</td>\n",
       "      <td>0.812156</td>\n",
       "      <td>0.158561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25246</th>\n",
       "      <td>4.2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>4.73</td>\n",
       "      <td>3.95</td>\n",
       "      <td>1.80</td>\n",
       "      <td>3.60</td>\n",
       "      <td>True</td>\n",
       "      <td>0.543791</td>\n",
       "      <td>0.326400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25248</th>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Watford</td>\n",
       "      <td>1.29</td>\n",
       "      <td>6.21</td>\n",
       "      <td>11.51</td>\n",
       "      <td>0.28</td>\n",
       "      <td>True</td>\n",
       "      <td>0.824997</td>\n",
       "      <td>0.043747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25249</th>\n",
       "      <td>6.4</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Empoli</td>\n",
       "      <td>2.99</td>\n",
       "      <td>3.52</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.04</td>\n",
       "      <td>True</td>\n",
       "      <td>0.558717</td>\n",
       "      <td>0.229770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25252</th>\n",
       "      <td>7.8</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Lazio</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.47</td>\n",
       "      <td>True</td>\n",
       "      <td>0.548395</td>\n",
       "      <td>0.260211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25255</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>H</td>\n",
       "      <td>Udinese</td>\n",
       "      <td>3.81</td>\n",
       "      <td>3.52</td>\n",
       "      <td>2.11</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.558454</td>\n",
       "      <td>0.271921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25259</th>\n",
       "      <td>5.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Malaga</td>\n",
       "      <td>1.24</td>\n",
       "      <td>7.48</td>\n",
       "      <td>12.70</td>\n",
       "      <td>0.23</td>\n",
       "      <td>True</td>\n",
       "      <td>0.830143</td>\n",
       "      <td>0.017135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25264</th>\n",
       "      <td>4.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Pescara</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.57</td>\n",
       "      <td>2.31</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.833293</td>\n",
       "      <td>0.501068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25266</th>\n",
       "      <td>4.8</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Bologna</td>\n",
       "      <td>1.78</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.78</td>\n",
       "      <td>True</td>\n",
       "      <td>0.631421</td>\n",
       "      <td>0.069624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25267</th>\n",
       "      <td>6.6</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>14.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Cagliari</td>\n",
       "      <td>1.93</td>\n",
       "      <td>4.22</td>\n",
       "      <td>3.69</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.544879</td>\n",
       "      <td>0.021319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25268</th>\n",
       "      <td>6.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Crotone</td>\n",
       "      <td>3.18</td>\n",
       "      <td>4.06</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.802106</td>\n",
       "      <td>0.481593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25273</th>\n",
       "      <td>5.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Sampdoria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.17</td>\n",
       "      <td>True</td>\n",
       "      <td>0.800816</td>\n",
       "      <td>-0.053885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>761 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       A_MEANS_FIVE_AC  A_MEANS_FIVE_AF  A_MEANS_FIVE_AR  A_MEANS_FIVE_AS  \\\n",
       "23054              7.0              9.2              0.0             11.2   \n",
       "23059              5.0             11.2              0.2              9.0   \n",
       "23063              6.4             10.8              0.0             13.0   \n",
       "23067              6.4              9.8              0.0             15.0   \n",
       "23070              4.4             10.2              0.0             10.0   \n",
       "23073              4.0             11.2              0.0             15.4   \n",
       "23081              4.4             11.2              0.0             10.6   \n",
       "23083              4.6             12.4              0.2             12.6   \n",
       "23084              2.4             14.0              0.6              6.8   \n",
       "23087              2.4             17.4              0.0              7.8   \n",
       "23092              2.0             14.6              0.0              6.2   \n",
       "23097              5.0             13.0              0.0             13.2   \n",
       "23100              7.2             13.2              0.0             17.6   \n",
       "23101              2.4              9.8              0.0             11.8   \n",
       "23102              4.8             13.0              0.0             11.4   \n",
       "23104              4.6             14.4              0.0             15.6   \n",
       "23108              6.2             13.4              0.0             17.0   \n",
       "23112              3.6             13.0              0.6             12.6   \n",
       "23121              5.6             13.2              0.2             12.0   \n",
       "23123              4.2             15.0              0.2              8.8   \n",
       "23124              6.6             10.6              0.0             13.2   \n",
       "23126              6.6             13.6              0.2             15.6   \n",
       "23128              6.2             11.4              0.0             11.4   \n",
       "23129              3.6             12.8              0.0              8.8   \n",
       "23130              7.0             11.8              0.0             14.2   \n",
       "23131              6.6             11.0              0.2             12.0   \n",
       "23132              8.6             15.0              0.2             14.6   \n",
       "23134              5.6             15.4              0.0             14.2   \n",
       "23136              5.0             13.4              0.4             13.6   \n",
       "23138              3.8             11.4              0.4              9.2   \n",
       "...                ...              ...              ...              ...   \n",
       "25187              4.0             15.0              0.2             10.8   \n",
       "25193              5.8             12.8              0.2             16.8   \n",
       "25198              6.0             11.2              0.0             11.6   \n",
       "25200              3.6             11.4              0.0              8.0   \n",
       "25202              4.8             13.8              0.2             10.2   \n",
       "25204              5.0              9.0              0.0             16.6   \n",
       "25205              4.2             10.6              0.2             11.0   \n",
       "25208              7.0             10.4              0.0             14.6   \n",
       "25209              4.4             12.0              0.2             11.4   \n",
       "25213              5.4             17.8              0.0             11.4   \n",
       "25214              5.2             16.0              0.2             11.0   \n",
       "25220              5.6              9.0              0.2             12.0   \n",
       "25222              4.2             14.8              0.0             11.0   \n",
       "25223              5.2             12.4              0.2             12.6   \n",
       "25228              5.6             12.4              0.0             14.6   \n",
       "25230              4.6             12.8              0.0             17.0   \n",
       "25237              4.4             14.2              0.0              7.6   \n",
       "25240              3.0              8.4              0.2             10.6   \n",
       "25242              7.2              9.2              0.0             18.4   \n",
       "25246              4.2             13.4              0.0             10.6   \n",
       "25248              8.0             10.0              0.0             17.0   \n",
       "25249              6.4             16.4              0.2             11.0   \n",
       "25252              7.8             14.8              0.2             15.8   \n",
       "25255              3.0              9.4              0.2             10.4   \n",
       "25259              5.2             10.0              0.0             17.4   \n",
       "25264              4.0             14.8              0.6              8.6   \n",
       "25266              4.8              9.2              0.0             11.8   \n",
       "25267              6.6             12.4              0.4             14.4   \n",
       "25268              6.0             13.2              0.0             15.0   \n",
       "25273              5.0             11.4              0.0             14.8   \n",
       "\n",
       "       A_MEANS_FIVE_AST  A_MEANS_FIVE_AY  A_MEANS_FIVE_FTAG  \\\n",
       "23054               2.8              1.4                0.4   \n",
       "23059               4.6              1.0                1.6   \n",
       "23063               5.2              1.4                1.6   \n",
       "23067               3.8              2.2                1.4   \n",
       "23070               3.6              2.0                1.0   \n",
       "23073               5.2              2.2                1.8   \n",
       "23081               4.0              2.0                0.8   \n",
       "23083               3.8              2.0                0.6   \n",
       "23084               3.6              1.8                1.2   \n",
       "23087               2.0              4.0                0.6   \n",
       "23092               2.0              3.8                1.0   \n",
       "23097               4.4              3.2                1.4   \n",
       "23100               5.8              1.8                1.6   \n",
       "23101               4.4              2.0                1.0   \n",
       "23102               3.2              3.0                0.8   \n",
       "23104               4.8              2.8                1.0   \n",
       "23108               5.6              2.4                1.2   \n",
       "23112               3.0              2.6                1.2   \n",
       "23121               4.6              2.2                1.2   \n",
       "23123               3.8              1.8                0.8   \n",
       "23124               5.2              1.2                2.0   \n",
       "23126               5.8              2.6                2.0   \n",
       "23128               4.8              1.4                2.0   \n",
       "23129               4.0              2.2                1.0   \n",
       "23130               6.8              1.8                1.2   \n",
       "23131               4.6              1.0                1.0   \n",
       "23132               7.2              1.8                2.4   \n",
       "23134               5.4              2.0                2.0   \n",
       "23136               4.8              2.0                0.8   \n",
       "23138               4.0              3.0                1.8   \n",
       "...                 ...              ...                ...   \n",
       "25187               2.8              2.0                0.8   \n",
       "25193               7.8              1.6                2.2   \n",
       "25198               3.2              2.0                0.4   \n",
       "25200               3.6              1.8                1.6   \n",
       "25202               4.4              1.6                1.2   \n",
       "25204               7.4              1.4                3.8   \n",
       "25205               4.4              1.4                0.8   \n",
       "25208               5.2              2.0                1.2   \n",
       "25209               5.8              2.2                3.4   \n",
       "25213               4.6              2.4                2.0   \n",
       "25214               4.2              2.0                0.8   \n",
       "25220               3.2              1.0                0.8   \n",
       "25222               4.6              1.8                1.6   \n",
       "25223               3.4              2.0                1.0   \n",
       "25228               6.4              1.0                2.2   \n",
       "25230               6.2              1.6                3.4   \n",
       "25237               2.6              3.6                0.8   \n",
       "25240               4.0              1.2                1.0   \n",
       "25242               7.2              1.4                2.4   \n",
       "25246               2.4              2.2                0.4   \n",
       "25248               5.6              1.8                2.0   \n",
       "25249               5.2              2.4                2.0   \n",
       "25252               5.6              1.0                2.4   \n",
       "25255               4.2              1.8                1.6   \n",
       "25259               8.2              1.0                4.2   \n",
       "25264               2.8              2.0                1.0   \n",
       "25266               5.2              1.2                1.4   \n",
       "25267               4.8              2.2                1.2   \n",
       "25268               5.2              3.4                1.8   \n",
       "25273               5.8              2.2                2.8   \n",
       "\n",
       "       A_MEANS_FIVE_FTHG  A_MEANS_FIVE_FTR_A  A_MEANS_FIVE_FTR_D  \\\n",
       "23054                1.2                 0.0                 0.2   \n",
       "23059                1.2                 0.4                 0.2   \n",
       "23063                1.6                 0.2                 0.2   \n",
       "23067                1.6                 0.4                 0.2   \n",
       "23070                1.4                 0.2                 0.4   \n",
       "23073                1.0                 0.6                 0.0   \n",
       "23081                0.8                 0.2                 0.4   \n",
       "23083                1.8                 0.0                 0.0   \n",
       "23084                1.0                 0.4                 0.0   \n",
       "23087                0.8                 0.2                 0.4   \n",
       "23092                1.8                 0.2                 0.2   \n",
       "23097                2.2                 0.0                 0.6   \n",
       "23100                0.6                 0.4                 0.6   \n",
       "23101                1.0                 0.2                 0.4   \n",
       "23102                1.0                 0.2                 0.4   \n",
       "23104                1.6                 0.4                 0.2   \n",
       "23108                0.6                 0.6                 0.0   \n",
       "23112                1.6                 0.4                 0.2   \n",
       "23121                1.8                 0.4                 0.2   \n",
       "23123                2.4                 0.2                 0.2   \n",
       "23124                0.8                 0.8                 0.0   \n",
       "23126                1.4                 0.6                 0.2   \n",
       "23128                1.0                 0.8                 0.2   \n",
       "23129                1.6                 0.2                 0.0   \n",
       "23130                1.2                 0.2                 0.4   \n",
       "23131                1.0                 0.0                 1.0   \n",
       "23132                1.6                 0.4                 0.6   \n",
       "23134                0.6                 0.6                 0.4   \n",
       "23136                1.6                 0.2                 0.2   \n",
       "23138                2.4                 0.2                 0.2   \n",
       "...                  ...                 ...                 ...   \n",
       "25187                2.8                 0.2                 0.0   \n",
       "25193                1.4                 0.6                 0.0   \n",
       "25198                1.2                 0.2                 0.0   \n",
       "25200                1.0                 0.6                 0.2   \n",
       "25202                0.8                 0.4                 0.2   \n",
       "25204                1.4                 1.0                 0.0   \n",
       "25205                2.4                 0.0                 0.2   \n",
       "25208                0.8                 0.6                 0.0   \n",
       "25209                1.0                 0.8                 0.2   \n",
       "25213                1.8                 0.4                 0.4   \n",
       "25214                1.4                 0.0                 0.4   \n",
       "25220                2.0                 0.2                 0.0   \n",
       "25222                2.0                 0.4                 0.2   \n",
       "25223                1.2                 0.2                 0.4   \n",
       "25228                0.4                 1.0                 0.0   \n",
       "25230                0.6                 1.0                 0.0   \n",
       "25237                0.8                 0.4                 0.2   \n",
       "25240                2.0                 0.0                 0.4   \n",
       "25242                0.6                 0.8                 0.0   \n",
       "25246                1.4                 0.0                 0.4   \n",
       "25248                1.2                 0.4                 0.4   \n",
       "25249                1.8                 0.4                 0.4   \n",
       "25252                2.2                 0.2                 0.2   \n",
       "25255                2.2                 0.4                 0.2   \n",
       "25259                1.4                 1.0                 0.0   \n",
       "25264                3.6                 0.0                 0.2   \n",
       "25266                1.2                 0.4                 0.4   \n",
       "25267                1.4                 0.0                 0.8   \n",
       "25268                1.4                 0.4                 0.4   \n",
       "25273                0.8                 0.8                 0.2   \n",
       "\n",
       "           ...        INFO_FTR  INFO_HTR  INFO_HomeTeam  INFO_PSA  INFO_PSD  \\\n",
       "23054      ...               H         D       Coventry      4.99      3.85   \n",
       "23059      ...               A         A        Partick      1.87      3.81   \n",
       "23063      ...               A         A        Hamburg      1.48      4.82   \n",
       "23067      ...               H         H        Burnley      2.32      3.37   \n",
       "23070      ...               D         D       West Ham      4.13      3.59   \n",
       "23073      ...               D         D         Burton      3.43      3.55   \n",
       "23081      ...               A         A         Wolves      2.94      3.23   \n",
       "23083      ...               H         H       Bordeaux      5.01      3.65   \n",
       "23084      ...               D         H          Dijon      3.11      3.20   \n",
       "23087      ...               D         A         Nantes      3.45      3.13   \n",
       "23092      ...               A         D        Osasuna      3.54      3.25   \n",
       "23097      ...               D         D     Kilmarnock      2.69      3.45   \n",
       "23100      ...               D         H        Arsenal      4.35      3.59   \n",
       "23101      ...               H         A           Hull      1.70      3.88   \n",
       "23102      ...               A         D      Leicester      5.49      3.63   \n",
       "23104      ...               A         A        Swansea      1.76      4.01   \n",
       "23108      ...               A         D         Chievo      1.49      4.25   \n",
       "23112      ...               A         A        Palermo      1.69      3.92   \n",
       "23121      ...               D         D    Ross County      1.68      4.21   \n",
       "23123      ...               A         A           Bury      3.28      3.46   \n",
       "23124      ...               A         D       Coventry      2.41      3.46   \n",
       "23126      ...               H         H       Millwall      3.27      3.61   \n",
       "23128      ...               D         D         Oldham      2.60      3.36   \n",
       "23129      ...               H         D      Port Vale      2.67      3.36   \n",
       "23130      ...               H         H     Shrewsbury      2.03      3.74   \n",
       "23131      ...               H         H        Swindon      2.23      3.48   \n",
       "23132      ...               A         H   Chesterfield      1.68      4.02   \n",
       "23134      ...               A         H     Leverkusen      3.19      3.43   \n",
       "23136      ...               A         A          Lille      2.53      3.16   \n",
       "23138      ...               H         H          Betis      3.21      3.48   \n",
       "...        ...             ...       ...            ...       ...       ...   \n",
       "25187      ...               D         H      Sampdoria      4.82      3.82   \n",
       "25193      ...               A         A     Las Palmas      1.11     12.80   \n",
       "25198      ...               H         D        Arsenal     28.49     13.62   \n",
       "25200      ...               H         D     Motherwell      4.14      3.76   \n",
       "25202      ...               D         D    Southampton      2.80      3.33   \n",
       "25204      ...               A         A          Celta      1.23      7.79   \n",
       "25205      ...               A         A         Dundee      2.88      3.48   \n",
       "25208      ...               A         A      Leicester      1.88      3.78   \n",
       "25209      ...               A         A        Partick      1.34      5.43   \n",
       "25213      ...               D         A  Ein Frankfurt      1.97      3.93   \n",
       "25214      ...               H         H        FC Koln      5.47      4.24   \n",
       "25220      ...               H         D         Angers      3.66      3.60   \n",
       "25222      ...               H         H          Lille      2.98      3.56   \n",
       "25223      ...               D         A        Lorient      2.79      3.79   \n",
       "25228      ...               A         A         Rennes      2.09      3.80   \n",
       "25230      ...               A         D         Chievo      1.25      6.95   \n",
       "25237      ...               H         D    Inverness C      5.28      4.26   \n",
       "25240      ...               A         D        Burnley      3.26      3.40   \n",
       "25242      ...               A         A           Hull      1.55      4.61   \n",
       "25246      ...               A         D    Southampton      4.73      3.95   \n",
       "25248      ...               A         A        Watford      1.29      6.21   \n",
       "25249      ...               A         A         Empoli      2.99      3.52   \n",
       "25252      ...               A         A          Lazio      3.63      3.95   \n",
       "25255      ...               D         H        Udinese      3.81      3.52   \n",
       "25259      ...               A         A         Malaga      1.24      7.48   \n",
       "25264      ...               H         H        Pescara      3.26      3.57   \n",
       "25266      ...               A         D        Bologna      1.78      3.99   \n",
       "25267      ...               H         H       Cagliari      1.93      4.22   \n",
       "25268      ...               H         H        Crotone      3.18      4.06   \n",
       "25273      ...               A         A      Sampdoria       NaN       NaN   \n",
       "\n",
       "       INFO_PSH  INFO_WIN  pred      prob  prob_less_bet  \n",
       "23054      1.77     -1.00  True  0.505923       0.275509  \n",
       "23059      4.38      0.82  True  0.711606       0.162155  \n",
       "23063      7.30      0.46  True  0.733252       0.048321  \n",
       "23067      3.42     -1.00  True  0.613450       0.180549  \n",
       "23070      2.00     -1.00  True  0.652072       0.393674  \n",
       "23073      2.21     -1.00  True  0.616898       0.310150  \n",
       "23081      2.66      1.84  True  0.591446       0.239333  \n",
       "23083      1.81     -1.00  True  0.540715       0.326123  \n",
       "23084      2.56     -1.00  True  0.657867       0.325641  \n",
       "23087      2.41     -1.00  True  0.527696       0.232711  \n",
       "23092      2.32      2.42  True  0.592126       0.299729  \n",
       "23097      2.75     -1.00  True  0.628887       0.239782  \n",
       "23100      1.95     -1.00  True  0.739646       0.496337  \n",
       "23101      5.73     -1.00  True  0.695174       0.106939  \n",
       "23102      1.78      4.11  True  0.586321       0.390626  \n",
       "23104      4.94      0.74  True  0.628122       0.053409  \n",
       "23108      8.91      0.45  True  0.874600       0.184945  \n",
       "23112      5.80      0.66  True  0.779108       0.176698  \n",
       "23121      5.24     -1.00  True  0.602302      -0.003758  \n",
       "23123      2.32      2.10  True  0.541157       0.218577  \n",
       "23124      3.12      1.35  True  0.689296       0.263764  \n",
       "23126      2.26     -1.00  True  0.507258       0.191801  \n",
       "23128      2.96     -1.00  True  0.647099       0.245492  \n",
       "23129      2.84     -1.00  True  0.661268       0.262862  \n",
       "23130      3.76     -1.00  True  0.893711       0.396199  \n",
       "23131      3.45     -1.00  True  0.621383       0.158420  \n",
       "23132      5.56      0.65  True  0.811473       0.205412  \n",
       "23134      2.41      2.13  True  0.716957       0.397469  \n",
       "23136      3.24      1.46  True  0.584478       0.177974  \n",
       "23138      2.37     -1.00  True  0.559658       0.232860  \n",
       "...         ...       ...   ...       ...            ...  \n",
       "25187      1.78     -1.00  True  0.731437       0.507222  \n",
       "25193     23.00      0.10  True  0.551804      -0.357287  \n",
       "25198      1.10     -1.00  True  0.617039       0.579260  \n",
       "25200      1.93     -1.00  True  0.537731       0.283924  \n",
       "25202      2.76     -1.00  True  0.522154       0.158517  \n",
       "25204     12.87      0.22  True  0.870814       0.051142  \n",
       "25205      2.55      1.80  True  0.643317       0.286174  \n",
       "25208      4.44      0.85  True  0.513569      -0.026971  \n",
       "25209     10.55      0.30  True  0.539571      -0.229660  \n",
       "25213      3.86     -1.00  True  0.837197       0.319062  \n",
       "25214      1.66     -1.00  True  0.591860       0.404243  \n",
       "25220      2.11     -1.00  True  0.616097       0.322842  \n",
       "25222      2.45     -1.00  True  0.514667       0.171024  \n",
       "25223      2.49     -1.00  True  0.593530       0.224526  \n",
       "25228      3.53      1.06  True  0.593234       0.107798  \n",
       "25230     13.08      0.23  True  0.888811       0.075803  \n",
       "25237      1.66     -1.00  True  0.527988       0.319654  \n",
       "25240      2.38      2.02  True  0.555823       0.224697  \n",
       "25242      6.36      0.53  True  0.812156       0.158561  \n",
       "25246      1.80      3.60  True  0.543791       0.326400  \n",
       "25248     11.51      0.28  True  0.824997       0.043747  \n",
       "25249      2.43      2.04  True  0.558717       0.229770  \n",
       "25252      2.04      2.47  True  0.548395       0.260211  \n",
       "25255      2.11     -1.00  True  0.558454       0.271921  \n",
       "25259     12.70      0.23  True  0.830143       0.017135  \n",
       "25264      2.31     -1.00  True  0.833293       0.501068  \n",
       "25266      4.69      0.78  True  0.631421       0.069624  \n",
       "25267      3.69     -1.00  True  0.544879       0.021319  \n",
       "25268      2.15     -1.00  True  0.802106       0.481593  \n",
       "25273       NaN      0.17  True  0.800816      -0.053885  \n",
       "\n",
       "[761 rows x 194 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bet_current_season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot a leqrning curve\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='f1')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "plot_learning_curve(clf, 'Learning Curve', X, y, cv=4, n_jobs=-1).show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "prob_pos = y_probs[:,1]\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=20)\n",
    "\n",
    "ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",label=\"%s\" % ('Classifier', ))\n",
    "\n",
    "ax2.hist(prob_iso_pos, range=(0, 1), bins=10, label='Classifier',histtype=\"step\", lw=2)\n",
    "\n",
    "ax1.set_ylabel(\"Fraction of positives\")\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.set_title('Calibration plots  (reliability curve)')\n",
    "\n",
    "ax2.set_xlabel(\"Mean predicted value\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "iso = CalibratedClassifierCV(clf, cv=4, method='isotonic')\n",
    "iso.fit(X_train, y_train)\n",
    "y_iso_probs = iso.predict_proba(X_test)\n",
    "prob_iso_pos = y_iso_probs[:,1]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_iso_pos, n_bins=20)\n",
    "\n",
    "ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",label=\"%s\" % ('Classifier', ))\n",
    "\n",
    "ax2.hist(prob_pos, range=(0, 1), bins=10, label='Classifier',histtype=\"step\", lw=2)\n",
    "\n",
    "ax1.set_ylabel(\"Fraction of positives\")\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.set_title('Calibration plots  (reliability curve)')\n",
    "\n",
    "ax2.set_xlabel(\"Mean predicted value\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_iso_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
