{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "import matplotlib.pyplot as plt # plotting library\n",
    "import seaborn as sns # visualization library based on matplotlib\n",
    "from IPython.display import display # Manage multiple output per cell\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define target\n",
    "target = 'INFO_FTR_H'\n",
    "start_date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_features = ['A_MEANS_FIVE_FTHG', 'A_MEANS_FIVE_HS', 'H_MEANS_FIVE_AF',\n",
    "       'H_MEANS_FIVE_HF', 'A_MEANS_THREE_AF', 'A_MEANS_THREE_HF',\n",
    "       'A_STD_FIVE_AC', 'A_STD_FIVE_AY', 'DNN_adam_bf', 'DNN_sgd_all',\n",
    "       'DNN_sgd_bf', 'XGBoost_all', 'SVM', 'RF_all', 'RF_bf', 'NB',\n",
    "       'MLP', 'LDA', 'KNN', 'ExraTree', 'Bagging_min_sample_all',\n",
    "       'Bagging_min_sample_bf', 'GB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "# normal\n",
    "#train_df = pd.DataFrame.from_csv('./report/layer2-train-INFO_FTR_H-2017-07-08-19-28.csv')\n",
    "#test_df = pd.DataFrame.from_csv('./report/layer2-test-INFO_FTR_H-2017-07-08-19-28.csv')\n",
    "# isotonic\n",
    "#train_df = pd.DataFrame.from_csv('./report/layer2-train-INFO_FTR_H-2017-07-10-10-32.csv')\n",
    "#test_df = pd.DataFrame.from_csv('./report/layer2-test-INFO_FTR_H-2017-07-10-10-32.csv')\n",
    "# sigmoid\n",
    "#train_df = pd.DataFrame.from_csv('./report/layer2-train-INFO_FTR_H-2017-07-10-12-44.csv')\n",
    "#test_df = pd.DataFrame.from_csv('./report/layer2-test-INFO_FTR_H-2017-07-10-12-44.csv')\n",
    "# calibrated TODO: Change after applying sigmoid to DNN_adam_all instead of no\n",
    "train_df = pd.DataFrame.from_csv('./report/layer2-train-INFO_FTR_H-2017-07-10-15-56.csv')\n",
    "test_df = pd.DataFrame.from_csv('./report/layer2-test-INFO_FTR_H-2017-07-10-15-56.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_df =  train_df.drop(target, 1)\n",
    "X_test_df = test_df.drop(target, 1)\n",
    "y_train_df = train_df[target].astype('bool_')\n",
    "y_test_df= test_df[target].astype('bool_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# import xgboost as xgb\n",
    "# # Find the best n_estimator\n",
    "# import xgboost as xgb\n",
    "# xgb1 = XGBClassifier(\n",
    "#  learning_rate =0.01,\n",
    "#  n_estimators=1000,\n",
    "#  max_depth=3,\n",
    "#  min_child_weight=5,\n",
    "#  gamma=0,\n",
    "#  subsample=0.8,\n",
    "#  colsample_bytree=0.8,\n",
    "#  objective= 'binary:logistic',\n",
    "#  nthread=4,\n",
    "#  scale_pos_weight=1,\n",
    "#  seed=27)\n",
    "# xgtrain = xgb.DMatrix(X_train_df, label=y_train_df)\n",
    "# cvresult = xgb.cv(xgb1.get_xgb_params(), xgtrain, num_boost_round=1000, nfold=8,\n",
    "#             metrics='logloss', early_stopping_rounds=50, verbose_eval=True)\n",
    "# cvresult.shape[0]\n",
    "# Result 0.1 = 28\n",
    "# Result 0.01 = 283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import model\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "classifier = XGBClassifier(nthread=4, seed=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n"
     ]
    }
   ],
   "source": [
    "# Applying Grid Search to find the best hyper-parameters for our Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.classification import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "parameters = [{\n",
    "    'learning_rate': [0.01],\n",
    "    'n_estimators': [283],\n",
    "    'max_depth': [1, 3, 5, 10],\n",
    "    'min_child_weight': [1, 3, 5, 8],\n",
    "    'gamma': [0],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'objective': ['binary:logistic'],\n",
    "    'scale_pos_weight': [1]\n",
    "}]\n",
    "parameters = [{\n",
    "    'learning_rate': [0.01],\n",
    "    'n_estimators': [283],\n",
    "    'max_depth': [5],\n",
    "    'min_child_weight': [1],\n",
    "    'gamma': [0],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'objective': ['binary:logistic'],\n",
    "    'scale_pos_weight': [1]\n",
    "}]\n",
    "# 'max_depth': 1,'min_child_weight': 1\n",
    "grid_search = GridSearchCV(estimator=classifier,\n",
    "                           param_grid=parameters,\n",
    "                           #scoring=make_scorer(log_loss, greater_is_better=False),\n",
    "                           scoring='roc_auc',\n",
    "                           cv=8,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "grid_search.fit(X_train_df, y_train_df)\n",
    "\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best score calculated with the GridSearchCV\n",
    "best_score = grid_search.best_score_\n",
    "display(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best hyper-parameter calculated with the GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get all results of Grid Search\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results.to_csv('./tuning/layer2-XGBoost-'+start_date+'.csv')\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new classifier using the best parameters found by the grid search\n",
    "clf = XGBClassifier(\n",
    "    nthread=4, \n",
    "    seed=15,\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_child_weight=best_params['min_child_weight'],\n",
    "    gamma=best_params['gamma'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree'],\n",
    "    objective=best_params['objective'],\n",
    "    scale_pos_weight=best_params['scale_pos_weight']\n",
    ")\n",
    "clf.fit(X_train_df, y_train_df, eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict target values\n",
    "y_pred = clf.predict(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_probs = clf.predict_proba(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cross-entropy score\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test_df, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute precision, recall, F-measure and support\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test_df, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test_df, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion Matrix\n",
    "df_confusion = pd.crosstab(y_test_df, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot a ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test_df, y_probs[:, 1].ravel())\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('ROC')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.4f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot\n",
    "# fit model no training data\n",
    "model = XGBClassifier(\n",
    " learning_rate =0.01,\n",
    " n_estimators=283,\n",
    " max_depth=3,\n",
    " min_child_weight=5,\n",
    " gamma=0,\n",
    " subsample=0.9,\n",
    " colsample_bytree=0.9,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=0)\n",
    "model.fit(X_train_df, y_train_df, eval_metric='logloss')\n",
    "# plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "plot_importance(model, ax=ax)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit model using each importance as a threshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from numpy import sort\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics.classification import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "seed=15\n",
    "thresholds = np.unique(sort(model.feature_importances_))\n",
    "df_fs = pd.DataFrame(columns=('Thresh', 'n', 'll_mean', 'll_std'))\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(X_train_df)\n",
    "    # define model\n",
    "    #selection_model = RandomForestClassifier(random_state=0, n_jobs=-1,criterion='entropy', max_depth=None, max_features='sqrt', min_samples_leaf=1, min_samples_split=2, n_estimators=360)\n",
    "    selection_model = LogisticRegression(random_state=0,n_jobs=-1)\n",
    "    #selection_model = GaussianNB()\n",
    "    #selection_model = MLPClassifier(random_state=0,activation='identity', alpha=3, hidden_layer_sizes=(10,), max_iter=200, solver= 'adam')\n",
    "    #selection_model= SVC(random_state=0,C=0.02,kernel='linear',gamma=1,probability=True)\n",
    "    #selection_model = CalibratedClassifierCV(ExtraTreesClassifier(random_state=0,n_jobs=-1,criterion='entropy',max_depth=30,max_features=None,min_samples_leaf=1,min_samples_split=16,n_estimators=350), cv=4, method='isotonic')\n",
    "    # eval model\n",
    "    logloss = cross_val_score(selection_model, select_X_train, y_train_df, cv=8, scoring='roc_auc', n_jobs=-1)\n",
    "    ll_mean = logloss.mean()\n",
    "    ll_std = logloss.std()\n",
    "    df2 = pd.DataFrame([[thresh, select_X_train.shape[1], ll_mean, ll_std]], columns=('Thresh', 'n', 'll_mean', 'll_std'))\n",
    "    df_fs = df_fs.append(df2)\n",
    "    print(\"Thresh=%.5f, n=%d, AUC mean: %.5f, AUC std: %.5f\" % (thresh, select_X_train.shape[1], ll_mean, ll_std))\n",
    "df_fs.to_csv('./feature_selection/'+start_date+'-layer2-Feature_selection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "selection = SelectFromModel(model, threshold=0.0, prefit=True)\n",
    "feature_idx = selection.get_support()\n",
    "feature_name = pd.get_dummies(X_train_df).columns[feature_idx]\n",
    "feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BEST ARE:\n",
    "# Logsitic Regression: Thresh=0.00154, n=111, AUC mean: 0.65870, AUC std: 0.01013\n",
    "# Naive Bayes: Thresh=0.00768, n=36, AUC mean: 0.65462, AUC std: 0.01036\n",
    "# MLP: Thresh=0.02971, n=8, AUC mean: 0.65596, AUC std: 0.01026\n",
    "# Extra Tree: Thresh=0.00922, n=28, AUC mean: 0.65177, AUC std: 0.01343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for feature in feature_name:\n",
    "    print \"'\"+feature+\"',\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
