{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearning with Kera FTR HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the library\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "import matplotlib.pyplot as plt # plotting library\n",
    "from IPython.display import display # Manage multiple output per cell\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features = [\"A_MEANS_FIVE_AC\",\"A_MEANS_FIVE_AF\",\"A_MEANS_FIVE_AR\",\"A_MEANS_FIVE_AS\",\"A_MEANS_FIVE_AST\",\"A_MEANS_FIVE_AY\",\"A_MEANS_FIVE_FTAG\",\"A_MEANS_FIVE_FTHG\",\"A_MEANS_FIVE_FTR_A\",\"A_MEANS_FIVE_FTR_D\",\"A_MEANS_FIVE_FTR_H\",\"A_MEANS_FIVE_HC\",\"A_MEANS_FIVE_HF\",\"A_MEANS_FIVE_HR\",\"A_MEANS_FIVE_HS\",\"A_MEANS_FIVE_HST\",\"A_MEANS_FIVE_HTAG\",\"A_MEANS_FIVE_HTHG\",\"A_MEANS_FIVE_HTR_A\",\"A_MEANS_FIVE_HTR_D\",\"A_MEANS_FIVE_HTR_H\",\"A_MEANS_FIVE_HY\",\"H_MEANS_FIVE_AC\",\"H_MEANS_FIVE_AF\",\"H_MEANS_FIVE_AR\",\"H_MEANS_FIVE_AS\",\"H_MEANS_FIVE_AST\",\"H_MEANS_FIVE_AY\",\"H_MEANS_FIVE_FTAG\",\"H_MEANS_FIVE_FTHG\",\"H_MEANS_FIVE_FTR_A\",\"H_MEANS_FIVE_FTR_D\",\"H_MEANS_FIVE_FTR_H\",\"H_MEANS_FIVE_HC\",\"H_MEANS_FIVE_HF\",\"H_MEANS_FIVE_HR\",\"H_MEANS_FIVE_HS\",\"H_MEANS_FIVE_HST\",\"H_MEANS_FIVE_HTAG\",\"H_MEANS_FIVE_HTHG\",\"H_MEANS_FIVE_HTR_A\",\"H_MEANS_FIVE_HTR_D\",\"H_MEANS_FIVE_HTR_H\",\"H_MEANS_FIVE_HY\",\"A_MEANS_THREE_AC\",\"A_MEANS_THREE_AF\",\"A_MEANS_THREE_AR\",\"A_MEANS_THREE_AS\",\"A_MEANS_THREE_AST\",\"A_MEANS_THREE_AY\",\"A_MEANS_THREE_FTAG\",\"A_MEANS_THREE_FTHG\",\"A_MEANS_THREE_FTR_A\",\"A_MEANS_THREE_FTR_D\",\"A_MEANS_THREE_FTR_H\",\"A_MEANS_THREE_HC\",\"A_MEANS_THREE_HF\",\"A_MEANS_THREE_HR\",\"A_MEANS_THREE_HS\",\"A_MEANS_THREE_HST\",\"A_MEANS_THREE_HTAG\",\"A_MEANS_THREE_HTHG\",\"A_MEANS_THREE_HTR_A\",\"A_MEANS_THREE_HTR_D\",\"A_MEANS_THREE_HTR_H\",\"A_MEANS_THREE_HY\",\"H_MEANS_THREE_AC\",\"H_MEANS_THREE_AF\",\"H_MEANS_THREE_AR\",\"H_MEANS_THREE_AS\",\"H_MEANS_THREE_AST\",\"H_MEANS_THREE_AY\",\"H_MEANS_THREE_FTAG\",\"H_MEANS_THREE_FTHG\",\"H_MEANS_THREE_FTR_A\",\"H_MEANS_THREE_FTR_D\",\"H_MEANS_THREE_FTR_H\",\"H_MEANS_THREE_HC\",\"H_MEANS_THREE_HF\",\"H_MEANS_THREE_HR\",\"H_MEANS_THREE_HS\",\"H_MEANS_THREE_HST\",\"H_MEANS_THREE_HTAG\",\"H_MEANS_THREE_HTHG\",\"H_MEANS_THREE_HTR_A\",\"H_MEANS_THREE_HTR_D\",\"H_MEANS_THREE_HTR_H\",\"H_MEANS_THREE_HY\",\"A_STD_FIVE_AC\",\"A_STD_FIVE_AF\",\"A_STD_FIVE_AR\",\"A_STD_FIVE_AS\",\"A_STD_FIVE_AST\",\"A_STD_FIVE_AY\",\"A_STD_FIVE_FTAG\",\"A_STD_FIVE_FTHG\",\"A_STD_FIVE_FTR_A\",\"A_STD_FIVE_FTR_D\",\"A_STD_FIVE_FTR_H\",\"A_STD_FIVE_HC\",\"A_STD_FIVE_HF\",\"A_STD_FIVE_HR\",\"A_STD_FIVE_HS\",\"A_STD_FIVE_HST\",\"A_STD_FIVE_HTAG\",\"A_STD_FIVE_HTHG\",\"A_STD_FIVE_HTR_A\",\"A_STD_FIVE_HTR_D\",\"A_STD_FIVE_HTR_H\",\"A_STD_FIVE_HY\",\"H_STD_FIVE_AC\",\"H_STD_FIVE_AF\",\"H_STD_FIVE_AR\",\"H_STD_FIVE_AS\",\"H_STD_FIVE_AST\",\"H_STD_FIVE_AY\",\"H_STD_FIVE_FTAG\",\"H_STD_FIVE_FTHG\",\"H_STD_FIVE_FTR_A\",\"H_STD_FIVE_FTR_D\",\"H_STD_FIVE_FTR_H\",\"H_STD_FIVE_HC\",\"H_STD_FIVE_HF\",\"H_STD_FIVE_HR\",\"H_STD_FIVE_HS\",\"H_STD_FIVE_HST\",\"H_STD_FIVE_HTAG\",\"H_STD_FIVE_HTHG\",\"H_STD_FIVE_HTR_A\",\"H_STD_FIVE_HTR_D\",\"H_STD_FIVE_HTR_H\",\"H_STD_FIVE_HY\",\"A_STD_THREE_AC\",\"A_STD_THREE_AF\",\"A_STD_THREE_AR\",\"A_STD_THREE_AS\",\"A_STD_THREE_AST\",\"A_STD_THREE_AY\",\"A_STD_THREE_FTAG\",\"A_STD_THREE_FTHG\",\"A_STD_THREE_FTR_A\",\"A_STD_THREE_FTR_D\",\"A_STD_THREE_FTR_H\",\"A_STD_THREE_HC\",\"A_STD_THREE_HF\",\"A_STD_THREE_HR\",\"A_STD_THREE_HS\",\"A_STD_THREE_HST\",\"A_STD_THREE_HTAG\",\"A_STD_THREE_HTHG\",\"A_STD_THREE_HTR_A\",\"A_STD_THREE_HTR_D\",\"A_STD_THREE_HTR_H\",\"A_STD_THREE_HY\",\"H_STD_THREE_AC\",\"H_STD_THREE_AF\",\"H_STD_THREE_AR\",\"H_STD_THREE_AS\",\"H_STD_THREE_AST\",\"H_STD_THREE_AY\",\"H_STD_THREE_FTAG\",\"H_STD_THREE_FTHG\",\"H_STD_THREE_FTR_A\",\"H_STD_THREE_FTR_D\",\"H_STD_THREE_FTR_H\",\"H_STD_THREE_HC\",\"H_STD_THREE_HF\",\"H_STD_THREE_HR\",\"H_STD_THREE_HS\",\"H_STD_THREE_HST\",\"H_STD_THREE_HTAG\",\"H_STD_THREE_HTHG\",\"H_STD_THREE_HTR_A\",\"H_STD_THREE_HTR_D\",\"H_STD_THREE_HTR_H\",\"H_STD_THREE_HY\",\"INFO_Div\"]\n",
    "best_features_adam_38 = ['A_MEANS_FIVE_AC', 'A_MEANS_FIVE_AS', 'A_MEANS_FIVE_AST',\n",
    "       'A_MEANS_FIVE_FTAG', 'A_MEANS_FIVE_FTHG', 'A_MEANS_FIVE_FTR_H',\n",
    "       'A_MEANS_FIVE_HC', 'A_MEANS_FIVE_HF', 'A_MEANS_FIVE_HS',\n",
    "       'A_MEANS_FIVE_HST', 'A_MEANS_FIVE_HTR_A', 'A_MEANS_FIVE_HY',\n",
    "       'H_MEANS_FIVE_AC', 'H_MEANS_FIVE_AS', 'H_MEANS_FIVE_AST',\n",
    "       'H_MEANS_FIVE_AY', 'H_MEANS_FIVE_FTAG', 'H_MEANS_FIVE_FTHG',\n",
    "       'H_MEANS_FIVE_FTR_A', 'H_MEANS_FIVE_FTR_H', 'H_MEANS_FIVE_HC',\n",
    "       'H_MEANS_FIVE_HS', 'H_MEANS_FIVE_HST', 'H_MEANS_FIVE_HTR_H',\n",
    "       'A_MEANS_THREE_AC', 'A_MEANS_THREE_AS', 'A_MEANS_THREE_FTHG',\n",
    "       'A_MEANS_THREE_HF', 'A_MEANS_THREE_HS', 'H_MEANS_THREE_AS',\n",
    "       'H_MEANS_THREE_HC', 'A_STD_FIVE_AST', 'A_STD_FIVE_HF',\n",
    "       'H_STD_FIVE_AF', 'H_STD_FIVE_AS', 'H_STD_FIVE_HC', 'H_STD_FIVE_HF',\n",
    "       'H_STD_FIVE_HST']\n",
    "best_features_SGD_56 = ['A_MEANS_FIVE_AC', 'A_MEANS_FIVE_AS', 'A_MEANS_FIVE_AST',\n",
    "       'A_MEANS_FIVE_FTAG', 'A_MEANS_FIVE_FTHG', 'A_MEANS_FIVE_FTR_A',\n",
    "       'A_MEANS_FIVE_FTR_H', 'A_MEANS_FIVE_HC', 'A_MEANS_FIVE_HF',\n",
    "       'A_MEANS_FIVE_HS', 'A_MEANS_FIVE_HST', 'A_MEANS_FIVE_HTHG',\n",
    "       'A_MEANS_FIVE_HTR_A', 'A_MEANS_FIVE_HY', 'H_MEANS_FIVE_AC',\n",
    "       'H_MEANS_FIVE_AS', 'H_MEANS_FIVE_AST', 'H_MEANS_FIVE_AY',\n",
    "       'H_MEANS_FIVE_FTAG', 'H_MEANS_FIVE_FTHG', 'H_MEANS_FIVE_FTR_A',\n",
    "       'H_MEANS_FIVE_FTR_H', 'H_MEANS_FIVE_HC', 'H_MEANS_FIVE_HF',\n",
    "       'H_MEANS_FIVE_HS', 'H_MEANS_FIVE_HST', 'H_MEANS_FIVE_HTAG',\n",
    "       'H_MEANS_FIVE_HTR_H', 'A_MEANS_THREE_AC', 'A_MEANS_THREE_AS',\n",
    "       'A_MEANS_THREE_FTHG', 'A_MEANS_THREE_FTR_A', 'A_MEANS_THREE_HC',\n",
    "       'A_MEANS_THREE_HF', 'A_MEANS_THREE_HS', 'A_MEANS_THREE_HST',\n",
    "       'H_MEANS_THREE_AC', 'H_MEANS_THREE_AS', 'H_MEANS_THREE_FTHG',\n",
    "       'H_MEANS_THREE_HC', 'H_MEANS_THREE_HST', 'H_MEANS_THREE_HTR_H',\n",
    "       'A_STD_FIVE_AF', 'A_STD_FIVE_AST', 'A_STD_FIVE_HC', 'A_STD_FIVE_HF',\n",
    "       'A_STD_FIVE_HS', 'H_STD_FIVE_AF', 'H_STD_FIVE_AS', 'H_STD_FIVE_AST',\n",
    "       'H_STD_FIVE_HC', 'H_STD_FIVE_HF', 'H_STD_FIVE_HST',\n",
    "       'H_STD_FIVE_HTHG', 'H_STD_THREE_AS', 'H_STD_THREE_HST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'DeepLearning_Kera-all_features'\n",
    "target = 'INFO_FTR_A'\n",
    "odd = 'INFO_BbAvA'\n",
    "bet_on = 'A'\n",
    "start_date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DB Sqlite connection\n",
    "import sqlite3\n",
    "db = \"/Users/thibaultclement/Project/ligue1-predict/src/notebook/data/db/soccer_predict.sqlite\"\n",
    "conn = sqlite3.connect(db)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25275, 190)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all prematch data\n",
    "df = pd.read_sql_query(\"SELECT * FROM pre_matchs ORDER BY INFO_Date ASC;\", conn)\n",
    "df = (df[df.columns.drop(['index'])])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18027, 190)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all game between June (include) and October (include)\n",
    "df['INFO_Date'] = pd.to_datetime(df['INFO_Date'])\n",
    "df['INFO_Date'].dt.month\n",
    "df = df[(df['INFO_Date'].dt.month < 6) | (df['INFO_Date'].dt.month > 10)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18027, 190)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a particular league\n",
    "#df = df[(df['INFO_Div'] == 'D1')]\n",
    "#df = df[(df['INFO_Div'] == 'E0')]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.289410328951023"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the average odd\n",
    "df[odd].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18027, 190)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing column where odd is too high or too low\n",
    "# df = df.drop(df[df['INFO_BbAvH'] < 2].index)\n",
    "# df = df.drop(df[df['INFO_BbAvA'] < 2].index)\n",
    "# df = df.drop(df[df['INFO_BbAvH'] > 10].index)\n",
    "# df = df.drop(df[df['INFO_BbAvA'] > 10].index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a INFO_WIN column containing the gain. If bet success it's equal to odd -1, else -1 (loose your bet)\n",
    "df['INFO_WIN'] = df[odd]-1\n",
    "df.loc[df.INFO_FTR != bet_on, 'INFO_WIN'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "H    45.515061\n",
       "A    28.856715\n",
       "D    25.628224\n",
       "Name: INFO_FTR, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Statistic about winners\n",
    "display(plt.show(), 100. * df.INFO_FTR.value_counts() / len(df.INFO_FTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11119542907860423"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much did you win/lost per match if bet on all\n",
    "df.INFO_WIN.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep season 2016/2017 for further test and don't use it for traning\n",
    "import datetime\n",
    "date_start_current_season = datetime.date(2016, 8, 1)\n",
    "df_current_season = df[(df['INFO_Date'] > date_start_current_season)]\n",
    "df = df[(df['INFO_Date'] < date_start_current_season)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2222, 191)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of matches in current season\n",
    "df_current_season.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "features_list = all_features\n",
    "X = pd.get_dummies(df[features_list]).astype(float)\n",
    "y = pd.get_dummies(df)[target].astype('bool_')\n",
    "X_current_season = pd.get_dummies(df_current_season[features_list])\n",
    "y_current_season = pd.get_dummies(df_current_season)[target].astype('bool_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_feature = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler().fit(X)\n",
    "X = sc_X.transform(X)\n",
    "X_current_season = sc_X.transform(X_current_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impute of missing values (NaN) with the mean\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp = imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "X_current_season = imp.transform(X_current_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# create model\n",
    "def create_model(optimizer='adam'):\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(nb_feature, input_dim=nb_feature, kernel_initializer='normal', activation='relu'))\n",
    "  model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "  # Compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "  return model\n",
    "classifier = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   24.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34.32907199859619"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Grid Search to find the best hyper-parameters for our Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.classification import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "parameters = [{\n",
    "    'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "    'epochs': [10, 50, 100]\n",
    "}]\n",
    "# {'batch_size': 100, 'epochs': 10}\n",
    "parameters = [{\n",
    "    'batch_size': [100, 150, 200],\n",
    "    'epochs': [5, 10, 15, 20, 25]\n",
    "}]\n",
    "# {'batch_size': 200, 'epochs': 5}\n",
    "parameters = [{\n",
    "    'batch_size': [200, 250, 300],\n",
    "    'epochs': [3, 5, 7]\n",
    "}]\n",
    "# {'batch_size': 300, 'epochs': 3}\n",
    "parameters = [{\n",
    "    'batch_size': [300, 400, 500],\n",
    "    'epochs': [1, 2, 3, 4]\n",
    "}]\n",
    "# {'batch_size': 500, 'epochs': 3}\n",
    "parameters = [{\n",
    "    'batch_size': [500, 800, 1000],\n",
    "    'epochs': [3]\n",
    "}]\n",
    "# {'batch_size': 1000, 'epochs': 3}\n",
    "parameters = [{\n",
    "    'batch_size': [1000, 2000, 5000],\n",
    "    'epochs': [3]\n",
    "}]\n",
    "# {'batch_size': 1000, 'epochs': 3}\n",
    "parameters = [{\n",
    "    'batch_size': [900, 1000, 1100, 1200, 1300, 1400, 1500],\n",
    "    'epochs': [3]\n",
    "}]\n",
    "# {'batch_size': 1500, 'epochs': 3}\n",
    "parameters = [{\n",
    "    'optimizer': ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "}]\n",
    "# {'optimizer': 'SGD'}\n",
    "parameters = [{\n",
    "    'optimizer': ['SGD'],\n",
    "    'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "    'epochs': [10, 50, 100]\n",
    "}]\n",
    "# {'batch_size': 80, 'epochs': 10, 'optimizer': 'SGD'}\n",
    "parameters = [{\n",
    "    'optimizer': ['SGD'],\n",
    "    'batch_size': [80],\n",
    "    'epochs': [3, 5, 7, 10]\n",
    "}]\n",
    "# {'batch_size': 80, 'epochs': 5, 'optimizer': 'SGD'} -> -13.715096670302973\n",
    "parameters = [{\n",
    "    'optimizer': ['Adam'],\n",
    "    'batch_size': [1500],\n",
    "    'epochs': [3]\n",
    "}]\n",
    "# -13.711978854182211\n",
    "# FINAL\n",
    "parameters = [{\n",
    "    'optimizer': ['SGD'],\n",
    "    'batch_size': [80],\n",
    "    'epochs': [5]\n",
    "}]\n",
    "# -13.715096670302973\n",
    "# FINAL\n",
    "parameters = [{\n",
    "    'optimizer': ['Adam'],\n",
    "    'batch_size': [30, 100, 300, 1000, 3000],\n",
    "    'epochs': [3, 10, 30]\n",
    "}]\n",
    "# {'batch_size': 3000, 'epochs': 3, 'optimizer': 'Adam'}\n",
    "parameters = [{\n",
    "    'optimizer': ['Adam'],\n",
    "    'batch_size': [2000, 3000, 4000],\n",
    "    'epochs': [2, 3, 4, 5]\n",
    "}]\n",
    "# {'batch_size': 3000, 'epochs': 3, 'optimizer': 'Adam'}\n",
    "parameters = [{\n",
    "    'optimizer': ['Adam'],\n",
    "    'batch_size': [1800, 2000, 2200, 2400],\n",
    "    'epochs': [3]\n",
    "}]\n",
    "# {'batch_size': 2200, 'epochs': 3, 'optimizer': 'Adam'}\n",
    "parameters = [{\n",
    "    'optimizer': ['Adam'],\n",
    "    'batch_size': [2100, 2200, 2300],\n",
    "    'epochs': [3]\n",
    "}]\n",
    "# {'batch_size': 2300, 'epochs': 3, 'optimizer': 'Adam'}\n",
    "parameters = [{\n",
    "    'optimizer': ['Adam'],\n",
    "    'batch_size': [2250, 2300, 2350],\n",
    "    'epochs': [3]\n",
    "}]\n",
    "# {'batch_size': 2350, 'epochs': 3, 'optimizer': 'Adam'}\n",
    "parameters = [{\n",
    "    'optimizer': ['Adam'],\n",
    "    'batch_size': [2350],\n",
    "    'epochs': [3]\n",
    "}]\n",
    "grid_search = GridSearchCV(estimator=classifier,\n",
    "                           param_grid=parameters,\n",
    "                           #scoring=make_scorer(log_loss, greater_is_better=False),\n",
    "                           scoring='roc_auc',\n",
    "                           cv=8,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62714979029757456"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract best score calculated with the GridSearchCV\n",
    "best_score = grid_search.best_score_\n",
    "display(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 2350, 'epochs': 3, 'optimizer': 'Adam'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract best hyper-parameter calculated with the GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_epochs</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.150695</td>\n",
       "      <td>0.44306</td>\n",
       "      <td>0.62715</td>\n",
       "      <td>0.650964</td>\n",
       "      <td>2350</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>{u'epochs': 3, u'optimizer': u'Adam', u'batch_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.647528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612165</td>\n",
       "      <td>0.650111</td>\n",
       "      <td>0.641434</td>\n",
       "      <td>0.647562</td>\n",
       "      <td>0.631417</td>\n",
       "      <td>0.647268</td>\n",
       "      <td>1.495827</td>\n",
       "      <td>0.173024</td>\n",
       "      <td>0.015477</td>\n",
       "      <td>0.002631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0      11.150695          0.44306          0.62715          0.650964   \n",
       "\n",
       "  param_batch_size param_epochs param_optimizer  \\\n",
       "0             2350            3            Adam   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0  {u'epochs': 3, u'optimizer': u'Adam', u'batch_...                1   \n",
       "\n",
       "   split0_test_score       ...         split5_test_score  split5_train_score  \\\n",
       "0           0.647528       ...                  0.612165            0.650111   \n",
       "\n",
       "   split6_test_score  split6_train_score  split7_test_score  \\\n",
       "0           0.641434            0.647562           0.631417   \n",
       "\n",
       "   split7_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "0            0.647268      1.495827        0.173024        0.015477   \n",
       "\n",
       "   std_train_score  \n",
       "0         0.002631  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all results of Grid Search\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results.to_csv('./tuning/'+model_name+'-'+target+'_'+start_date+'.csv')\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12e9c0350>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a new classifier using the best parameters found by the grid search\n",
    "clf = KerasClassifier(build_fn=create_model, \n",
    "                      verbose=0,\n",
    "                      optimizer=best_params['optimizer'],\n",
    "                      batch_size=best_params['batch_size'],\n",
    "                      epochs=best_params['epochs']\n",
    ")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict target values\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_probs = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.007657096353238"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cross-entropy score\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.625, 0.04642857142857143, 0.08643617021276595, None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute precision, recall, F-measure and support\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>3303</td>\n",
       "      <td>39</td>\n",
       "      <td>3342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1335</td>\n",
       "      <td>65</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4638</td>\n",
       "      <td>104</td>\n",
       "      <td>4742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True   All\n",
       "Actual                      \n",
       "False       3303    39  3342\n",
       "True        1335    65  1400\n",
       "All         4638   104  4742"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the confusion Matrix\n",
    "df_confusion = pd.crosstab(y_test, y_pred[:,0], rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113931810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test.ravel(), y_probs[:, 1].ravel())\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('ROC')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.4f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL DATASET TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply on all the dataset\n",
    "X_pred = clf.predict(X)\n",
    "df['pred'] = X_pred\n",
    "df_bet_all_seasons = df.drop(df[df.pred == 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(349, 192)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many bet I did\n",
    "df_bet_all_seasons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "A    65.042980\n",
       "D    18.338109\n",
       "H    16.618911\n",
       "Name: INFO_FTR, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What will be the real result of games I bet on\n",
    "display(plt.show(), 100. * df_bet_all_seasons.INFO_FTR.value_counts() / len(df_bet_all_seasons.INFO_FTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.7464751833083287"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cross-entropy score\n",
    "log_loss(y, X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.65042979942693413, 0.049726177437020812, 0.092389092389092389, None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score for all dataset\n",
    "precision_recall_fscore_support(y, X_pred, average='binary') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>11118</td>\n",
       "      <td>122</td>\n",
       "      <td>11240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>4338</td>\n",
       "      <td>227</td>\n",
       "      <td>4565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>15456</td>\n",
       "      <td>349</td>\n",
       "      <td>15805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True    All\n",
       "Actual                       \n",
       "False      11118   122  11240\n",
       "True        4338   227   4565\n",
       "All        15456   349  15805"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the confusion Matrix\n",
    "df_confusion = pd.crosstab(y, X_pred[:,0], rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07429799426934097"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bet_all_seasons.INFO_WIN.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEASON 2016/2017 TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply on current season that is not use for train and test set\n",
    "X_pred_current_season = clf.predict(X_current_season)\n",
    "X_prob_current_season = clf.predict_proba(X_current_season)\n",
    "df_current_season['pred'] = X_pred_current_season\n",
    "df_current_season['prob'] = X_prob_current_season[:,1:]\n",
    "df_current_season['prob_less_bet'] = df_current_season['prob'] - df_current_season[odd].apply(lambda x: 1/x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove games I didn't bet on\n",
    "df_bet_current_season = df_current_season\n",
    "df_bet_current_season = df_bet_current_season.drop(df_bet_current_season[df_bet_current_season.pred == 0].index)\n",
    "#df_bet_current_season = df_bet_current_season[df_bet_current_season.prob > 0]\n",
    "#df_bet_current_season = df_bet_current_season[df_bet_current_season.prob_less_bet > 0]\n",
    "#df_bet_current_season = df_bet_current_season[df_bet_current_season[odd] > 2]\n",
    "#df_bet_current_season = df_bet_current_season[df_bet_current_season[odd] < 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313, 194)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many bet I did\n",
    "df_bet_current_season.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2222, 1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many matches was play\n",
    "X_pred_current_season.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "H    46.325879\n",
       "A    28.753994\n",
       "D    24.920128\n",
       "Name: INFO_FTR, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What will be the real result of games I bet on\n",
    "display(plt.show(), 100. * df_bet_current_season.INFO_FTR.value_counts() / len(df_bet_current_season.INFO_FTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.5129418275441502"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cross-entropy score\n",
    "log_loss(y_current_season, X_pred_current_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6404494382022472, 0.089481946624803771, 0.15702479338842976, None)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score for this current season\n",
    "precision_recall_fscore_support(y_current_season, X_pred_current_season, average='binary') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>1553</td>\n",
       "      <td>32</td>\n",
       "      <td>1585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>580</td>\n",
       "      <td>57</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>2133</td>\n",
       "      <td>89</td>\n",
       "      <td>2222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  False  True   All\n",
       "Actual                      \n",
       "False       1553    32  1585\n",
       "True         580    57   637\n",
       "All         2133    89  2222"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the confusion Matrix\n",
    "df_confusion = pd.crosstab(y_current_season, X_pred_current_season[:,0], rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11872204472843453"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What I win/lost on each match\n",
    "df_bet_current_season.INFO_WIN.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_MEANS_FIVE_AC</th>\n",
       "      <th>A_MEANS_FIVE_AF</th>\n",
       "      <th>A_MEANS_FIVE_AR</th>\n",
       "      <th>A_MEANS_FIVE_AS</th>\n",
       "      <th>A_MEANS_FIVE_AST</th>\n",
       "      <th>A_MEANS_FIVE_AY</th>\n",
       "      <th>A_MEANS_FIVE_FTAG</th>\n",
       "      <th>A_MEANS_FIVE_FTHG</th>\n",
       "      <th>A_MEANS_FIVE_FTR_A</th>\n",
       "      <th>A_MEANS_FIVE_FTR_D</th>\n",
       "      <th>...</th>\n",
       "      <th>INFO_FTR</th>\n",
       "      <th>INFO_HTR</th>\n",
       "      <th>INFO_HomeTeam</th>\n",
       "      <th>INFO_PSA</th>\n",
       "      <th>INFO_PSD</th>\n",
       "      <th>INFO_PSH</th>\n",
       "      <th>INFO_WIN</th>\n",
       "      <th>pred</th>\n",
       "      <th>prob</th>\n",
       "      <th>prob_less_bet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23056</th>\n",
       "      <td>6.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Brentford</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.55</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.07</td>\n",
       "      <td>False</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>0.066967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23064</th>\n",
       "      <td>2.6</td>\n",
       "      <td>16.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Ingolstadt</td>\n",
       "      <td>3.81</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.64</td>\n",
       "      <td>False</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.035275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23070</th>\n",
       "      <td>4.4</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>4.13</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.261110</td>\n",
       "      <td>0.002712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23073</th>\n",
       "      <td>4.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Burton</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.55</td>\n",
       "      <td>2.21</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.353792</td>\n",
       "      <td>0.047043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23077</th>\n",
       "      <td>4.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>H</td>\n",
       "      <td>Nott'm Forest</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.48</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.330281</td>\n",
       "      <td>0.014824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23080</th>\n",
       "      <td>4.6</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Wigan</td>\n",
       "      <td>3.09</td>\n",
       "      <td>3.23</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.98</td>\n",
       "      <td>False</td>\n",
       "      <td>0.364211</td>\n",
       "      <td>0.028640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23091</th>\n",
       "      <td>6.2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>9.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Las Palmas</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.71</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.351427</td>\n",
       "      <td>0.093695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23097</th>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Kilmarnock</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.75</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.408364</td>\n",
       "      <td>0.019259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23115</th>\n",
       "      <td>4.4</td>\n",
       "      <td>17.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Sassuolo</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.24</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.401712</td>\n",
       "      <td>0.068379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23123</th>\n",
       "      <td>4.2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>8.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Bury</td>\n",
       "      <td>3.28</td>\n",
       "      <td>3.46</td>\n",
       "      <td>2.32</td>\n",
       "      <td>2.10</td>\n",
       "      <td>False</td>\n",
       "      <td>0.480311</td>\n",
       "      <td>0.157731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23125</th>\n",
       "      <td>8.6</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>A</td>\n",
       "      <td>Gillingham</td>\n",
       "      <td>2.81</td>\n",
       "      <td>3.29</td>\n",
       "      <td>2.74</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.480196</td>\n",
       "      <td>0.101408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23126</th>\n",
       "      <td>6.6</td>\n",
       "      <td>13.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.6</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Millwall</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.61</td>\n",
       "      <td>2.26</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.436138</td>\n",
       "      <td>0.120680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23128</th>\n",
       "      <td>6.2</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Oldham</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.36</td>\n",
       "      <td>2.96</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420252</td>\n",
       "      <td>0.018645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23130</th>\n",
       "      <td>7.0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Shrewsbury</td>\n",
       "      <td>2.03</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.76</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.617744</td>\n",
       "      <td>0.120232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23131</th>\n",
       "      <td>6.6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Swindon</td>\n",
       "      <td>2.23</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.45</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.537559</td>\n",
       "      <td>0.074596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23134</th>\n",
       "      <td>5.6</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>H</td>\n",
       "      <td>Leverkusen</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.43</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.13</td>\n",
       "      <td>False</td>\n",
       "      <td>0.406818</td>\n",
       "      <td>0.087329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23145</th>\n",
       "      <td>5.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Wolfsburg</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.44</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.88</td>\n",
       "      <td>False</td>\n",
       "      <td>0.349191</td>\n",
       "      <td>0.001969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23159</th>\n",
       "      <td>4.2</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.32</td>\n",
       "      <td>2.35</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.358623</td>\n",
       "      <td>0.038110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23160</th>\n",
       "      <td>5.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>17.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Ipswich</td>\n",
       "      <td>3.98</td>\n",
       "      <td>3.53</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.71</td>\n",
       "      <td>False</td>\n",
       "      <td>0.367904</td>\n",
       "      <td>0.098362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23163</th>\n",
       "      <td>6.4</td>\n",
       "      <td>10.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Reading</td>\n",
       "      <td>3.96</td>\n",
       "      <td>3.51</td>\n",
       "      <td>2.05</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.295379</td>\n",
       "      <td>0.032221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23164</th>\n",
       "      <td>2.8</td>\n",
       "      <td>16.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>AFC Wimbledon</td>\n",
       "      <td>4.08</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.334551</td>\n",
       "      <td>0.065010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23165</th>\n",
       "      <td>5.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Bolton</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.43</td>\n",
       "      <td>2.39</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.329141</td>\n",
       "      <td>0.002343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23169</th>\n",
       "      <td>5.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Northampton</td>\n",
       "      <td>2.87</td>\n",
       "      <td>3.63</td>\n",
       "      <td>2.49</td>\n",
       "      <td>1.79</td>\n",
       "      <td>False</td>\n",
       "      <td>0.369547</td>\n",
       "      <td>0.011124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23170</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Oxford</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.54</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.447769</td>\n",
       "      <td>0.152784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23175</th>\n",
       "      <td>5.6</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>9.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Walsall</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.66</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.14</td>\n",
       "      <td>False</td>\n",
       "      <td>0.335420</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23193</th>\n",
       "      <td>4.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>H</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1.56</td>\n",
       "      <td>False</td>\n",
       "      <td>0.441418</td>\n",
       "      <td>0.050793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23198</th>\n",
       "      <td>3.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>St Etienne</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.22</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.63</td>\n",
       "      <td>False</td>\n",
       "      <td>0.433261</td>\n",
       "      <td>0.157778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23202</th>\n",
       "      <td>4.8</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Empoli</td>\n",
       "      <td>2.15</td>\n",
       "      <td>3.37</td>\n",
       "      <td>3.87</td>\n",
       "      <td>1.08</td>\n",
       "      <td>False</td>\n",
       "      <td>0.494870</td>\n",
       "      <td>0.014101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23206</th>\n",
       "      <td>2.2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>Alaves</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3.21</td>\n",
       "      <td>2.54</td>\n",
       "      <td>2.06</td>\n",
       "      <td>False</td>\n",
       "      <td>0.335994</td>\n",
       "      <td>0.009197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23217</th>\n",
       "      <td>7.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Millwall</td>\n",
       "      <td>3.91</td>\n",
       "      <td>3.41</td>\n",
       "      <td>2.10</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.312022</td>\n",
       "      <td>0.038049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25009</th>\n",
       "      <td>3.6</td>\n",
       "      <td>10.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.8</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Stoke</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.323418</td>\n",
       "      <td>0.046409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25013</th>\n",
       "      <td>3.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.48</td>\n",
       "      <td>2.62</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.365649</td>\n",
       "      <td>0.000685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25014</th>\n",
       "      <td>4.8</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Blackburn</td>\n",
       "      <td>3.68</td>\n",
       "      <td>3.38</td>\n",
       "      <td>2.17</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.317039</td>\n",
       "      <td>0.030506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25019</th>\n",
       "      <td>5.2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.69</td>\n",
       "      <td>2.17</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.360722</td>\n",
       "      <td>0.054912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25021</th>\n",
       "      <td>4.8</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>QPR</td>\n",
       "      <td>2.82</td>\n",
       "      <td>3.38</td>\n",
       "      <td>2.67</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.369331</td>\n",
       "      <td>0.004368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25027</th>\n",
       "      <td>4.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Montpellier</td>\n",
       "      <td>3.91</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.09</td>\n",
       "      <td>2.57</td>\n",
       "      <td>False</td>\n",
       "      <td>0.441650</td>\n",
       "      <td>0.161538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25028</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Nantes</td>\n",
       "      <td>3.91</td>\n",
       "      <td>3.58</td>\n",
       "      <td>2.04</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.303666</td>\n",
       "      <td>0.031186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25052</th>\n",
       "      <td>5.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>Rochdale</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.68</td>\n",
       "      <td>2.27</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.351348</td>\n",
       "      <td>0.024551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25063</th>\n",
       "      <td>7.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Empoli</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.49</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.53</td>\n",
       "      <td>False</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.088807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25074</th>\n",
       "      <td>2.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>FC Koln</td>\n",
       "      <td>2.77</td>\n",
       "      <td>3.65</td>\n",
       "      <td>2.60</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.372453</td>\n",
       "      <td>0.004806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25076</th>\n",
       "      <td>4.4</td>\n",
       "      <td>12.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>St Etienne</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.18</td>\n",
       "      <td>2.71</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.360536</td>\n",
       "      <td>0.005927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25085</th>\n",
       "      <td>4.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Burnley</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.31</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.288009</td>\n",
       "      <td>0.003918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25092</th>\n",
       "      <td>3.6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Lorient</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2.21</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.325305</td>\n",
       "      <td>0.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25103</th>\n",
       "      <td>3.6</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Inverness C</td>\n",
       "      <td>3.37</td>\n",
       "      <td>3.37</td>\n",
       "      <td>2.32</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.362611</td>\n",
       "      <td>0.054918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25123</th>\n",
       "      <td>3.4</td>\n",
       "      <td>11.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>9.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>4.01</td>\n",
       "      <td>3.74</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.320526</td>\n",
       "      <td>0.060110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25124</th>\n",
       "      <td>4.8</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Rennes</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.42</td>\n",
       "      <td>2.36</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.406712</td>\n",
       "      <td>0.093232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25134</th>\n",
       "      <td>1.4</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>La Coruna</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.79</td>\n",
       "      <td>False</td>\n",
       "      <td>0.270905</td>\n",
       "      <td>0.007053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25153</th>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>Wolfsburg</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.02</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.304267</td>\n",
       "      <td>0.032528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25159</th>\n",
       "      <td>7.2</td>\n",
       "      <td>14.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>14.4</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>H</td>\n",
       "      <td>Atalanta</td>\n",
       "      <td>3.83</td>\n",
       "      <td>3.68</td>\n",
       "      <td>2.05</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.309435</td>\n",
       "      <td>0.036211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25160</th>\n",
       "      <td>6.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Fiorentina</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.03</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>True</td>\n",
       "      <td>0.514124</td>\n",
       "      <td>0.226767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25200</th>\n",
       "      <td>3.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Motherwell</td>\n",
       "      <td>4.14</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1.93</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.337272</td>\n",
       "      <td>0.083465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25205</th>\n",
       "      <td>4.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Dundee</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.48</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.80</td>\n",
       "      <td>False</td>\n",
       "      <td>0.357914</td>\n",
       "      <td>0.000771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25216</th>\n",
       "      <td>5.6</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>Hertha</td>\n",
       "      <td>3.96</td>\n",
       "      <td>3.66</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.73</td>\n",
       "      <td>False</td>\n",
       "      <td>0.411688</td>\n",
       "      <td>0.143592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25220</th>\n",
       "      <td>5.6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>D</td>\n",
       "      <td>Angers</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.11</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.365187</td>\n",
       "      <td>0.071932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25226</th>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Nancy</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.66</td>\n",
       "      <td>2.11</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.300381</td>\n",
       "      <td>0.007984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25233</th>\n",
       "      <td>3.6</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>9.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>Leganes</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.57</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.332157</td>\n",
       "      <td>0.059677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25250</th>\n",
       "      <td>4.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Genoa</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.08</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.353599</td>\n",
       "      <td>0.095201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25256</th>\n",
       "      <td>6.8</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Ath Madrid</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.66</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.378426</td>\n",
       "      <td>0.111046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25263</th>\n",
       "      <td>5.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>St Johnstone</td>\n",
       "      <td>2.24</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1.20</td>\n",
       "      <td>True</td>\n",
       "      <td>0.524282</td>\n",
       "      <td>0.069737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25268</th>\n",
       "      <td>6.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>Crotone</td>\n",
       "      <td>3.18</td>\n",
       "      <td>4.06</td>\n",
       "      <td>2.15</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>0.424195</td>\n",
       "      <td>0.103682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313 rows Ã— 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       A_MEANS_FIVE_AC  A_MEANS_FIVE_AF  A_MEANS_FIVE_AR  A_MEANS_FIVE_AS  \\\n",
       "23056              6.0              9.4              0.0             10.4   \n",
       "23064              2.6             16.6              0.0              9.6   \n",
       "23070              4.4             10.2              0.0             10.0   \n",
       "23073              4.0             11.2              0.0             15.4   \n",
       "23077              4.8             15.0              0.0              9.8   \n",
       "23080              4.6              9.2              0.2             14.8   \n",
       "23091              6.2             13.0              0.2              9.8   \n",
       "23097              5.0             13.0              0.0             13.2   \n",
       "23115              4.4             17.6              0.4             10.4   \n",
       "23123              4.2             15.0              0.2              8.8   \n",
       "23125              8.6             12.8              0.2             12.2   \n",
       "23126              6.6             13.6              0.2             15.6   \n",
       "23128              6.2             11.4              0.0             11.4   \n",
       "23130              7.0             11.8              0.0             14.2   \n",
       "23131              6.6             11.0              0.2             12.0   \n",
       "23134              5.6             15.4              0.0             14.2   \n",
       "23145              5.0             16.6              0.0             10.8   \n",
       "23159              4.2             12.4              0.0             11.4   \n",
       "23160              5.4             11.0              0.2             17.2   \n",
       "23163              6.4             10.8              0.2             14.4   \n",
       "23164              2.8             16.2              0.4              7.8   \n",
       "23165              5.0             15.4              0.0             11.4   \n",
       "23169              5.8             10.8              0.2             14.0   \n",
       "23170              4.0              9.4              0.0             12.8   \n",
       "23175              5.6             13.4              0.2              9.8   \n",
       "23193              4.0             13.8              0.0             11.2   \n",
       "23198              3.0             11.4              0.0             11.4   \n",
       "23202              4.8             13.4              0.0             10.4   \n",
       "23206              2.2             15.0              0.2              7.8   \n",
       "23217              7.0             11.6              0.0             12.4   \n",
       "...                ...              ...              ...              ...   \n",
       "25009              3.6             10.6              0.4             10.8   \n",
       "25013              3.4             12.0              0.0             11.0   \n",
       "25014              4.8              9.4              0.2             10.6   \n",
       "25019              5.2             15.0              0.2             12.2   \n",
       "25021              4.8              9.6              0.2             14.2   \n",
       "25027              4.0             13.4              0.2             12.4   \n",
       "25028              4.0             10.6              0.0              8.6   \n",
       "25052              5.6             12.2              0.0             11.6   \n",
       "25063              7.0             10.4              0.0              8.6   \n",
       "25074              2.4             12.0              0.0              9.2   \n",
       "25076              4.4             12.6              0.0             13.2   \n",
       "25085              4.0             13.2              0.0             10.0   \n",
       "25092              3.6             11.0              0.2             12.8   \n",
       "25103              3.6             12.8              0.2              7.8   \n",
       "25123              3.4             11.6              0.2              9.4   \n",
       "25124              4.8              9.8              0.2             12.8   \n",
       "25134              1.4             14.4              0.0              7.6   \n",
       "25153              3.0             15.0              0.2             10.4   \n",
       "25159              7.2             14.2              0.4             14.4   \n",
       "25160              6.0             13.6              0.0             14.8   \n",
       "25200              3.6             11.4              0.0              8.0   \n",
       "25205              4.2             10.6              0.2             11.0   \n",
       "25216              5.6             12.8              0.0             13.4   \n",
       "25220              5.6              9.0              0.2             12.0   \n",
       "25226              4.0             12.0              0.2             10.0   \n",
       "25233              3.6             14.4              0.2              9.2   \n",
       "25250              4.0             11.4              0.4             12.6   \n",
       "25256              6.8             11.2              0.0             14.2   \n",
       "25263              5.8             15.0              0.0             12.0   \n",
       "25268              6.0             13.2              0.0             15.0   \n",
       "\n",
       "       A_MEANS_FIVE_AST  A_MEANS_FIVE_AY  A_MEANS_FIVE_FTAG  \\\n",
       "23056               3.0              1.8                1.2   \n",
       "23064               3.4              2.6                1.0   \n",
       "23070               3.6              2.0                1.0   \n",
       "23073               5.2              2.2                1.8   \n",
       "23077               4.0              3.2                1.0   \n",
       "23080               4.0              1.8                1.2   \n",
       "23091               4.8              3.6                1.6   \n",
       "23097               4.4              3.2                1.4   \n",
       "23115               3.4              2.6                1.0   \n",
       "23123               3.8              1.8                0.8   \n",
       "23125               5.6              2.0                1.8   \n",
       "23126               5.8              2.6                2.0   \n",
       "23128               4.8              1.4                2.0   \n",
       "23130               6.8              1.8                1.2   \n",
       "23131               4.6              1.0                1.0   \n",
       "23134               5.4              2.0                2.0   \n",
       "23145               3.4              2.2                0.4   \n",
       "23159               3.8              1.8                0.8   \n",
       "23160               4.2              2.2                1.0   \n",
       "23163               2.4              2.6                0.4   \n",
       "23164               3.8              4.6                1.6   \n",
       "23165               3.0              2.4                1.6   \n",
       "23169               5.6              1.4                0.8   \n",
       "23170               4.8              2.0                1.0   \n",
       "23175               3.2              3.4                1.2   \n",
       "23193               3.2              2.4                1.0   \n",
       "23198               5.0              1.4                1.4   \n",
       "23202               4.4              2.6                2.0   \n",
       "23206               3.4              2.2                1.6   \n",
       "23217               5.0              1.4                1.6   \n",
       "...                 ...              ...                ...   \n",
       "25009               3.6              1.4                1.2   \n",
       "25013               3.6              1.0                0.6   \n",
       "25014               2.8              2.0                1.2   \n",
       "25019               4.2              1.6                1.0   \n",
       "25021               3.2              1.0                0.2   \n",
       "25027               4.4              1.6                1.0   \n",
       "25028               3.6              1.6                1.4   \n",
       "25052               5.0              1.4                1.4   \n",
       "25063               3.2              2.0                1.0   \n",
       "25074               4.4              2.0                2.8   \n",
       "25076               4.2              2.0                1.2   \n",
       "25085               2.6              2.4                0.6   \n",
       "25092               4.8              1.4                1.6   \n",
       "25103               3.0              2.4                0.4   \n",
       "25123               4.2              2.0                1.2   \n",
       "25124               4.0              1.0                1.4   \n",
       "25134               2.4              2.4                0.6   \n",
       "25153               4.0              2.2                1.8   \n",
       "25159               5.2              2.6                1.2   \n",
       "25160               5.4              3.0                1.8   \n",
       "25200               3.6              1.8                1.6   \n",
       "25205               4.4              1.4                0.8   \n",
       "25216               5.4              1.8                0.8   \n",
       "25220               3.2              1.0                0.8   \n",
       "25226               4.0              1.0                0.6   \n",
       "25233               3.0              2.4                1.4   \n",
       "25250               3.8              1.8                2.0   \n",
       "25256               6.0              2.4                1.4   \n",
       "25263               5.0              2.2                1.4   \n",
       "25268               5.2              3.4                1.8   \n",
       "\n",
       "       A_MEANS_FIVE_FTHG  A_MEANS_FIVE_FTR_A  A_MEANS_FIVE_FTR_D  \\\n",
       "23056                0.8                 0.4                 0.4   \n",
       "23064                1.2                 0.2                 0.4   \n",
       "23070                1.4                 0.2                 0.4   \n",
       "23073                1.0                 0.6                 0.0   \n",
       "23077                1.0                 0.4                 0.2   \n",
       "23080                1.2                 0.6                 0.2   \n",
       "23091                1.8                 0.2                 0.4   \n",
       "23097                2.2                 0.0                 0.6   \n",
       "23115                1.2                 0.4                 0.2   \n",
       "23123                2.4                 0.2                 0.2   \n",
       "23125                2.0                 0.4                 0.2   \n",
       "23126                1.4                 0.6                 0.2   \n",
       "23128                1.0                 0.8                 0.2   \n",
       "23130                1.2                 0.2                 0.4   \n",
       "23131                1.0                 0.0                 1.0   \n",
       "23134                0.6                 0.6                 0.4   \n",
       "23145                1.2                 0.0                 0.4   \n",
       "23159                1.0                 0.4                 0.2   \n",
       "23160                2.0                 0.0                 0.2   \n",
       "23163                1.6                 0.0                 0.4   \n",
       "23164                2.0                 0.4                 0.0   \n",
       "23165                2.0                 0.4                 0.0   \n",
       "23169                1.2                 0.2                 0.4   \n",
       "23170                1.4                 0.2                 0.4   \n",
       "23175                2.2                 0.0                 0.2   \n",
       "23193                0.4                 0.4                 0.2   \n",
       "23198                0.8                 0.6                 0.2   \n",
       "23202                1.8                 0.4                 0.2   \n",
       "23206                1.6                 0.4                 0.4   \n",
       "23217                0.8                 0.6                 0.4   \n",
       "...                  ...                 ...                 ...   \n",
       "25009                2.2                 0.0                 0.4   \n",
       "25013                1.4                 0.4                 0.2   \n",
       "25014                1.0                 0.4                 0.2   \n",
       "25019                2.4                 0.2                 0.2   \n",
       "25021                0.8                 0.0                 0.4   \n",
       "25027                0.8                 0.6                 0.2   \n",
       "25028                2.0                 0.4                 0.0   \n",
       "25052                1.4                 0.4                 0.2   \n",
       "25063                1.0                 0.4                 0.4   \n",
       "25074                1.6                 0.6                 0.4   \n",
       "25076                1.2                 0.4                 0.2   \n",
       "25085                1.6                 0.0                 0.6   \n",
       "25092                2.2                 0.4                 0.0   \n",
       "25103                2.0                 0.2                 0.2   \n",
       "25123                0.6                 0.6                 0.4   \n",
       "25124                1.8                 0.4                 0.0   \n",
       "25134                1.2                 0.2                 0.4   \n",
       "25153                2.0                 0.4                 0.2   \n",
       "25159                1.2                 0.2                 0.6   \n",
       "25160                0.8                 0.6                 0.4   \n",
       "25200                1.0                 0.6                 0.2   \n",
       "25205                2.4                 0.0                 0.2   \n",
       "25216                1.0                 0.2                 0.2   \n",
       "25220                2.0                 0.2                 0.0   \n",
       "25226                1.2                 0.4                 0.2   \n",
       "25233                1.4                 0.4                 0.2   \n",
       "25250                1.8                 0.4                 0.4   \n",
       "25256                1.0                 0.6                 0.0   \n",
       "25263                0.8                 0.4                 0.4   \n",
       "25268                1.4                 0.4                 0.4   \n",
       "\n",
       "           ...        INFO_FTR  INFO_HTR  INFO_HomeTeam  INFO_PSA  INFO_PSD  \\\n",
       "23056      ...               A         A      Brentford      3.20      3.55   \n",
       "23064      ...               A         D     Ingolstadt      3.81      3.45   \n",
       "23070      ...               D         D       West Ham      4.13      3.59   \n",
       "23073      ...               D         D         Burton      3.43      3.55   \n",
       "23077      ...               D         H  Nott'm Forest      3.30      3.48   \n",
       "23080      ...               A         A          Wigan      3.09      3.23   \n",
       "23091      ...               H         D     Las Palmas      4.04      3.71   \n",
       "23097      ...               D         D     Kilmarnock      2.69      3.45   \n",
       "23115      ...               A         A       Sassuolo      3.20      3.24   \n",
       "23123      ...               A         A           Bury      3.28      3.46   \n",
       "23125      ...               H         A     Gillingham      2.81      3.29   \n",
       "23126      ...               H         H       Millwall      3.27      3.61   \n",
       "23128      ...               D         D         Oldham      2.60      3.36   \n",
       "23130      ...               H         H     Shrewsbury      2.03      3.74   \n",
       "23131      ...               H         H        Swindon      2.23      3.48   \n",
       "23134      ...               A         H     Leverkusen      3.19      3.43   \n",
       "23145      ...               A         D      Wolfsburg      2.96      3.44   \n",
       "23159      ...               D         A         Fulham      3.35      3.32   \n",
       "23160      ...               A         A        Ipswich      3.98      3.53   \n",
       "23163      ...               H         H        Reading      3.96      3.51   \n",
       "23164      ...               H         H  AFC Wimbledon      4.08      3.60   \n",
       "23165      ...               H         H         Bolton      3.16      3.43   \n",
       "23169      ...               A         D    Northampton      2.87      3.63   \n",
       "23170      ...               H         H         Oxford      3.53      3.54   \n",
       "23175      ...               A         D        Walsall      3.19      3.66   \n",
       "23193      ...               A         H  Werder Bremen      2.60      3.50   \n",
       "23198      ...               A         D     St Etienne      3.95      3.22   \n",
       "23202      ...               A         A         Empoli      2.15      3.37   \n",
       "23206      ...               A         D         Alaves      3.17      3.21   \n",
       "23217      ...               D         D       Millwall      3.91      3.41   \n",
       "...        ...             ...       ...            ...       ...       ...   \n",
       "25009      ...               D         D          Stoke      3.85      3.45   \n",
       "25013      ...               H         H     Birmingham      2.80      3.48   \n",
       "25014      ...               H         D      Blackburn      3.68      3.38   \n",
       "25019      ...               D         A          Leeds      3.42      3.69   \n",
       "25021      ...               H         D            QPR      2.82      3.38   \n",
       "25027      ...               A         A    Montpellier      3.91      3.45   \n",
       "25028      ...               H         D         Nantes      3.91      3.58   \n",
       "25052      ...               D         A       Rochdale      3.20      3.68   \n",
       "25063      ...               A         A         Empoli      3.74      3.49   \n",
       "25074      ...               H         H        FC Koln      2.77      3.65   \n",
       "25076      ...               D         D     St Etienne      2.96      3.18   \n",
       "25085      ...               D         D        Burnley      3.64      3.20   \n",
       "25092      ...               D         D        Lorient      3.40      3.59   \n",
       "25103      ...               H         H    Inverness C      3.37      3.37   \n",
       "25123      ...               H         H      Marseille      4.01      3.74   \n",
       "25124      ...               H         D         Rennes      3.23      3.42   \n",
       "25134      ...               A         A      La Coruna      3.92      3.60   \n",
       "25153      ...               D         A      Wolfsburg      3.74      3.90   \n",
       "25159      ...               D         H       Atalanta      3.83      3.68   \n",
       "25160      ...               H         D     Fiorentina      3.63      3.98   \n",
       "25200      ...               H         D     Motherwell      4.14      3.76   \n",
       "25205      ...               A         A         Dundee      2.88      3.48   \n",
       "25216      ...               A         A         Hertha      3.96      3.66   \n",
       "25220      ...               H         D         Angers      3.66      3.60   \n",
       "25226      ...               H         H          Nancy      3.59      3.66   \n",
       "25233      ...               D         D        Leganes      3.79      3.57   \n",
       "25250      ...               H         H          Genoa      4.00      3.40   \n",
       "25256      ...               H         H     Ath Madrid      3.99      3.66   \n",
       "25263      ...               A         A   St Johnstone      2.24      3.53   \n",
       "25268      ...               H         H        Crotone      3.18      4.06   \n",
       "\n",
       "       INFO_PSH  INFO_WIN   pred      prob  prob_less_bet  \n",
       "23056      2.35      2.07  False  0.392700       0.066967  \n",
       "23064      2.14      2.64  False  0.310000       0.035275  \n",
       "23070      2.00     -1.00  False  0.261110       0.002712  \n",
       "23073      2.21     -1.00  False  0.353792       0.047043  \n",
       "23077      2.30     -1.00  False  0.330281       0.014824  \n",
       "23080      2.55      1.98  False  0.364211       0.028640  \n",
       "23091      1.99     -1.00  False  0.351427       0.093695  \n",
       "23097      2.75     -1.00  False  0.408364       0.019259  \n",
       "23115      2.51      2.00  False  0.401712       0.068379  \n",
       "23123      2.32      2.10  False  0.480311       0.157731  \n",
       "23125      2.74     -1.00  False  0.480196       0.101408  \n",
       "23126      2.26     -1.00  False  0.436138       0.120680  \n",
       "23128      2.96     -1.00  False  0.420252       0.018645  \n",
       "23130      3.76     -1.00   True  0.617744       0.120232  \n",
       "23131      3.45     -1.00   True  0.537559       0.074596  \n",
       "23134      2.41      2.13  False  0.406818       0.087329  \n",
       "23145      2.55      1.88  False  0.349191       0.001969  \n",
       "23159      2.35     -1.00  False  0.358623       0.038110  \n",
       "23160      2.04      2.71  False  0.367904       0.098362  \n",
       "23163      2.05     -1.00  False  0.295379       0.032221  \n",
       "23164      1.99     -1.00  False  0.334551       0.065010  \n",
       "23165      2.39     -1.00  False  0.329141       0.002343  \n",
       "23169      2.49      1.79  False  0.369547       0.011124  \n",
       "23170      2.18     -1.00  False  0.447769       0.152784  \n",
       "23175      2.26      2.14  False  0.335420       0.016949  \n",
       "23193      2.87      1.56  False  0.441418       0.050793  \n",
       "23198      2.17      2.63  False  0.433261       0.157778  \n",
       "23202      3.87      1.08  False  0.494870       0.014101  \n",
       "23206      2.54      2.06  False  0.335994       0.009197  \n",
       "23217      2.10     -1.00  False  0.312022       0.038049  \n",
       "...         ...       ...    ...       ...            ...  \n",
       "25009      2.08     -1.00  False  0.323418       0.046409  \n",
       "25013      2.62     -1.00  False  0.365649       0.000685  \n",
       "25014      2.17     -1.00  False  0.317039       0.030506  \n",
       "25019      2.17     -1.00  False  0.360722       0.054912  \n",
       "25021      2.67     -1.00  False  0.369331       0.004368  \n",
       "25027      2.09      2.57  False  0.441650       0.161538  \n",
       "25028      2.04     -1.00  False  0.303666       0.031186  \n",
       "25052      2.27     -1.00  False  0.351348       0.024551  \n",
       "25063      2.15      2.53  False  0.372093       0.088807  \n",
       "25074      2.60     -1.00  False  0.372453       0.004806  \n",
       "25076      2.71     -1.00  False  0.360536       0.005927  \n",
       "25085      2.31     -1.00  False  0.288009       0.003918  \n",
       "25092      2.21     -1.00  False  0.325305       0.024100  \n",
       "25103      2.32     -1.00  False  0.362611       0.054918  \n",
       "25123      1.96     -1.00  False  0.320526       0.060110  \n",
       "25124      2.36     -1.00  False  0.406712       0.093232  \n",
       "25134      2.05      2.79  False  0.270905       0.007053  \n",
       "25153      2.02     -1.00  False  0.304267       0.032528  \n",
       "25159      2.05     -1.00  False  0.309435       0.036211  \n",
       "25160      2.03     -1.00   True  0.514124       0.226767  \n",
       "25200      1.93     -1.00  False  0.337272       0.083465  \n",
       "25205      2.55      1.80  False  0.357914       0.000771  \n",
       "25216      2.02      2.73  False  0.411688       0.143592  \n",
       "25220      2.11     -1.00  False  0.365187       0.071932  \n",
       "25226      2.11     -1.00  False  0.300381       0.007984  \n",
       "25233      2.08     -1.00  False  0.332157       0.059677  \n",
       "25250      2.08     -1.00  False  0.353599       0.095201  \n",
       "25256      2.00     -1.00  False  0.378426       0.111046  \n",
       "25263      3.40      1.20   True  0.524282       0.069737  \n",
       "25268      2.15     -1.00  False  0.424195       0.103682  \n",
       "\n",
       "[313 rows x 194 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bet_current_season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot a leqrning curve\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='f1')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "plot_learning_curve(clf, 'Learning Curve', X, y, cv=4, n_jobs=-1).show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
